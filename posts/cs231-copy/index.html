<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=2815&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Notes for CS231n: Deep Learning for Computer Vision | Bo's Log | What I cannot create, I do not understand</title>
<meta name=keywords content><meta name=description content="Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch & Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Deep Learning Basics Lecture 1: Computer vision overview
Lecture 2: Image Classification with linear Classifiers
The data-driven approch What is data-driven approaches linear classification & kNN means? Data-Driven Approach: 1."><meta name=author content="Bo Liu"><link rel=canonical href=http://localhost:2815/posts/cs231-copy/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=http://localhost:2815/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:2815/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:2815/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:2815/apple-touch-icon.png><link rel=mask-icon href=http://localhost:2815/android-chrome-192x192.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:2815/posts/cs231-copy/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Notes for CS231n: Deep Learning for Computer Vision"><meta property="og:description" content="Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch & Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Deep Learning Basics Lecture 1: Computer vision overview
Lecture 2: Image Classification with linear Classifiers
The data-driven approch What is data-driven approaches linear classification & kNN means? Data-Driven Approach: 1."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:2815/posts/cs231-copy/"><meta property="og:image" content="http://localhost:2815/favicon.ico"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-06T16:30:19+08:00"><meta property="article:modified_time" content="2024-05-06T16:30:19+08:00"><meta property="og:site_name" content="Bo's Log"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:2815/favicon.ico"><meta name=twitter:title content="Notes for CS231n: Deep Learning for Computer Vision"><meta name=twitter:description content="Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch & Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Deep Learning Basics Lecture 1: Computer vision overview
Lecture 2: Image Classification with linear Classifiers
The data-driven approch What is data-driven approaches linear classification & kNN means? Data-Driven Approach: 1."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:2815/posts/"},{"@type":"ListItem","position":2,"name":"Notes for CS231n: Deep Learning for Computer Vision","item":"http://localhost:2815/posts/cs231-copy/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Notes for CS231n: Deep Learning for Computer Vision","name":"Notes for CS231n: Deep Learning for Computer Vision","description":"Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch \u0026amp; Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Deep Learning Basics Lecture 1: Computer vision overview\nLecture 2: Image Classification with linear Classifiers\nThe data-driven approch What is data-driven approaches linear classification \u0026amp; kNN means? Data-Driven Approach: 1.","keywords":[],"articleBody":"Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch \u0026 Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Deep Learning Basics Lecture 1: Computer vision overview\nLecture 2: Image Classification with linear Classifiers\nThe data-driven approch What is data-driven approaches linear classification \u0026 kNN means? Data-Driven Approach: 1. Colloect a dataset of images and labels. 2. Use Machine Learning algorithms to train a classifier. 3. Evaluate the classifier on new images. There is two basic data-driven aprroaches to image classification\nK-nearest neighbor (kNN) What is kNN?\nkNN is a non-parametric algorithm, meaning it does’t make any assumptions about the underlying data distribution. It doesn’t build a model until a new instance is presented to it. How does kNN work? Data Preparation: The dataset is divided into a training set and a testing set(or validation set). Distance Calculation: When a new instance(query point) is presented to the model, the algorithm calculates the distance between the new query point and each instance in the training set. The most common distance metrics used in kNN are Euclidean distance(L2 norm), Manhattan distance(L1 norm), and Minkowski distance(Lp norm). K-nearest Neighbor Selection: The algorithm selects the K most similar instances (nearest neighbors) to the query point based on the calculated distances. The value of K is a hyperparameter that needs to be set. Majority vote (classification): In classification problems, the algorithm assigns the query point to the class that is most common among its K nearest neighbors. This is done by taking a majority vote among the classes of the nearest neighbors. Average value (regression): In regression problems, the algorithm predicts the value of the query point by taking the average of the values of its K nearest neighbors. Prediction: The final prediction is made based on the majority vote (classification) or average value (regression). Key takeways In image classification we start with a training set of images and labels, and must predict labels on the test set The K-Nearest Neighbors classifier predicts labels based on the K nearest training examples Distance metric and K are hyperparameters Choose hyperparameters using the validation set Only run on the test set once at the very end! K value: The choice of K is crucial, as it affects the performance of the algorithm. A small value of K can lead to overfitting, while a large value can lead to underfitting. Distance metric: The choice of distance metric can impact the performance of the algorithm, especially when dealing with high-dimensional data. Computational complexity: KNN can be computationally expensive, especially for large datasets, since it requires calculating distances between all instances. Distance Metric L1 (Manhattan)distance $$d_1(I_1,I_2) = \\sum_{p} |I_1^p - I_2^p|$$ L2 (Euclidean) distance $$d_2(I_1,I_2) = \\sqrt{\\sum_{p} (I_1^p - I_2^p)^2}$$ Linear classifiers What is a linear classifier?\nLinear classifier is a function $$f(x,W) = Wx + b$$ W is parameters or weights Algebraic/Visual/ Geometric veiwpoints\nSVM loss What is Multiclass SVM loss?\nGiven an example $(x_i, y_i)$ where $x_i$ is the image and where $y_i$ is the (integer) label, and using the shorthand for the scores vector: $s=f(x_i, W)$ the SVM loss has the form: $$\rL_i = ∑_{j≠y_i}\r\\begin{cases}\r0 \u0026 \\text{if } s_{y_i} ≥ s_j + 1 \\\\\rs_j - s_{y_i} + 1 \u0026 \\text{otherwise}\r\\end{cases}\\\\\r= ∑_{j≠y_i} max(0, s_j - s_{y_i} + 1)\r$$ $S_{y_i} - s_j$ is the difference in scores between correct and incorrect class calculation Q1: What happens to loss if car scores decrease by 0.5 for this training example? To determine the effect of decreasing the car score by 0.5, we need to look at the multiclass SVM loss formula:\n$ L*i = \\sum*{j \\neq y*i} \\max(0, s_j - s*{y_i} + 1) $\nFor this training example, the scores are:\ncat: 1.3 car: 4.9 frog: 2.0 Assuming the correct class is “car” (i.e., $ y_i = \\text{car} $), the loss is calculated as:\n$ L_i = \\max(0, 1.3 - 4.9 + 1) + \\max(0, 2.0 - 4.9 + 1) $ $ L_i = \\max(0, -2.6) + \\max(0, -1.9) $ $ L_i = 0 + 0 = 0 $\nNow, if the car score decreases by 0.5, the new scores are:\ncat: 1.3 car: 4.4 frog: 2.0 The new loss is:\n$ L_i = \\max(0, 1.3 - 4.4 + 1) + \\max(0, 2.0 - 4.4 + 1) $ $ L_i = \\max(0, -2.1) + \\max(0, -1.4) $ $ L_i = 0 + 0 = 0 $\nThus, the loss remains the same at 0.\nQ2: What is the min/max possible SVM loss $ L_i $? Minimum SVM Loss: The minimum SVM loss occurs when the model perfectly classifies the example with a margin of at least 1. Hence, for all incorrect classes $ j $:\n$ s*j - s*{y_i} + 1 \\leq 0 $\nIn this case, the loss for these classes will be 0. Therefore, the minimum SVM loss $ L_i $ is 0.\nMaximum SVM Loss: The maximum SVM loss occurs when the scores for all incorrect classes $ j $ are infinitely higher than the score for the correct class $ y_i $. As the scores for incorrect classes increase without bound, the loss also increases without bound. Therefore, the maximum SVM loss $ L_i $ is infinity.\nQ3: At initialization $ W $ is small so all $ s \\approx 0 $. What is the loss $ L_i $, assuming $ N $ examples and $ C $ classes? At initialization, if all scores $ s \\approx 0 $:\nThe multiclass SVM loss for a single example is:\n$ L*i = \\sum*{j \\neq y*i} \\max(0, s_j - s*{y_i} + 1) $\nSince $ s_j \\approx 0 $ for all $ j $:\n$ L*i = \\sum*{j \\neq y*i} \\max(0, 0 - 0 + 1) $ $ L_i = \\sum*{j \\neq y_i} 1 $ $ L_i = C - 1 $\n(where $ C $ is the number of classes)\nFor $ N $ examples, the total loss is:\n$ L = \\sum*{i=1}^{N} L_i = \\sum*{i=1}^{N} (C - 1) $ $ L = N(C - 1) $\nSo, the loss $ L_i $ for each example is $ C - 1 $, and the total loss for $ N $ examples is $ N(C - 1) $.\nQ4: What if the sum was over all classes (including $ j = y_i $)? If the sum was over all classes, including $ j = y_i $, the loss formula would be:\n$ L*i = \\sum*{j} \\max(0, s*j - s*{y_i} + 1) $\nSince the term for $ j = y_i $ is included, we have:\n$ \\max(0, s*{y_i} - s*{y_i} + 1) = \\max(0, 1) = 1 $\nSo, the loss would be:\n$ L*i = \\sum*{j \\neq y*i} \\max(0, s_j - s*{y_i} + 1) + 1 $\nThus, it includes an additional $ 1 $ term compared to the standard SVM loss for each example.\nSoftmax loss What is Softmax Classifier(Multinomial Logistic Regression) - Want to interpret raw classifier scores as probabilities $$ s = f(x*i; W) P(Y=k|X=x_i) = \\frac{e^{s_k}}{\\sum*{j=1}^{K} e^{s*j}}$$ Softmax Function\n$L_i = - log P(Y=y_i|X=x_i)$ Kullback-Leibler divergence $$ D*{KL}(P||Q) = \\sum*{y\\in \\mathcal{Y}} P(y) log \\frac{P(y)}{Q(y)}$$ Cross-Entropy $$ H(P,Q)=H(p)+D*{KL}(P||Q)$$ Questions\nQ1: What is the min/max possible softmax loss $ L_i $? Minimum Softmax Loss: The minimum softmax loss occurs when the model is perfectly confident and correct. In this case, the probability $ P(Y = y_i | X = x_i) $ for the correct class is 1. Therefore, the loss is:\n$ L_i = -\\log(1) = 0 $\nThus, the minimum possible softmax loss $ L_i $ is 0.\nMaximum Softmax Loss: The maximum softmax loss occurs when the model is infinitely wrong, meaning it assigns a probability of 0 to the correct class. In practical terms, this happens when the score for the correct class is much lower than the scores for all other classes. As the probability approaches 0, the loss approaches infinity:\n$ L_i = -\\log(0) = \\infty $\nThus, the maximum possible softmax loss $ L_i $ is infinity.\nQ2: At initialization, all $ s_j $ will be approximately equal; what is the softmax loss $ L_i $, assuming $ C $ classes? At initialization, if all scores $ s_j $ are approximately equal, the softmax probabilities for each class will be evenly distributed. For $ C $ classes, each class will have a probability of $ \\frac{1}{C} $.\nThe softmax loss for the correct class $ y_i $ is:\n$ L_i = -\\log\\left(\\frac{1}{C}\\right) = \\log(C) $\nTherefore, the softmax loss $ L_i $ at initialization, assuming $ C $ classes, is $ \\log(C) $.\nQ3: If all scores are small random values, what is the loss? $$-log(\\frac{1}{C})\\\\\rlog(10)\\approx 2.3$$ Cross-Entropy vs SVM Loss Q: What is cross-entropy loss? What is SVM loss? Cross-entropy loss \u003e 0, SVM loss = 0\nQ: What happens to each loss if I slightly change the scores of the last datapoint? Cross-entropy loss will change; SVM loss will stay the same\nSummary\nSoftmax: $L_i = -\\log(\\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}})$ Softmax: A type of loss function used for multi-class classification. It measures the negative log-likelihood of the correct class, encouraging the model to assign higher probabilities to the correct class. L_i: The loss for the i-th training example. s_j: The score for the j-th class. y_i: The true class label for the i-th training example. e^{s_j}: The exponential of the score for the j-th class. ∑_j e^{s_j}: The sum of exponentials of scores for all classes. SVM:$L_i = \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + 1)$\nSVM: A type of loss function used for binary classification. It measures the hinge loss, which encourages the model to have a margin between the scores for the correct and incorrect classes. max(0, s_j - s_{y_i} + 1): The hinge loss, which is zero if the score for the correct class is greater than the score for the incorrect class by at least 1, and otherwise is equal to the difference between the scores plus 1. Lecture 3: Regularization and Optimization\nRegularization\nWhat is regularization? Regularization pushes agiainst fitting the data too well so we dont’t fir noise in the data\n$L(W) = \\frac{1}{N} \\sum_{i=1}^{N} L_i(f(x_i, W), y_i) + \\lambda R(W)$ Where:\nL(W) is the overall loss function N is the number of training examples L_i is the loss function for the i-th training example f(x_i, W) is the model’s prediction for the i-th training example y_i is the true label for the i-th training example R(W) is the regularization term λ is the regularization strength (hyperparameter)\nL2 regularization: $R(W) = \\sum_k \\sum_l W_{k,l}^2$\nL2 regularization: A type of regularization that penalizes the sum of squared weights. It encourages the model to have smaller weights, and likes to “spread out” the weights which can prevent overfitting. R(W): The regularization term, which is added to the loss function to penalize complex models. W: The weight matrix of the model. k, l: Indices for the rows and columns of the weight matrix. W_{k,l}^2: The square of the weight at row k and column l. L1 regularization: $R(W) = \\sum_k \\sum_l |W_{k,l}|$\nL1 regularization: A type of regularization that penalizes the sum of absolute values of weights. It encourages the model to have sparse weights, meaning that many weights are set to zero. This can help with feature selection and prevent overfitting. |W_{k,l}|: The absolute value of the weight at row k and column l. Elastic net (L1 + L2): $R(W) = \\sum_k \\sum_l \\beta W_{k,l}^2 + |W_{k,l}|$\nElastic net: A type of regularization that combines L1 and L2 regularization. It encourages both sparsity and smaller weights. β: A hyperparameter that controls the balance between L1 and L2 regularization. Dropout, Batch normalization, Stochastic depth, fractinal pooling, etc\nWhy regularize?\nExpress pregerences over weights Make the model simple so it works on test data Improve optimization by adding curvature Stochastic Gradient Descent\nGradient descent is an iterative optimization algorithm used to find the minimum of a function. It’s widely used in ML to train models by adjusting their parameters to minimize the loss function. Here’s a breakdown:\n1. The Goal:\nImagine you’re trying to find the lowest point in a valley. You don’t know where it is, but you can see the slope of the ground around you. In ML, the “valley” is represented by the loss function, which measures how well your model performs. The goal is to find the set of model parameters that minimize this loss. 2. How it Works:\nStart at a random point: You begin with an initial guess for the model parameters (like starting at a random spot in the valley). Calculate the gradient: The gradient is a vector that points in the direction of the steepest ascent of the function. In our valley analogy, it’s like looking at the slope of the ground to see which direction is uphill. Take a step in the opposite direction: To move towards the minimum (the lowest point), you take a small step in the direction opposite to the gradient. This is like walking downhill. Repeat: You continue calculating the gradient and taking steps in the opposite direction until you reach a point where the gradient is close to zero. This indicates you’ve found a local minimum (a point where the function is lower than its immediate neighbors). 3. Key Concepts:\nLearning rate: This parameter controls how big each step you take is. A large learning rate can lead to overshooting the minimum, while a small learning rate can make the optimization process very slow. Local vs. global minimum: Gradient descent can get stuck in a local minimum, which is a point where the function is lower than its neighbors, but not the absolute lowest point. Finding the global minimum is often difficult, but good initialization and other optimization techniques can help. Cost function: The function that measures how well your model performs. It’s the function you’re trying to minimize. 4. Applications in Machine Learning:\nTraining neural networks: Gradient descent is the workhorse algorithm for training deep learning models. Linear regression: Finding the best line of fit for a dataset. Logistic regression: Classifying data points into different categories. Many other machine learning algorithms: Gradient descent is a versatile tool used in a wide range of machine learning tasks. Visual Analogy:\nImagine you’re trying to find the lowest point in a valley. You can’t see the entire valley, but you can see the slope of the ground around you. You start at a random point and take small steps in the direction that is downhill. You keep taking steps until you reach a point where the slope is close to zero, indicating that you’ve found a low point in the valley.\nStochastic Gradient Descent. It’s a variation of the standard Gradient Descent algorithm, and it’s widely used in machine learning, especially for training large models. Here’s a breakdown:\n1. The Problem with Standard Gradient Descent:\nStandard Gradient Descent calculates the gradient of the loss function using all training examples. This can be computationally expensive, especially when dealing with large datasets. 2. How SGD Solves the Problem:\nStochasticity: Instead of using all training examples, SGD randomly selects a mini batch of examples (32 / 64 / 128 common ) to calculate the gradient. Faster Updates: Since it’s working with a smaller subset of data, SGD can update the model parameters much faster than standard Gradient Descent. Noise: The randomness introduced by using mini-batches can actually help the algorithm escape local minima and find better solutions. 3. Key Concepts:\nMini-batch size: This determines how many examples are used to calculate the gradient in each iteration. A larger mini-batch size reduces noise but slows down training. Epoch: One pass through the entire training dataset. SGD typically runs for multiple epochs. Learning rate: Controls the step size during parameter updates. 4. Advantages of SGD:\nSpeed: Much faster than standard Gradient Descent for large datasets. Escape local minima: The noise introduced by mini-batches can help the algorithm find better solutions. Online learning: SGD can be used for online learning, where new data points arrive continuously. 5. Disadvantages of SGD:\nNoisy updates: The randomness can make the optimization process less stable. Learning rate tuning: Finding the optimal learning rate can be challenging. 6. Applications in Machine Learning:\nTraining deep neural networks: SGD is the most common optimization algorithm used for training deep learning models. Natural language processing: Training language models like BERT and GPT-3. Computer vision: Training image classification and object detection models. In multiple dimensions, the gradient is the vector of (partial derivatives) along each dimension. The slope in any direction is the dot product of the direction with the gradient. The direction of steepest descent is the negative gradient\nMomentum, SGD, SGD+Momentum, RMSProp, Adam, AdamW\n1. Gradient Descent (GD):\nThe foundation: GD iteratively updates model parameters (weights) to minimize a loss function. It calculates the gradient of the loss function with respect to the parameters and takes a step in the opposite direction of the gradient. Issue: Can get stuck in local minima, especially in complex landscapes. It can also oscillate around the minimum if the learning rate is too high. 2. Stochastic Gradient Descent (SGD):\nThe speed boost: SGD uses a random subset of the training data (a mini-batch) to calculate the gradient in each iteration. This makes it significantly faster than GD, especially for large datasets. Issue: The noisy updates from mini-batches can lead to oscillations and slow convergence. 3. SGD with Momentum (SGD+Momentum):\nSmoother steps: Momentum introduces a “velocity” term that accumulates the gradient over multiple iterations. This helps the algorithm: Escape local minima: The momentum term can help the algorithm “roll over” local minima. Reduce oscillations: It smooths out the updates, making convergence more stable. 4. RMSprop (Root Mean Square Propagation):\nAdaptive learning rates: RMSprop adapts the learning rate for each parameter individually. It uses a moving average of squared gradients to scale the learning rate. Faster convergence: It can speed up convergence by adjusting the learning rate for each parameter based on the history of its gradients. Less sensitive to learning rate: It’s more robust to the choice of learning rate than SGD or SGD+Momentum. 5. Adam (Adaptive Moment Estimation):\nCombining the best: Adam combines the momentum term (like SGD+Momentum) with adaptive learning rates (like RMSprop). It estimates the first and second moments of the gradients to adjust the learning rate for each parameter. Efficient and robust: Adam is often considered a very effective and robust optimization algorithm. 6. AdamW (Adam with Weight Decay):\nRegularization boost: AdamW adds a weight decay term to the Adam optimizer. Weight decay helps prevent overfitting by penalizing large weights, similar to L2 regularization. Improved generalization: AdamW often leads to better generalization performance compared to standard Adam. 7. Nesterov Momentum:\nNesterov Momentum is an improvement over standard Momentum that looks “ahead” in the direction of the momentum before calculating the gradient. This allows it to anticipate the future direction of the parameter update and potentially avoid sharp turns or oscillations. Faster Convergence: Nesterov Momentum often converges faster than standard Momentum, especially in situations where the loss function has many local minima. Smoother Updates: It helps to smooth out the updates, reducing oscillations and making convergence more stable. 8. Adagrad:\nAdaGrad adapts the learning rate for each parameter individually based on the history of its gradients. It accumulates the squared gradients for each parameter and uses this information to scale the learning rate. Adaptive Learning Rates: AdaGrad automatically adjusts the learning rate for each parameter, making it more robust to different scales and magnitudes of gradients. Less Sensitive to Learning Rate: It’s less sensitive to the choice of the initial learning rate than standard Gradient Descent or Momentum. Learning Rate Decay: The learning rate can decay too quickly, especially in the later stages of training, leading to slow convergence. Analogy:\nImagine you’re trying to find the lowest point in a valley.\nGD: You carefully measure the slope at your current location and take a small step downhill. SGD: You quickly glance at a few random spots in the valley and take a step based on the average slope you see. SGD+Momentum: You use a running average of the slopes you’ve seen, allowing you to take larger steps downhill and escape small dips. RMSprop: You adjust your step size based on how steep the ground is in each direction, taking smaller steps in steeper areas. Adam: You combine the momentum and adaptive learning rate techniques for a more efficient and stable descent. AdamW: You add a mechanism to prevent you from straying too far from the center of the valley, helping you find a more general solution. Nesterov Imagine you’re rolling a ball down a hill. Nesterov Momentum is like looking ahead slightly in the direction you’re rolling before making a decision about the next step. This helps you avoid sharp turns and find a smoother path to the bottom. AdaGrad Imagine you’re walking on a terrain with different types of surfaces. AdaGrad is like adjusting your step size based on how rough or smooth the terrain is under each foot. You take smaller steps on rough terrain to avoid stumbling and larger steps on smooth terrain to move faster. Key Takeaways:\nSGD+Momentum, RMSprop, Adam, and AdamW are all improvements upon the basic Gradient Descent algorithm. They address the limitations of GD and SGD by introducing momentum, adaptive learning rates, and regularization. The choice of optimization algorithm depends on the specific problem and dataset. Learning rate schedules\nlearning rate scheduling is about how to adjust the learning rate of your optimization algorithm (like SGD, Adam, etc.) during training to improve performance.\nThe Basic Idea:\nEarly Stages: You usually start with a relatively high learning rate to quickly find a good region in the parameter space (the space of all possible model weights). Later Stages: As training progresses, you often want to decrease the learning rate. This helps the model fine-tune its parameters and avoid overshooting the optimal solution. Learning Rate Schedules:\nCommon learning rate schedules:\nThe Step: The image suggests a simple approach: reduce the learning rate by a fixed factor (e.g., 0.1) at specific epochs. This is often done for ResNet models, where the learning rate is multiplied by 0.1 after epochs 30, 60, and 90.\nCosine: The learning rate follows a cosine curve, decreasing gradually from the initial learning rate to zero. This schedule often provides a smooth transition and avoids abrupt changes in the learning rate.\nLinear: The learning rate decreases linearly from the initial learning rate to zero. This schedule is simpler to implement but might not be as effective as the cosine schedule.\nInverse Square Root: The learning rate decreases inversely proportional to the square root of the epoch number. This schedule is often used in deep learning, as it provides a gradual decay that is not too fast or too slow.\nLinear Warmup: A warmup period where the learning rate increases linearly from zero to the initial learning rate. This helps the model to start with a small learning rate and gradually increase it as the training progresses.\nLinear warmup is the solution for **High Initial Learning Rates: **Using a high initial learning rate can cause the training process to become unstable. The loss function might explode (increase rapidly) instead of decreasing, making the model difficult to train. Key Terms:\nα₀: Initial learning rate (the starting learning rate). αₜ: Learning rate at epoch t (the learning rate at a specific training epoch). T: Total number of epochs (the total number of times you go through the entire training dataset). Why Use Learning Rate Scheduling?\nImproved Convergence: It helps the model converge to a better solution by adjusting the learning rate based on the training progress. Reduced Oscillations: It can prevent the model from oscillating around the optimal solution, especially in the later stages of training. Better Generalization: It can help the model generalize better to unseen data by preventing overfitting. In practice：\nAdam(W) is a good default choice in many cases; it often works ok even with constant learning rate SGD with Momentum can outperform Adam but may require more tuning of LR and schedule If you can afford to do full batch updates then look beyond 1^st order optimization (2^nd order and beyond), Try out L-BFGS The Problem:\nHessian Inversion: The Hessian matrix has a size proportional to the square of the number of parameters (N^2) in the model, the Newton’s method update rule requires inverting the Hessian matrix, which has a computational complexity of O(N^3) (N being the number of parameters). which becomes prohibitively expensive for deep learning models with billions or trillions of parameters. The Quasi-Newton Approach:\nApproximation: Quasi-Newton methods avoid directly inverting the Hessian. Instead, they build an approximation of the inverse Hessian using rank-1 updates over time. This approximation is much more efficient to compute, with a complexity of O(N^2). BFGS (Broyden–Fletcher–Goldfarb–Shanno): The most popular Quasi-Newton method is the BFGS algorithm. It iteratively updates the approximation of the inverse Hessian based on the current gradient and the previous gradient. L-BFGS (Limited Memory BFGS):\nMemory Efficiency: L-BFGS is a variation of BFGS that is even more efficient in terms of memory usage. It doesn’t store the full inverse Hessian matrix. Instead, it keeps track of a limited number of past gradient and parameter updates. This makes it suitable for very large models where storing the entire Hessian would be impractical. Key Advantages:\nComputational Efficiency: Quasi-Newton methods are significantly faster than Newton’s method, especially for large models. Accuracy: They often achieve better accuracy than first-order methods like SGD or Momentum, as they incorporate information about the curvature of the loss function. In Summary:\nQuasi-Newton methods offer a practical compromise between the accuracy of second-order optimization and the computational efficiency of first-order methods. BFGS and L-BFGS are popular algorithms that effectively approximate the inverse Hessian, making them suitable for training large deep learning models. The Alternative: First-Order Methods: First-order methods like SGD, Adam, etc., are computationally much cheaper because they only require calculating the gradient, which has a complexity of O(N). While they might not be as accurate as second-order methods, their efficiency makes them the preferred choice for training large deep learning models. Lecture 4: Neural Networks and Backpropagation\nMulti-layer Perceptron, (Fully Connected) Neural Networks Backpropagation Let’s break down Neural Networks and Backpropagation: 1. Neural Networks: The Basics\nImagine a network of interconnected nodes, like a simplified model of the human brain. These nodes, called “neurons,” process information and pass it along to other neurons. Here’s how it works:\nInput Layer: This layer receives the raw data, like pixels in an image or words in a sentence. Hidden Layers: These layers are the “thinking” part of the network. They use mathematical functions to transform the input data, extracting features and patterns. Output Layer: This layer produces the final prediction, like a classification label or a numerical value. Key Concepts:\nWeights: Each connection between neurons has a “weight” associated with it. These weights determine the strength of the connection and influence how information is passed along. Activation Functions: Each neuron applies a “non-linear” function to its input, adding complexity and allowing the network to learn intricate relationships. Learning: The network learns by adjusting its weights based on the feedback it receives. The goal is to minimize the difference between the network’s predictions and the actual values. Why are they useful?\nNeural networks are powerful because they can learn complex patterns from data. They excel at tasks like:\nImage Recognition: Identifying objects in images. Natural Language Processing: Understanding and generating text. Machine Translation: Translating text from one language to another. Speech Recognition: Converting spoken words into text. 2. Backpropagation: The Learning Engine\nBackpropagation is the algorithm that allows neural networks to learn. It’s a clever way to calculate how much each weight in the network contributes to the error in the output. Think of it like this:\nForward Pass: Input data is fed through the network, and the output is generated. Error Calculation: The difference between the predicted output and the actual output is calculated. Backward Pass: The error is “propagated” backwards through the network, adjusting the weights along the way. How it works (simplified):\nCalculate the error: How far off was the network’s prediction? Find the “blame”: Which weights contributed most to the error? Adjust the weights: Slightly change the weights to reduce the error. Key Points:\nGradient Descent: Backpropagation uses a mathematical technique called gradient descent to find the optimal weights. Chain Rule: Backpropagation relies on the chain rule of calculus to calculate the gradients efficiently. Iterative Process: The network learns through many iterations of forward and backward passes, gradually improving its accuracy. In a nutshell:\nNeural networks are powerful tools for learning complex patterns from data. Backpropagation is the algorithm that enables them to learn by adjusting their internal weights based on the error in their predictions.\nPerceiving and Understanding the Visual World Lecture 5: Image Classification with CNNs History Higher-level representations, image features Convolution and pooling Lecture 6: CNN Architectures Batch Normalization Transfer learning AlexNet, VGG, GoogLeNet, ResNet Lecture 7: Recurrent Neural Networks RNN, LSTM, GRU Language modeling Image captioning Sequence-to-sequence Lecture 8: Attention and Transformers Self-Attention Transformers Lecture 9: Object Detection and Image Segmentation Single-stage detectors Two-stage detectors Semantic/Instance/Panoptic segmentation Lecture 10: Video Understanding Video classification 3D CNNs Two-stream networks Multimodal video understanding Lecture 11: Visualizing and Understanding Feature visualization and inversion Adversarial examples DeepDream and style transfer Generative and Interactive Visual Intelligence Lecture 12: Self-supervised Learning Pretext tasks Contrastive learning Multisensory supervision Lecture 13: Generative Models Generative Adversarial Network Diffusion models Autoregressive models Lecture 14: OpenAI Sora Diffusion models Lecture 15: Robot Learning Deep Reinforcement Learning Model Learning Robotic Manipulation Lecture 16: Human-Centered Artificial Intelligence Lecture 17: Guest Lecture by Prof. Serena Yeung-Levy Lecture 18: 3D Vision 3D shape representations Shape reconstruction Neural implicit representations ","wordCount":"4988","inLanguage":"en","image":"http://localhost:2815/favicon.ico","datePublished":"2024-05-06T16:30:19+08:00","dateModified":"2024-05-06T16:30:19+08:00","author":{"@type":"Person","name":"Bo Liu"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:2815/posts/cs231-copy/"},"publisher":{"@type":"Organization","name":"Bo's Log | What I cannot create, I do not understand","logo":{"@type":"ImageObject","url":"http://localhost:2815/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:2815/ accesskey=h title="Bo's Log (Alt + H)"><img src=http://localhost:2815/apple-touch-icon.png alt aria-label=logo height=35>Bo's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:2815/ title=Home><span>Home</span></a></li><li><a href=http://localhost:2815/posts/ title=Posts><span>Posts</span></a></li><li><a href=http://localhost:2815/projects/ title=Projects><span>Projects</span></a></li><li><a href=http://localhost:2815/experience/ title=Experience><span>Experience</span></a></li><li><a href=http://localhost:2815/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=http://localhost:2815/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:2815/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:2815/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Notes for CS231n: Deep Learning for Computer Vision</h1><div class=post-meta><span title='2024-05-06 16:30:19 +0800 CST'>May 6, 2024</span>&nbsp;·&nbsp;24 min&nbsp;·&nbsp;4988 words&nbsp;·&nbsp;Bo Liu&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/cs231%20copy.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#assignments>Assignments</a></li><li><a href=#deep-learning-basics>Deep Learning Basics</a><ul><li><ul><li><a href=#q1-what-happens-to-loss-if-car-scores-decrease-by-05-for-this-training-example>Q1: What happens to loss if car scores decrease by 0.5 for this training example?</a></li><li><a href=#q2-what-is-the-minmax-possible-svm-loss--l_i->Q2: What is the min/max possible SVM loss ?</a></li><li><a href=#q3-at-initialization--w--is-small-so-all--s-approx-0--what-is-the-loss--l_i--assuming--n--examples-and--c--classes>Q3: At initialization is small so all . What is the loss , assuming examples and classes?</a></li><li><a href=#q4-what-if-the-sum-was-over-all-classes-including--j--y_i->Q4: What if the sum was over all classes (including )?</a></li><li><a href=#q1-what-is-the-minmax-possible-softmax-loss--l_i->Q1: What is the min/max possible softmax loss ?</a></li><li><a href=#q2-at-initialization-all--s_j--will-be-approximately-equal-what-is-the-softmax-loss--l_i--assuming--c--classes>Q2: At initialization, all will be approximately equal; what is the softmax loss , assuming classes?</a></li><li><a href=#q3-if-all-scores-are-small-random-values-what-is-the-loss>Q3: If all scores are small random values, what is the loss?</a></li><li><a href=#q-what-is-cross-entropy-loss-what-is-svm-loss>Q: What is cross-entropy loss? What is SVM loss?</a></li><li><a href=#q-what-happens-to-each-loss-if-i-slightly-change-the-scores-of-the-last-datapoint>Q: What happens to each loss if I slightly change the scores of the last datapoint?</a></li></ul></li></ul></li><li><a href=#perceiving-and-understanding-the-visual-world>Perceiving and Understanding the Visual World</a></li><li><a href=#generative-and-interactive-visual-intelligence>Generative and Interactive Visual Intelligence</a></li></ul></nav></div></details></div><div class=post-content><h1 id=assignments>Assignments<a hidden class=anchor aria-hidden=true href=#assignments>#</a></h1><ul><li><input disabled type=checkbox> Assignment #1
Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network</li><li><input disabled type=checkbox> Assignment #2
Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch & Network Visualization</li><li><input disabled type=checkbox> Assignment #3
Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning</li></ul><h1 id=deep-learning-basics>Deep Learning Basics<a hidden class=anchor aria-hidden=true href=#deep-learning-basics>#</a></h1><ul><li><p><input checked disabled type=checkbox> Lecture 1: Computer vision overview</p></li><li><p><input disabled type=checkbox> <strong>Lecture 2: Image Classification with linear Classifiers</strong></p><ul><li><p><input checked disabled type=checkbox> The data-driven approch
What is data-driven approaches linear classification & kNN means?
Data-Driven Approach: 1. Colloect a dataset of images and labels. 2. Use Machine Learning algorithms to train a classifier. 3. Evaluate the classifier on new images.
There is two basic data-driven aprroaches to image classification</p></li><li><p><input disabled type=checkbox> K-nearest neighbor (kNN)
What is kNN?</p><ul><li>kNN is a non-parametric algorithm, meaning it does&rsquo;t make any assumptions about the underlying data distribution. It doesn&rsquo;t build a model until a new instance is presented to it.
How does kNN work?</li></ul><ol><li>Data Preparation: The dataset is divided into a training set and a testing set(or validation set).
<img loading=lazy src=image-5.png alt="alt text">
<img loading=lazy src=image-6.png alt="alt text"></li><li>Distance Calculation: When a new instance(query point) is presented to the model, the algorithm calculates the distance between the new query point and each instance in the training set. The most common distance metrics used in kNN are Euclidean distance(L2 norm), Manhattan distance(L1 norm), and Minkowski distance(Lp norm).</li><li>K-nearest Neighbor Selection: The algorithm selects the K most similar instances (nearest neighbors) to the query point based on the calculated distances. The value of K is a hyperparameter that needs to be set.</li><li>Majority vote (classification): In classification problems, the algorithm assigns the query point to the class that is most common among its K nearest neighbors. This is done by taking a majority vote among the classes of the nearest neighbors.</li><li>Average value (regression): In regression problems, the algorithm predicts the value of the query point by taking the average of the values of its K nearest neighbors.</li><li>Prediction: The final prediction is made based on the majority vote (classification) or average value (regression).</li></ol><ul><li>Key takeways<ul><li>In image classification we start with a training set of images and labels, and must predict labels on the test set</li><li>The K-Nearest Neighbors classifier predicts labels based on the K nearest training examples</li><li>Distance metric and K are hyperparameters</li><li>Choose hyperparameters using the validation set</li><li>Only run on the test set once at the very end!</li><li><strong>K value</strong>: The choice of K is crucial, as it affects the performance of the algorithm. A small value of K can lead to overfitting, while a large value can lead to underfitting.</li><li><strong>Distance metric</strong>: The choice of distance metric can impact the performance of the algorithm, especially when dealing with high-dimensional data.</li><li>Computational complexity: KNN can be computationally expensive, especially for large datasets, since it requires calculating distances between all instances.</li></ul></li><li>Distance Metric</li><li>L1 (Manhattan)distance $$d_1(I_1,I_2) = \sum_{p} |I_1^p - I_2^p|$$</li><li>L2 (Euclidean) distance $$d_2(I_1,I_2) = \sqrt{\sum_{p} (I_1^p - I_2^p)^2}$$</li></ul></li><li><p><input disabled type=checkbox> Linear classifiers
What is a linear classifier?</p><ul><li>Linear classifier is a function $$f(x,W) = Wx + b$$</li><li>W is parameters or weights</li></ul></li><li><p><input disabled type=checkbox> Algebraic/Visual/ Geometric veiwpoints</p></li><li><p><input disabled type=checkbox> SVM loss
What is Multiclass SVM loss?</p><ul><li>Given an example $(x_i, y_i)$ where $x_i$ is the image and where $y_i$ is the (integer) label, and using the shorthand for the scores vector: $s=f(x_i, W)$</li><li>the SVM loss has the form:
$$
L_i = ∑_{j≠y_i}
\begin{cases}
0 & \text{if } s_{y_i} ≥ s_j + 1 \\
s_j - s_{y_i} + 1 & \text{otherwise}
      \end{cases}\\
= ∑_{j≠y_i} max(0, s_j - s_{y_i} + 1)
$$</li><li>$S_{y_i} - s_j$ is the difference in scores between correct and incorrect class
<img loading=lazy src=image-7.png alt="alt text"></li><li>calculation
<img loading=lazy src=image-8.png alt="alt text">
<img loading=lazy src=image-9.png alt="alt text"></li></ul><h3 id=q1-what-happens-to-loss-if-car-scores-decrease-by-05-for-this-training-example>Q1: What happens to loss if car scores decrease by 0.5 for this training example?<a hidden class=anchor aria-hidden=true href=#q1-what-happens-to-loss-if-car-scores-decrease-by-05-for-this-training-example>#</a></h3><p>To determine the effect of decreasing the car score by 0.5, we need to look at the multiclass SVM loss formula:</p><p>$ L*i = \sum*{j \neq y*i} \max(0, s_j - s*{y_i} + 1) $</p><p>For this training example, the scores are:</p><ul><li>cat: 1.3</li><li>car: 4.9</li><li>frog: 2.0</li></ul><p>Assuming the correct class is &ldquo;car&rdquo; (i.e., $ y_i = \text{car} $), the loss is calculated as:</p><p>$ L_i = \max(0, 1.3 - 4.9 + 1) + \max(0, 2.0 - 4.9 + 1) $
$ L_i = \max(0, -2.6) + \max(0, -1.9) $
$ L_i = 0 + 0 = 0 $</p><p>Now, if the car score decreases by 0.5, the new scores are:</p><ul><li>cat: 1.3</li><li>car: 4.4</li><li>frog: 2.0</li></ul><p>The new loss is:</p><p>$ L_i = \max(0, 1.3 - 4.4 + 1) + \max(0, 2.0 - 4.4 + 1) $
$ L_i = \max(0, -2.1) + \max(0, -1.4) $
$ L_i = 0 + 0 = 0 $</p><p>Thus, the loss remains the same at <strong>0</strong>.</p><h3 id=q2-what-is-the-minmax-possible-svm-loss--l_i->Q2: What is the min/max possible SVM loss $ L_i $?<a hidden class=anchor aria-hidden=true href=#q2-what-is-the-minmax-possible-svm-loss--l_i->#</a></h3><h4 id=minimum-svm-loss>Minimum SVM Loss:<a hidden class=anchor aria-hidden=true href=#minimum-svm-loss>#</a></h4><p>The minimum SVM loss occurs when the model perfectly classifies the example with a margin of at least 1. Hence, for all incorrect classes $ j $:</p><p>$ s*j - s*{y_i} + 1 \leq 0 $</p><p>In this case, the loss for these classes will be 0. Therefore, the minimum SVM loss $ L_i $ is <strong>0</strong>.</p><h4 id=maximum-svm-loss>Maximum SVM Loss:<a hidden class=anchor aria-hidden=true href=#maximum-svm-loss>#</a></h4><p>The maximum SVM loss occurs when the scores for all incorrect classes $ j $ are infinitely higher than the score for the correct class $ y_i $. As the scores for incorrect classes increase without bound, the loss also increases without bound. Therefore, the maximum SVM loss $ L_i $ is <strong>infinity</strong>.</p><h3 id=q3-at-initialization--w--is-small-so-all--s-approx-0--what-is-the-loss--l_i--assuming--n--examples-and--c--classes>Q3: At initialization $ W $ is small so all $ s \approx 0 $. What is the loss $ L_i $, assuming $ N $ examples and $ C $ classes?<a hidden class=anchor aria-hidden=true href=#q3-at-initialization--w--is-small-so-all--s-approx-0--what-is-the-loss--l_i--assuming--n--examples-and--c--classes>#</a></h3><p>At initialization, if all scores $ s \approx 0 $:</p><p>The multiclass SVM loss for a single example is:</p><p>$ L*i = \sum*{j \neq y*i} \max(0, s_j - s*{y_i} + 1) $</p><p>Since $ s_j \approx 0 $ for all $ j $:</p><p>$ L*i = \sum*{j \neq y*i} \max(0, 0 - 0 + 1) $
$ L_i = \sum*{j \neq y_i} 1 $
$ L_i = C - 1 $</p><p>(where $ C $ is the number of classes)</p><p>For $ N $ examples, the total loss is:</p><p>$ L = \sum*{i=1}^{N} L_i = \sum*{i=1}^{N} (C - 1) $
$ L = N(C - 1) $</p><p>So, the loss $ L_i $ for each example is $ C - 1 $, and the total loss for $ N $ examples is $ N(C - 1) $.</p><h3 id=q4-what-if-the-sum-was-over-all-classes-including--j--y_i->Q4: What if the sum was over all classes (including $ j = y_i $)?<a hidden class=anchor aria-hidden=true href=#q4-what-if-the-sum-was-over-all-classes-including--j--y_i->#</a></h3><p>If the sum was over all classes, including $ j = y_i $, the loss formula would be:</p><p>$ L*i = \sum*{j} \max(0, s*j - s*{y_i} + 1) $</p><p>Since the term for $ j = y_i $ is included, we have:</p><p>$ \max(0, s*{y_i} - s*{y_i} + 1) = \max(0, 1) = 1 $</p><p>So, the loss would be:</p><p>$ L*i = \sum*{j \neq y*i} \max(0, s_j - s*{y_i} + 1) + 1 $</p><p>Thus, it includes an additional $ 1 $ term compared to the standard SVM loss for each example.</p><ul><li><p><input disabled type=checkbox> Softmax loss
What is Softmax Classifier(Multinomial Logistic Regression) - Want to interpret raw classifier scores as probabilities</p>$$ s = f(x*i; W) P(Y=k|X=x_i) = \frac{e^{s_k}}{\sum*{j=1}^{K} e^{s*j}}$$</li><li><p>Softmax Function</p></li><li><p>$L_i = - log P(Y=y_i|X=x_i)$
<img loading=lazy src=image-10.png alt="alt text">
<img loading=lazy src=image-11.png alt="alt text">
<img loading=lazy src=image-12.png alt="alt text"></p></li><li><p>Kullback-Leibler divergence</p>$$ D*{KL}(P||Q) = \sum*{y\in \mathcal{Y}} P(y) log \frac{P(y)}{Q(y)}$$</li><li><p>Cross-Entropy</p>$$ H(P,Q)=H(p)+D*{KL}(P||Q)$$</li><li><p>Questions</p><h3 id=q1-what-is-the-minmax-possible-softmax-loss--l_i->Q1: What is the min/max possible softmax loss $ L_i $?<a hidden class=anchor aria-hidden=true href=#q1-what-is-the-minmax-possible-softmax-loss--l_i->#</a></h3><h4 id=minimum-softmax-loss>Minimum Softmax Loss:<a hidden class=anchor aria-hidden=true href=#minimum-softmax-loss>#</a></h4><p>The minimum softmax loss occurs when the model is perfectly confident and correct. In this case, the probability $ P(Y = y_i | X = x_i) $ for the correct class is 1. Therefore, the loss is:</p><p>$ L_i = -\log(1) = 0 $</p><p>Thus, the minimum possible softmax loss $ L_i $ is <strong>0</strong>.</p><h4 id=maximum-softmax-loss>Maximum Softmax Loss:<a hidden class=anchor aria-hidden=true href=#maximum-softmax-loss>#</a></h4><p>The maximum softmax loss occurs when the model is infinitely wrong, meaning it assigns a probability of 0 to the correct class. In practical terms, this happens when the score for the correct class is much lower than the scores for all other classes. As the probability approaches 0, the loss approaches infinity:</p><p>$ L_i = -\log(0) = \infty $</p><p>Thus, the maximum possible softmax loss $ L_i $ is <strong>infinity</strong>.</p><h3 id=q2-at-initialization-all--s_j--will-be-approximately-equal-what-is-the-softmax-loss--l_i--assuming--c--classes>Q2: At initialization, all $ s_j $ will be approximately equal; what is the softmax loss $ L_i $, assuming $ C $ classes?<a hidden class=anchor aria-hidden=true href=#q2-at-initialization-all--s_j--will-be-approximately-equal-what-is-the-softmax-loss--l_i--assuming--c--classes>#</a></h3><p>At initialization, if all scores $ s_j $ are approximately equal, the softmax probabilities for each class will be evenly distributed. For $ C $ classes, each class will have a probability of $ \frac{1}{C} $.</p><p>The softmax loss for the correct class $ y_i $ is:</p><p>$ L_i = -\log\left(\frac{1}{C}\right) = \log(C) $</p><p>Therefore, the softmax loss $ L_i $ at initialization, assuming $ C $ classes, is <strong>$ \log(C) $</strong>.</p><h3 id=q3-if-all-scores-are-small-random-values-what-is-the-loss>Q3: If all scores are small random values, what is the loss?<a hidden class=anchor aria-hidden=true href=#q3-if-all-scores-are-small-random-values-what-is-the-loss>#</a></h3>$$-log(\frac{1}{C})\\
log(10)\approx 2.3$$<ul><li>Cross-Entropy vs SVM Loss</li></ul><h3 id=q-what-is-cross-entropy-loss-what-is-svm-loss>Q: What is cross-entropy loss? What is SVM loss?<a hidden class=anchor aria-hidden=true href=#q-what-is-cross-entropy-loss-what-is-svm-loss>#</a></h3><p>Cross-entropy loss > 0, SVM loss = 0</p><h3 id=q-what-happens-to-each-loss-if-i-slightly-change-the-scores-of-the-last-datapoint>Q: What happens to each loss if I slightly change the scores of the last datapoint?<a hidden class=anchor aria-hidden=true href=#q-what-happens-to-each-loss-if-i-slightly-change-the-scores-of-the-last-datapoint>#</a></h3><p>Cross-entropy loss will change; SVM loss will stay the same</p><ul><li><p>Summary</p><ul><li>Softmax: $L_i = -\log(\frac{e^{s_{y_i}}}{\sum_j e^{s_j}})$</li><li><strong>Softmax</strong>: A type of loss function used for multi-class classification. It measures the negative log-likelihood of the correct class, encouraging the model to assign higher probabilities to the correct class.</li><li><strong>L_i</strong>: The loss for the i-th training example.</li><li><strong>s_j</strong>: The score for the j-th class.</li><li><strong>y_i</strong>: The true class label for the i-th training example.</li><li><strong>e^{s_j}</strong>: The exponential of the score for the j-th class.</li><li><strong>∑_j e^{s_j}</strong>: The sum of exponentials of scores for all classes.</li></ul><p>SVM:$L_i = \sum_{j \neq y_i} \max(0, s_j - s_{y_i} + 1)$</p><ul><li><strong>SVM</strong>: A type of loss function used for binary classification. It measures the hinge loss, which encourages the model to have a margin between the scores for the correct and incorrect classes.</li><li><strong>max(0, s_j - s_{y_i} + 1)</strong>: The hinge loss, which is zero if the score for the correct class is greater than the score for the incorrect class by at least 1, and otherwise is equal to the difference between the scores plus 1.
<img loading=lazy src=image-14.png alt="alt text">
<img loading=lazy src=image-13.png alt="alt text"></li></ul></li></ul></li></ul></li></ul></li><li><p><input checked disabled type=checkbox> <strong>Lecture 3: Regularization and Optimization</strong></p><ul><li><p><input checked disabled type=checkbox> Regularization</p><h4 id=what-is-regularization>What is regularization?<a hidden class=anchor aria-hidden=true href=#what-is-regularization>#</a></h4><p>Regularization pushes agiainst fitting the data too well so we dont&rsquo;t fir noise in the data</p><p>$L(W) = \frac{1}{N} \sum_{i=1}^{N} L_i(f(x_i, W), y_i) + \lambda R(W)$
Where:</p><p>L(W) is the overall loss function
N is the number of training examples
L_i is the loss function for the i-th training example
f(x_i, W) is the model&rsquo;s prediction for the i-th training example
y_i is the true label for the i-th training example
R(W) is the regularization term
λ is the regularization strength (hyperparameter)</p><p>L2 regularization: $R(W) = \sum_k \sum_l W_{k,l}^2$</p><ul><li><strong>L2 regularization</strong>: A type of regularization that penalizes the sum of squared weights. It encourages the model to have smaller weights, and likes to “spread out” the weights which can prevent overfitting.</li><li><strong>R(W)</strong>: The regularization term, which is added to the loss function to penalize complex models.</li><li><strong>W</strong>: The weight matrix of the model.</li><li><strong>k, l</strong>: Indices for the rows and columns of the weight matrix.</li><li><strong>W_{k,l}^2</strong>: The square of the weight at row k and column l.</li></ul><p>L1 regularization: $R(W) = \sum_k \sum_l |W_{k,l}|$</p><ul><li><strong>L1 regularization</strong>: A type of regularization that penalizes the sum of absolute values of weights. It encourages the model to have sparse weights, meaning that many weights are set to zero. This can help with feature selection and prevent overfitting.</li><li><strong>|W_{k,l}|</strong>: The absolute value of the weight at row k and column l.</li></ul><p>Elastic net (L1 + L2): $R(W) = \sum_k \sum_l \beta W_{k,l}^2 + |W_{k,l}|$</p><ul><li><strong>Elastic net</strong>: A type of regularization that combines L1 and L2 regularization. It encourages both sparsity and smaller weights.</li><li><strong>β</strong>: A hyperparameter that controls the balance between L1 and L2 regularization.</li></ul><p>Dropout, Batch normalization, Stochastic depth, fractinal pooling, etc</p><p>Why regularize?</p><ul><li>Express pregerences over weights</li><li>Make the model simple so it works on test data</li><li>Improve optimization by adding curvature</li></ul></li><li><p><input disabled type=checkbox> Stochastic Gradient Descent</p><p>Gradient descent is an iterative optimization algorithm used to find the minimum of a function. It&rsquo;s widely used in ML to train models by adjusting their parameters to minimize the loss function. Here&rsquo;s a breakdown:</p><p><strong>1. The Goal:</strong></p><ul><li>Imagine you&rsquo;re trying to find the lowest point in a valley. You don&rsquo;t know where it is, but you can see the slope of the ground around you.</li><li>In ML, the &ldquo;valley&rdquo; is represented by the loss function, which measures how well your model performs. The goal is to find the set of model parameters that minimize this loss.</li></ul><p><strong>2. How it Works:</strong></p><ul><li><strong>Start at a random point:</strong> You begin with an initial guess for the model parameters (like starting at a random spot in the valley).</li><li><strong>Calculate the gradient:</strong> The gradient is a vector that points in the direction of the steepest ascent of the function. In our valley analogy, it&rsquo;s like looking at the slope of the ground to see which direction is uphill.</li><li><strong>Take a step in the opposite direction:</strong> To move towards the minimum (the lowest point), you take a small step in the direction opposite to the gradient. This is like walking downhill.</li><li><strong>Repeat:</strong> You continue calculating the gradient and taking steps in the opposite direction until you reach a point where the gradient is close to zero. This indicates you&rsquo;ve found a local minimum (a point where the function is lower than its immediate neighbors).</li></ul><p><strong>3. Key Concepts:</strong></p><ul><li><strong>Learning rate:</strong> This parameter controls how big each step you take is. A large learning rate can lead to overshooting the minimum, while a small learning rate can make the optimization process very slow.</li><li><strong>Local vs. global minimum:</strong> Gradient descent can get stuck in a local minimum, which is a point where the function is lower than its neighbors, but not the absolute lowest point. Finding the global minimum is often difficult, but good initialization and other optimization techniques can help.</li><li><strong>Cost function:</strong> The function that measures how well your model performs. It&rsquo;s the function you&rsquo;re trying to minimize.</li></ul><p><strong>4. Applications in Machine Learning:</strong></p><ul><li><strong>Training neural networks:</strong> Gradient descent is the workhorse algorithm for training deep learning models.</li><li><strong>Linear regression:</strong> Finding the best line of fit for a dataset.</li><li><strong>Logistic regression:</strong> Classifying data points into different categories.</li><li><strong>Many other machine learning algorithms:</strong> Gradient descent is a versatile tool used in a wide range of machine learning tasks.</li></ul><p><strong>Visual Analogy:</strong></p><p>Imagine you&rsquo;re trying to find the lowest point in a valley. You can&rsquo;t see the entire valley, but you can see the slope of the ground around you. You start at a random point and take small steps in the direction that is downhill. You keep taking steps until you reach a point where the slope is close to zero, indicating that you&rsquo;ve found a low point in the valley.</p><p><strong>Stochastic Gradient Descent</strong>. It&rsquo;s a variation of the standard Gradient Descent algorithm, and it&rsquo;s widely used in machine learning, especially for training large models. Here&rsquo;s a breakdown:</p><p><strong>1. The Problem with Standard Gradient Descent:</strong></p><ul><li>Standard Gradient Descent calculates the gradient of the loss function using <em>all</em> training examples. This can be computationally expensive, especially when dealing with large datasets.</li></ul><p><strong>2. How SGD Solves the Problem:</strong></p><ul><li><strong>Stochasticity:</strong> Instead of using all training examples, SGD randomly selects a <strong>mini batch</strong> of examples (32 / 64 / 128 common
) to calculate the gradient.</li><li><strong>Faster Updates:</strong> Since it&rsquo;s working with a smaller subset of data, SGD can update the model parameters much faster than standard Gradient Descent.</li><li><strong>Noise:</strong> The randomness introduced by using mini-batches can actually help the algorithm escape local minima and find better solutions.</li></ul><p><strong>3. Key Concepts:</strong></p><ul><li><strong>Mini-batch size:</strong> This determines how many examples are used to calculate the gradient in each iteration. A larger mini-batch size reduces noise but slows down training.</li><li><strong>Epoch:</strong> One pass through the entire training dataset. SGD typically runs for multiple epochs.</li><li><strong>Learning rate:</strong> Controls the step size during parameter updates.</li></ul><p><strong>4. Advantages of SGD:</strong></p><ul><li><strong>Speed:</strong> Much faster than standard Gradient Descent for large datasets.</li><li><strong>Escape local minima:</strong> The noise introduced by mini-batches can help the algorithm find better solutions.</li><li><strong>Online learning:</strong> SGD can be used for online learning, where new data points arrive continuously.</li></ul><p><strong>5. Disadvantages of SGD:</strong></p><ul><li><strong>Noisy updates:</strong> The randomness can make the optimization process less stable.</li><li><strong>Learning rate tuning:</strong> Finding the optimal learning rate can be challenging.</li></ul><p><strong>6. Applications in Machine Learning:</strong></p><ul><li><strong>Training deep neural networks:</strong> SGD is the most common optimization algorithm used for training deep learning models.</li><li><strong>Natural language processing:</strong> Training language models like BERT and GPT-3.</li><li><strong>Computer vision:</strong> Training image classification and object detection models.</li></ul><p>In multiple dimensions, the <strong>gradient</strong> is the vector of (partial derivatives) along each dimension. The <strong>slope</strong> in any direction is the <strong>dot product</strong> of the direction with the gradient. The <strong>direction</strong> of steepest descent is the <strong>negative gradient</strong></p></li><li><p><input checked disabled type=checkbox> Momentum, SGD, SGD+Momentum, RMSProp, Adam, AdamW</p><p><strong>1. Gradient Descent (GD):</strong></p><ul><li>The foundation: GD iteratively updates model parameters (weights) to minimize a loss function. It calculates the gradient of the loss function with respect to the parameters and takes a step in the opposite direction of the gradient.</li><li>Issue: Can get stuck in <strong>local minima</strong>, especially in complex landscapes. It can also oscillate around the minimum if the learning rate is too high.</li></ul><p><strong>2. Stochastic Gradient Descent (SGD):</strong></p><ul><li>The speed boost: SGD uses a <em>random subset</em> of the training data (a mini-batch) to calculate the gradient in each iteration. This makes it significantly faster than GD, especially for large datasets.</li><li>Issue: The noisy updates from mini-batches can lead to oscillations and slow convergence.</li></ul><p><strong>3. SGD with Momentum (SGD+Momentum):</strong></p><ul><li>Smoother steps: Momentum introduces a &ldquo;velocity&rdquo; term that accumulates the gradient over multiple iterations. This helps the algorithm:<ul><li><strong>Escape local minima:</strong> The momentum term can help the algorithm &ldquo;roll over&rdquo; local minima.</li><li><strong>Reduce oscillations:</strong> It smooths out the updates, making convergence more stable.</li></ul></li></ul><p><strong>4. RMSprop (Root Mean Square Propagation):</strong></p><ul><li>Adaptive learning rates: RMSprop adapts the learning rate for each parameter individually. It uses a moving average of squared gradients to scale the learning rate.<ul><li><strong>Faster convergence:</strong> It can speed up convergence by adjusting the learning rate for each parameter based on the history of its gradients.</li><li><strong>Less sensitive to learning rate:</strong> It&rsquo;s more robust to the choice of learning rate than SGD or SGD+Momentum.</li></ul></li></ul><p><strong>5. Adam (Adaptive Moment Estimation):</strong></p><ul><li>Combining the best: Adam combines the momentum term (like SGD+Momentum) with adaptive learning rates (like RMSprop). It estimates the first and second moments of the gradients to adjust the learning rate for each parameter.<ul><li><strong>Efficient and robust:</strong> Adam is often considered a very effective and robust optimization algorithm.</li></ul></li></ul><p><strong>6. AdamW (Adam with Weight Decay):</strong></p><ul><li>Regularization boost: AdamW adds a weight decay term to the Adam optimizer. Weight decay helps prevent overfitting by penalizing large weights, similar to L2 regularization.<ul><li><strong>Improved generalization:</strong> AdamW often leads to better generalization performance compared to standard Adam.</li></ul></li></ul><p><strong>7. Nesterov Momentum:</strong></p><ul><li>Nesterov Momentum is an improvement over standard Momentum that looks &ldquo;ahead&rdquo; in the direction of the momentum before calculating the gradient. This allows it to anticipate the future direction of the parameter update and potentially avoid sharp turns or oscillations.<ul><li><strong>Faster Convergence:</strong> Nesterov Momentum often converges faster than standard Momentum, especially in situations where the loss function has many local minima.</li><li><strong>Smoother Updates:</strong> It helps to smooth out the updates, reducing oscillations and making convergence more stable.</li></ul></li></ul><p><strong>8. Adagrad:</strong></p><ul><li>AdaGrad adapts the learning rate for each parameter individually based on the history of its gradients. It accumulates the squared gradients for each parameter and uses this information to scale the learning rate.<ul><li><strong>Adaptive Learning Rates:</strong> AdaGrad automatically adjusts the learning rate for each parameter, making it more robust to different scales and magnitudes of gradients.</li><li><strong>Less Sensitive to Learning Rate:</strong> It&rsquo;s less sensitive to the choice of the initial learning rate than standard Gradient Descent or Momentum.</li><li>Learning Rate Decay: The learning rate can decay too quickly, especially in the later stages of training, leading to <strong>slow</strong> convergence.</li></ul></li></ul><p><strong>Analogy:</strong></p><p>Imagine you&rsquo;re trying to find the lowest point in a valley.</p><ul><li><strong>GD:</strong> You carefully measure the slope at your current location and take a small step downhill.</li><li><strong>SGD:</strong> You quickly glance at a few random spots in the valley and take a step based on the average slope you see.</li><li><strong>SGD+Momentum:</strong> You use a running average of the slopes you&rsquo;ve seen, allowing you to take larger steps downhill and escape small dips.</li><li><strong>RMSprop:</strong> You adjust your step size based on how steep the ground is in each direction, taking smaller steps in steeper areas.</li><li><strong>Adam:</strong> You combine the momentum and adaptive learning rate techniques for a more efficient and stable descent.</li><li><strong>AdamW:</strong> You add a mechanism to prevent you from straying too far from the center of the valley, helping you find a more general solution.</li><li><strong>Nesterov</strong> Imagine you&rsquo;re rolling a ball down a hill. Nesterov Momentum is like looking ahead slightly in the direction you&rsquo;re rolling before making a decision about the next step. This helps you avoid sharp turns and find a smoother path to the bottom.</li><li><strong>AdaGrad</strong> Imagine you&rsquo;re walking on a terrain with different types of surfaces. AdaGrad is like adjusting your step size based on how rough or smooth the terrain is under each foot. You take smaller steps on rough terrain to avoid stumbling and larger steps on smooth terrain to move faster.</li></ul><p><strong>Key Takeaways:</strong></p><ul><li><strong>SGD+Momentum, RMSprop, Adam, and AdamW are all improvements upon the basic Gradient Descent algorithm.</strong></li><li><strong>They address the limitations of GD and SGD by introducing momentum, adaptive learning rates, and regularization.</strong></li><li><strong>The choice of optimization algorithm depends on the specific problem and dataset.</strong></li></ul></li><li><p><input checked disabled type=checkbox> Learning rate schedules</p><p><strong>learning rate scheduling</strong> is about how to adjust the learning rate of your optimization algorithm (like SGD, Adam, etc.) during training to improve performance.</p><p><strong>The Basic Idea:</strong></p><ul><li><strong>Early Stages:</strong> You usually start with a relatively high learning rate to quickly find a good region in the parameter space (the space of all possible model weights).</li><li><strong>Later Stages:</strong> As training progresses, you often want to decrease the learning rate. This helps the model fine-tune its parameters and avoid overshooting the optimal solution.</li></ul><p><strong>Learning Rate Schedules:</strong></p><p>Common learning rate schedules:</p><ul><li><p><strong>The Step:</strong> The image suggests a simple approach: reduce the learning rate by a fixed factor (e.g., 0.1) at specific epochs. This is often done for ResNet models, where the learning rate is multiplied by 0.1 after epochs 30, 60, and 90.</p></li><li><p><strong>Cosine:</strong> The learning rate follows a cosine curve, decreasing gradually from the initial learning rate to zero. This schedule often provides a smooth transition and avoids abrupt changes in the learning rate.</p></li><li><p><strong>Linear:</strong> The learning rate decreases linearly from the initial learning rate to zero. This schedule is simpler to implement but might not be as effective as the cosine schedule.</p></li><li><p><strong>Inverse Square Root:</strong> The learning rate decreases inversely proportional to the square root of the epoch number. This schedule is often used in deep learning, as it provides a gradual decay that is not too fast or too slow.</p></li><li><p><strong>Linear Warmup:</strong> A warmup period where the learning rate increases linearly from zero to the initial learning rate. This helps the model to start with a small learning rate and gradually increase it as the training progresses.</p><ul><li>Linear warmup is the solution for **High Initial Learning Rates: **Using a high initial learning rate can cause the training process to become unstable. The loss function might explode (increase rapidly) instead of decreasing, making the model difficult to train.</li></ul></li></ul><p><strong>Key Terms:</strong></p><ul><li><strong>α₀:</strong> Initial learning rate (the starting learning rate).</li><li><strong>αₜ:</strong> Learning rate at epoch t (the learning rate at a specific training epoch).</li><li><strong>T:</strong> Total number of epochs (the total number of times you go through the entire training dataset).</li></ul><p><strong>Why Use Learning Rate Scheduling?</strong></p><ul><li><strong>Improved Convergence:</strong> It helps the model converge to a better solution by adjusting the learning rate based on the training progress.</li><li><strong>Reduced Oscillations:</strong> It can prevent the model from oscillating around the optimal solution, especially in the later stages of training.</li><li><strong>Better Generalization:</strong> It can help the model generalize better to unseen data by preventing overfitting.</li></ul></li><li><p>In practice：</p><ul><li><strong>Adam(W)</strong> is a good default choice in many cases; it often works ok even with constant learning rate</li><li><strong>SGD with Momentum</strong> can outperform Adam but may require more tuning of LR and schedule</li><li>If you can afford to do full batch updates then look beyond 1^st order optimization (2^nd order and beyond), Try out L-BFGS</li></ul><p><strong>The Problem:</strong></p><ul><li><strong>Hessian Inversion:</strong> The Hessian matrix has a size proportional to the square of the number of parameters (N^2) in the model, the Newton&rsquo;s method update rule requires inverting the Hessian matrix, which has a computational complexity of O(N^3) (N being the number of parameters). which becomes prohibitively expensive for deep learning models with billions or trillions of parameters.</li></ul><p><strong>The Quasi-Newton Approach:</strong></p><ul><li><strong>Approximation:</strong> Quasi-Newton methods avoid directly inverting the Hessian. Instead, they build an approximation of the inverse Hessian using rank-1 updates over time. This approximation is much more efficient to compute, with a complexity of O(N^2).</li><li><strong>BFGS (Broyden–Fletcher–Goldfarb–Shanno):</strong> The most popular Quasi-Newton method is the BFGS algorithm. It iteratively updates the approximation of the inverse Hessian based on the current gradient and the previous gradient.</li></ul><p><strong>L-BFGS (Limited Memory BFGS):</strong></p><ul><li><strong>Memory Efficiency:</strong> L-BFGS is a variation of BFGS that is even more efficient in terms of memory usage. It doesn&rsquo;t store the full inverse Hessian matrix. Instead, it keeps track of a limited number of past gradient and parameter updates. This makes it suitable for very large models where storing the entire Hessian would be impractical.</li></ul><p><strong>Key Advantages:</strong></p><ul><li><strong>Computational Efficiency:</strong> Quasi-Newton methods are significantly faster than Newton&rsquo;s method, especially for large models.</li><li><strong>Accuracy:</strong> They often achieve better accuracy than first-order methods like SGD or Momentum, as they incorporate information about the curvature of the loss function.</li></ul><p><strong>In Summary:</strong></p><ul><li>Quasi-Newton methods offer a practical compromise between the accuracy of second-order optimization and the computational efficiency of first-order methods.</li><li>BFGS and L-BFGS are popular algorithms that effectively approximate the inverse Hessian, making them suitable for training large deep learning models.</li><li>The Alternative:<ul><li>First-Order Methods: First-order methods like SGD, Adam, etc., are computationally much cheaper because they only require calculating the gradient, which has a complexity of O(N). While they might not be as accurate as second-order methods, their efficiency makes them the preferred choice for training large deep learning models.</li></ul></li></ul></li></ul></li><li><p><input checked disabled type=checkbox> <strong>Lecture 4: Neural Networks and Backpropagation</strong></p><ul><li><input checked disabled type=checkbox> Multi-layer Perceptron, (Fully Connected) Neural Networks</li><li><input checked disabled type=checkbox> Backpropagation
Let&rsquo;s break down Neural Networks and Backpropagation:</li></ul><p><strong>1. Neural Networks: The Basics</strong></p><p>Imagine a network of interconnected nodes, like a simplified model of the human brain. These nodes, called &ldquo;neurons,&rdquo; process information and pass it along to other neurons. Here&rsquo;s how it works:</p><ul><li><strong>Input Layer:</strong> This layer receives the raw data, like pixels in an image or words in a sentence.</li><li><strong>Hidden Layers:</strong> These layers are the &ldquo;thinking&rdquo; part of the network. They use mathematical functions to transform the input data, extracting features and patterns.</li><li><strong>Output Layer:</strong> This layer produces the final prediction, like a classification label or a numerical value.</li></ul><p><strong>Key Concepts:</strong></p><ul><li><strong>Weights:</strong> Each connection between neurons has a &ldquo;weight&rdquo; associated with it. These weights determine the strength of the connection and influence how information is passed along.</li><li><strong>Activation Functions:</strong> Each neuron applies a &ldquo;non-linear&rdquo; function to its input, adding complexity and allowing the network to learn intricate relationships.</li><li><strong>Learning:</strong> The network learns by adjusting its weights based on the feedback it receives. The goal is to minimize the difference between the network&rsquo;s predictions and the actual values.</li></ul><p><strong>Why are they useful?</strong></p><p>Neural networks are powerful because they can learn complex patterns from data. They excel at tasks like:</p><ul><li><strong>Image Recognition:</strong> Identifying objects in images.</li><li><strong>Natural Language Processing:</strong> Understanding and generating text.</li><li><strong>Machine Translation:</strong> Translating text from one language to another.</li><li><strong>Speech Recognition:</strong> Converting spoken words into text.</li></ul><p><strong>2. Backpropagation: The Learning Engine</strong></p><p>Backpropagation is the algorithm that allows neural networks to learn. It&rsquo;s a clever way to calculate how much each weight in the network contributes to the error in the output. Think of it like this:</p><ul><li><strong>Forward Pass:</strong> Input data is fed through the network, and the output is generated.</li><li><strong>Error Calculation:</strong> The difference between the predicted output and the actual output is calculated.</li><li><strong>Backward Pass:</strong> The error is &ldquo;propagated&rdquo; backwards through the network, adjusting the weights along the way.</li></ul><p><strong>How it works (simplified):</strong></p><ol><li><strong>Calculate the error:</strong> How far off was the network&rsquo;s prediction?</li><li><strong>Find the &ldquo;blame&rdquo;:</strong> Which weights contributed most to the error?</li><li><strong>Adjust the weights:</strong> Slightly change the weights to reduce the error.</li></ol><p><strong>Key Points:</strong></p><ul><li><strong>Gradient Descent:</strong> Backpropagation uses a mathematical technique called gradient descent to find the optimal weights.</li><li><strong>Chain Rule:</strong> Backpropagation relies on the chain rule of calculus to calculate the gradients efficiently.</li><li><strong>Iterative Process:</strong> The network learns through many iterations of forward and backward passes, gradually improving its accuracy.</li></ul><p><strong>In a nutshell:</strong></p><p>Neural networks are powerful tools for learning complex patterns from data. Backpropagation is the algorithm that enables them to learn by adjusting their internal weights based on the error in their predictions.</p></li></ul><h1 id=perceiving-and-understanding-the-visual-world>Perceiving and Understanding the Visual World<a hidden class=anchor aria-hidden=true href=#perceiving-and-understanding-the-visual-world>#</a></h1><ul><li><input disabled type=checkbox> <strong>Lecture 5: Image Classification with CNNs</strong><ul><li><input disabled type=checkbox> History</li><li><input disabled type=checkbox> Higher-level representations, image features</li><li><input disabled type=checkbox> Convolution and pooling</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 6: CNN Architectures</strong><ul><li><input disabled type=checkbox> Batch Normalization</li><li><input disabled type=checkbox> Transfer learning</li><li><input disabled type=checkbox> AlexNet, VGG, GoogLeNet, ResNet</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 7: Recurrent Neural Networks</strong><ul><li><input disabled type=checkbox> RNN, LSTM, GRU</li><li><input disabled type=checkbox> Language modeling</li><li><input disabled type=checkbox> Image captioning</li><li><input disabled type=checkbox> Sequence-to-sequence</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 8: Attention and Transformers</strong><ul><li><input disabled type=checkbox> Self-Attention</li><li><input disabled type=checkbox> Transformers</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 9: Object Detection and Image Segmentation</strong><ul><li><input disabled type=checkbox> Single-stage detectors</li><li><input disabled type=checkbox> Two-stage detectors</li><li><input disabled type=checkbox> Semantic/Instance/Panoptic segmentation</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 10: Video Understanding</strong><ul><li><input disabled type=checkbox> Video classification</li><li><input disabled type=checkbox> 3D CNNs</li><li><input disabled type=checkbox> Two-stream networks</li><li><input disabled type=checkbox> Multimodal video understanding</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 11: Visualizing and Understanding</strong><ul><li><input disabled type=checkbox> Feature visualization and inversion</li><li><input disabled type=checkbox> Adversarial examples</li><li><input disabled type=checkbox> DeepDream and style transfer</li></ul></li></ul><h1 id=generative-and-interactive-visual-intelligence>Generative and Interactive Visual Intelligence<a hidden class=anchor aria-hidden=true href=#generative-and-interactive-visual-intelligence>#</a></h1><ul><li><input disabled type=checkbox> <strong>Lecture 12: Self-supervised Learning</strong><ul><li><input disabled type=checkbox> Pretext tasks</li><li><input disabled type=checkbox> Contrastive learning</li><li><input disabled type=checkbox> Multisensory supervision</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 13: Generative Models</strong><ul><li><input disabled type=checkbox> Generative Adversarial Network</li><li><input disabled type=checkbox> Diffusion models</li><li><input disabled type=checkbox> Autoregressive models</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 14: OpenAI Sora</strong><ul><li><input disabled type=checkbox> Diffusion models</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 15: Robot Learning</strong><ul><li><input disabled type=checkbox> Deep Reinforcement Learning</li><li><input disabled type=checkbox> Model Learning</li><li><input disabled type=checkbox> Robotic Manipulation</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 16: Human-Centered Artificial Intelligence</strong></li><li><input disabled type=checkbox> <strong>Lecture 17: Guest Lecture by Prof. Serena Yeung-Levy</strong></li><li><input disabled type=checkbox> <strong>Lecture 18: 3D Vision</strong><ul><li><input disabled type=checkbox> 3D shape representations</li><li><input disabled type=checkbox> Shape reconstruction</li><li><input disabled type=checkbox> Neural implicit representations</li></ul></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=http://localhost:2815/posts/cs231/><span class=title>Next »</span><br><span>Notes for CS231n: Deep Learning for Computer Vision</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:2815/>Bo's Log | What I cannot create, I do not understand</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script></body></html>