<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>CS231_part1_quiz | Bo's Log | What I cannot create, I do not understand</title>
<meta name=keywords content><meta name=description content='I used Gemini-1.5-flash to generate all the quizzes and answers. Enjoy!"
It covers topics like dataset, linear classification, neural networks, activation functions, backpropagation, and loss functions, helping us understand neural network training.
Quiz: Image Classification
Multiple Choice:
What is the primary task of image classification?
a. To identify all the objects present in an image. b. To segment an image into different regions. c. To create a new image based on an input image.'><meta name=author content="Bo Liu"><link rel=canonical href=https://csc-bo.github.io/posts/cs231_part1_quiz/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://csc-bo.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://csc-bo.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://csc-bo.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://csc-bo.github.io/apple-touch-icon.png><link rel=mask-icon href=https://csc-bo.github.io/android-chrome-192x192.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://csc-bo.github.io/posts/cs231_part1_quiz/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="CS231_part1_quiz"><meta property="og:description" content='I used Gemini-1.5-flash to generate all the quizzes and answers. Enjoy!"
It covers topics like dataset, linear classification, neural networks, activation functions, backpropagation, and loss functions, helping us understand neural network training.
Quiz: Image Classification
Multiple Choice:
What is the primary task of image classification?
a. To identify all the objects present in an image. b. To segment an image into different regions. c. To create a new image based on an input image.'><meta property="og:type" content="article"><meta property="og:url" content="https://csc-bo.github.io/posts/cs231_part1_quiz/"><meta property="og:image" content="https://csc-bo.github.io/favicon.ico"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-21T11:59:27+08:00"><meta property="article:modified_time" content="2024-05-21T11:59:27+08:00"><meta property="og:site_name" content="Bo's Log"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://csc-bo.github.io/favicon.ico"><meta name=twitter:title content="CS231_part1_quiz"><meta name=twitter:description content='I used Gemini-1.5-flash to generate all the quizzes and answers. Enjoy!"
It covers topics like dataset, linear classification, neural networks, activation functions, backpropagation, and loss functions, helping us understand neural network training.
Quiz: Image Classification
Multiple Choice:
What is the primary task of image classification?
a. To identify all the objects present in an image. b. To segment an image into different regions. c. To create a new image based on an input image.'><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://csc-bo.github.io/posts/"},{"@type":"ListItem","position":2,"name":"CS231_part1_quiz","item":"https://csc-bo.github.io/posts/cs231_part1_quiz/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"CS231_part1_quiz","name":"CS231_part1_quiz","description":"I used Gemini-1.5-flash to generate all the quizzes and answers. Enjoy!\u0026quot;\nIt covers topics like dataset, linear classification, neural networks, activation functions, backpropagation, and loss functions, helping us understand neural network training.\nQuiz: Image Classification\nMultiple Choice:\nWhat is the primary task of image classification?\na. To identify all the objects present in an image. b. To segment an image into different regions. c. To create a new image based on an input image.","keywords":[],"articleBody":"I used Gemini-1.5-flash to generate all the quizzes and answers. Enjoy!\"\nIt covers topics like dataset, linear classification, neural networks, activation functions, backpropagation, and loss functions, helping us understand neural network training.\nQuiz: Image Classification\nMultiple Choice:\nWhat is the primary task of image classification?\na. To identify all the objects present in an image. b. To segment an image into different regions. c. To create a new image based on an input image. d. To assign a label from a fixed set of categories to an input image. What is the data-driven approach to image classification?\na. Manually specifying the features of each category in code. b. Providing the computer with many examples of each class and letting it learn from them. c. Using a predefined set of rules to classify images. d. Using a pre-trained model to classify images. What is a hyperparameter?\na. A parameter that is learned from the data during training. b. A parameter that is set before training and affects the model’s behavior. c. A parameter that is used to evaluate the model’s performance. d. A parameter that is used to visualize the model’s predictions. Why is it important to use a validation set when tuning hyperparameters?\na. To avoid overfitting to the test set. b. To ensure that the model is generalizing well to unseen data. c. To improve the model’s performance on the training set. d. To reduce the computational cost of training. What is the main drawback of the Nearest Neighbor classifier for image classification?\na. It is difficult to implement. b. It is computationally expensive to evaluate on a test image. c. It does not generalize well to unseen data. d. It is not robust to variations in image data. What is the L1 distance?\na. The sum of the absolute differences between two vectors. b. The square root of the sum of the squared differences between two vectors. c. The dot product of two vectors. d. The maximum difference between two vectors. What is the purpose of k-Nearest Neighbor classifier?\na. To find the single closest training example to a test example. b. To find the k closest training examples to a test example and have them vote on the label. c. To learn a linear model that separates the classes. d. To learn a non-linear model that separates the classes. What is cross-validation?\na. A technique for splitting the training data into folds and using different folds as validation sets to tune hyperparameters. b. A technique for evaluating the model’s performance on the test set. c. A technique for visualizing the model’s predictions. d. A technique for reducing the computational cost of training. Why are pixel-based distances often inadequate for comparing images?\na. They are computationally expensive. b. They do not capture perceptual or semantic similarity. c. They are not robust to variations in image data. d. They are not suitable for high-dimensional data. What is t-SNE?\na. A technique for reducing the dimensionality of data while preserving local distances. b. A technique for learning a linear model that separates the classes. c. A technique for evaluating the model’s performance. d. A technique for visualizing the model’s predictions. Fill-in-the-blank:\nThe image classification pipeline consists of three steps: input, __________, and evaluation.\nWhen choosing a dataset for image classification, it’s important to consider factors like its __________, __________, and __________.\nThe L2 distance is also known as the __________ distance.\nA higher value of k in k-Nearest Neighbor classifier has a __________ effect that makes the classifier more resistant to outliers.\nThe __________ set is used to tune hyperparameters, while the __________ set is used to evaluate the final model’s performance.\nA __________ layer in a Convolutional Neural Network (CNN) applies a filter to the input image to extract features.\n__________ is a technique used to prevent overfitting in CNNs by randomly dropping out units during training.\nShort Answer:\nBriefly describe two challenges of image classification.\nExplain the difference between L1 and L2 distances in the context of image classification.\nWhat is the purpose of a validation set in hyperparameter tuning?\nWhy is the Nearest Neighbor classifier often not a good choice for image classification?\nWhat are some advantages of using t-SNE for visualizing high-dimensional data?\nWhat are some common evaluation metrics used in image classification besides accuracy?\nExplain the concept of overfitting in image classification.\nBriefly describe the role of feature extraction in image classification.\nWhat are some popular feature extraction techniques used in image classification?\nHow do convolutional layers work in a CNN?\nWhat are some real-world applications of image classification?\nExplain how a CNN can learn to recognize different objects in an image.\nWhat are some of the limitations of CNNs?\nAnswer Key:\nd b b a b a b a b a learning size, diversity, and relevance to the task. Euclidean smoothing validation, test Convolutional Dropout Two challenges of image classification are: Viewpoint variation: Objects can be oriented in many ways with respect to the camera. Illumination conditions: Lighting can drastically affect the appearance of an object. L1 distance is the sum of absolute differences between two vectors, while L2 distance is the square root of the sum of squared differences. L1 is more forgiving to outliers, while L2 is more sensitive to large differences. A validation set is used to tune hyperparameters to prevent overfitting to the training set and ensure the model generalizes well to unseen data. The Nearest Neighbor classifier is often not a good choice for image classification because it is computationally expensive to evaluate on a test image and it does not generalize well to unseen data. Advantages of using t-SNE for visualizing high-dimensional data include: Reducing the dimensionality of data while preserving local distances. Creating visually appealing and informative representations of complex data. Helping to identify clusters and patterns in the data. Some common evaluation metrics used in image classification besides accuracy include: Precision Recall F1-score Loss functions (e.g., cross-entropy loss) Overfitting in image classification occurs when a model learns the training data too well and fails to generalize to unseen data. This can happen if the model is too complex or if the training data is not representative of the real-world data. Feature extraction in image classification involves extracting meaningful features from images that can be used to distinguish between different classes. These features can be based on various aspects of the image, such as shape, texture, color, or edges. Some popular feature extraction techniques used in image classification include: SIFT (Scale-Invariant Feature Transform) HOG (Histogram of Oriented Gradients) Color histograms Edge detectors Convolutional layers in a CNN apply a filter to the input image, sliding it across the image and performing a dot product at each location. This process extracts features from the image, such as edges, textures, and patterns. Some real-world applications of image classification include: Object detection in self-driving cars Facial recognition in security systems Medical image analysis for disease diagnosis Image retrieval in search engines Content moderation on social media platforms A CNN learns to recognize different objects in an image by using convolutional layers to extract features, pooling layers to reduce dimensionality, and fully connected layers to classify the features into different categories. The network learns the weights of these layers during training by minimizing a loss function that measures the difference between the predicted and actual labels. Some limitations of CNNs include: They can be computationally expensive to train and deploy. They may struggle with recognizing objects that are not present in the training data. They can be susceptible to adversarial attacks, where small changes to the input image can drastically change the output. CS231n Linear Classification Quiz Instructions: Answer the following questions based on the provided content from the CS231n website.\n1. What are the two major components of the approach to image classification discussed in the article?\n2. What is the simplest form of the score function used in linear classification, and what are its parameters?\n3. Explain the bias trick and how it simplifies the score function.\n4. What is the purpose of the loss function in image classification?\n5. Describe the Multiclass Support Vector Machine (SVM) loss function. What is the role of the margin (Δ) in this loss function?\n6. What is the purpose of regularization in the SVM loss function?\n7. What is the difference between the hinge loss and the squared hinge loss?\n8. How does the magnitude of the weights (W) affect the score differences?\n9. Explain the relationship between the hyperparameters Δ and λ in the SVM loss function.\n10. How can the linear classifier be interpreted as template matching?\nBonus Question: What is the main advantage of using a linear classifier over the k-Nearest Neighbor (kNN) classifier?\nAnswer Key:\n1. The two major components are: * Score function: Maps raw image pixels to class scores. * Loss function: Quantifies the agreement between predicted scores and ground truth labels.\n2. The simplest score function is a linear mapping: * \\(f(x_i, W, b) = W x_i + b\\) * Parameters: * W: Weight matrix (size [K x D]) * b: Bias vector (size [K x 1])\n3. The bias trick combines the weight matrix (W) and bias vector (b) into a single matrix. It involves adding an extra dimension to the input vector \\(x_i\\) that always holds the constant 1 (bias dimension). This allows the score function to be expressed as a single matrix multiplication: * \\(f(x_i, W) = W x_i\\)\n4. The loss function measures how well the predicted scores from the score function match the ground truth labels in the training data. A lower loss indicates better classification performance.\n5. The Multiclass SVM loss aims to ensure that the correct class score is higher than the incorrect class scores by a fixed margin (Δ). It calculates the loss for each example as the sum of the maximum of zero and the difference between the score of each incorrect class and the score of the correct class, minus the margin.\n6. Regularization in the SVM loss function aims to prevent overfitting by penalizing large weights. It encourages the classifier to consider all input dimensions to small amounts rather than a few dimensions with very strong influence.\n7. Both hinge loss and squared hinge loss are used in SVMs. The hinge loss penalizes violated margins linearly, while the squared hinge loss penalizes them quadratically, meaning it penalizes larger violations more strongly.\n8. Larger weights lead to larger score differences, while smaller weights lead to smaller score differences.\n9. Both Δ and λ control the trade-off between the data loss and the regularization loss. While Δ appears to control the margin, the actual trade-off is determined by the magnitude of the weights, which is influenced by λ.\n10. Each row of the weight matrix (W) can be interpreted as a template for a specific class. The score for each class is calculated by comparing the image to each template using an inner product. This process is similar to template matching, where the templates are learned from the training data.\nBonus Question: The main advantage of using a linear classifier over kNN is that it is much faster for classifying new images. It only requires a single matrix multiplication and addition, while kNN needs to compare the test image to all training images. Additionally, linear classifiers can be trained efficiently and can be used to classify new images without storing the entire training set.\nCS231n Optimization Quiz: This quiz is based on the content of the CS231n Optimization section (https://cs231n.github.io/optimization-1/).\nMultiple Choice:\nWhat is the goal of optimization in the context of machine learning?\na. To find the best set of parameters that minimize the loss function. b. To maximize the accuracy of the model on the training data. c. To find the most complex model that can fit the training data perfectly. d. To ensure that the model generalizes well to unseen data. Which of the following is NOT a strategy for optimization?\na. Random Search b. Random Local Search c. Following the Gradient d. Backpropagation What is the main advantage of using the gradient to optimize the loss function?\na. It is computationally less expensive than random search. b. It guarantees finding the global minimum of the loss function. c. It is more accurate than numerical gradient computation. d. It can be easily implemented without calculus. What is the difference between numerical gradient computation and analytic gradient computation?\na. Numerical gradient is approximate, while analytic gradient is exact. b. Numerical gradient is faster, while analytic gradient is slower. c. Numerical gradient requires calculus, while analytic gradient does not. d. Numerical gradient is more error-prone, while analytic gradient is less error-prone. What is the term for the process of repeatedly evaluating the gradient and updating the parameters?\na. Backpropagation b. Gradient Descent c. Stochastic Gradient Descent d. Mini-batch Gradient Descent What is the main advantage of using mini-batch gradient descent over full gradient descent?\na. It is computationally less expensive. b. It is more accurate. c. It is less prone to getting stuck in local minima. d. It is easier to implement. What is the term for using a single example to compute the gradient in gradient descent?\na. Full gradient descent b. Mini-batch gradient descent c. Stochastic gradient descent d. Batch gradient descent True or False:\nThe SVM loss function is a convex function. The numerical gradient computation is always more accurate than the analytic gradient computation. The step size (learning rate) is a hyperparameter that needs to be carefully tuned. The size of the mini-batch is usually set to a power of 2 for computational efficiency. Backpropagation is a technique used to compute the gradient of the loss function. Short Answer:\nBriefly explain the blindfolded hiker analogy for optimization. What are the two main components of the loss function? Why is it important to check the correctness of the analytic gradient computation? Answer Key:\nMultiple Choice:\na To find the best set of parameters that minimize the loss function. d Backpropagation a It is computationally less expensive than random search. a Numerical gradient is approximate, while analytic gradient is exact. b Gradient Descent a It is computationally less expensive. c Stochastic gradient descent True or False:\nTrue False True True True Short Answer:\nThe blindfolded hiker analogy compares the optimization process to a blindfolded hiker trying to find the bottom of a hilly terrain. The hiker represents the optimization algorithm, the terrain represents the loss function, and the height of the terrain represents the loss value. The goal is to find the lowest point on the terrain, which corresponds to the minimum loss value.\nThe two main components of the loss function are the data loss and the regularization loss. The data loss measures how well the model’s predictions match the ground truth labels, while the regularization loss penalizes complex models and helps prevent overfitting.\nIt is important to check the correctness of the analytic gradient computation because it is more error-prone to implement than the numerical gradient. By comparing the analytic gradient to the numerical gradient, we can ensure that our implementation is correct and avoid errors in the optimization process.\nCS231n Optimization 2 Quiz This quiz covers the concepts discussed in the CS231n course page on backpropagation: https://cs231n.github.io/optimization-2/\nInstructions: Answer the following questions to the best of your ability.\nMultiple Choice:\nWhat is the primary reason we are interested in computing gradients in the context of neural networks?\na. To understand the sensitivity of the loss function to its inputs. b. To update the weights and biases of the network. c. To visualize the network’s internal representations. d. All of the above. What does the derivative of a function tell us?\na. The rate of change of the function at a specific point. b. The slope of the function at a specific point. c. The sensitivity of the function to small changes in its input. d. All of the above. What is the chain rule used for in backpropagation?\na. To compute the gradient of a composite function. b. To propagate gradients backwards through the circuit. c. To multiply local gradients to compute the overall gradient. d. All of the above. Which of the following gates has a local gradient of +1.0 for all its inputs?\na. Add gate b. Multiply gate c. Max gate d. Sigmoid gate What is the key difference between the add gate and the max gate in terms of how they distribute gradients?\na. The add gate distributes the gradient equally to all inputs, while the max gate routes the gradient to the highest input. b. The add gate routes the gradient to the highest input, while the max gate distributes the gradient equally to all inputs. c. The add gate distributes the gradient proportionally to the input values, while the max gate routes the gradient to the highest input. d. The add gate routes the gradient to the highest input, while the max gate distributes the gradient proportionally to the input values. Why is it important to use += instead of = when accumulating gradients in backpropagation?\na. To prevent overwriting the gradient on variables that are used multiple times in the forward pass. b. To ensure that the gradient is propagated correctly through the circuit. c. To account for the fact that gradients add up at forks in the circuit. d. All of the above. What is the main challenge in computing the gradient for matrix-matrix multiplication?\na. The need to use the chain rule multiple times. b. The need to handle the transpose operation correctly. c. The need to account for the dimensions of the matrices. d. All of the above. Short Answer:\nExplain the concept of “backward flow” in backpropagation. Describe how the scale of input data can affect the magnitude of gradients in a linear classifier. Briefly explain the importance of staged computation in backpropagation. Bonus:\nDerive the gradient of the sigmoid function with respect to its input. Write Python code to implement the forward and backward pass for the following function: $f(x,y) = (x + y)^2 / (x * y)$ This quiz is designed to test your understanding of the concepts covered in the CS231n course page on backpropagation. Good luck!\nHere’s a quiz based on the CS231n content you provided, covering the key concepts discussed:https://cs231n.github.io/neural-networks-1/\nMultiple Choice:\nWhich of the following is NOT a commonly used activation function in neural networks?\na. Sigmoid b. Tanh c. ReLU d. Exponential e. Leaky ReLU What is the primary advantage of using the ReLU activation function over the sigmoid function?\na. ReLU outputs are zero-centered. b. ReLU does not suffer from saturation. c. ReLU is computationally more efficient. d. Both b and c e. All of the above What does the universal approximation theorem state about neural networks?\na. Neural networks can approximate any function with a finite number of neurons. b. Neural networks with at least one hidden layer can approximate any continuous function. c. Neural networks are always better than other machine learning models. d. Neural networks are the only type of model that can achieve perfect accuracy. Which of the following is NOT a factor to consider when setting the number of layers and their sizes in a neural network?\na. The complexity of the task b. The amount of training data available c. The computational resources available d. The type of activation function used e. The number of parameters in the network What is the primary function of the output layer in a neural network?\na. To perform the final non-linear transformation. b. To represent the class scores or target values. c. To provide the input to the next hidden layer. d. To determine the learning rate for the network. True/False:\nA single neuron can be used to implement a binary classifier. (True/False)\nThe tanh activation function is a scaled version of the sigmoid function. (True/False)\nThe “dying ReLU” problem occurs when a ReLU neuron becomes permanently inactive during training. (True/False)\nNeural networks with more layers are always better than networks with fewer layers. (True/False)\nRegularization techniques are used to prevent overfitting in neural networks. (True/False)\nAnswers:\nd. Exponential d. Both b and c b. Neural networks with at least one hidden layer can approximate any continuous function. e. The number of parameters in the network b. To represent the class scores or target values. True True True False True Bonus Question:\nExplain the concept of “gradual forgetting” in the context of biological neurons and how it relates to regularization in neural networks.\nThis quiz covers the key concepts from the provided text, including activation functions, network architectures, representational power, and the role of regularization. It should help you assess your understanding of these fundamental concepts in neural networks.\nCS231n Neural Networks: Quiz Instructions: Choose the best answer for each question.\n1. What is the most common form of data preprocessing for neural networks?\na. Normalization b. Whitening c. Mean subtraction d. PCA 2. Why is it important to initialize weights with small random numbers instead of all zeros?\na. To prevent overfitting b. To ensure all neurons have the same output distribution c. To break symmetry and allow neurons to learn different features d. To speed up convergence 3. What is the purpose of Batch Normalization?\na. To reduce the number of parameters in a network b. To prevent overfitting c. To force activations to have a unit gaussian distribution d. To improve the performance of dropout 4. Which regularization technique encourages sparse weight vectors?\na. L2 regularization b. L1 regularization c. Max norm constraints d. Dropout 5. What is the main idea behind dropout?\na. To randomly drop connections between neurons during training b. To randomly drop neurons during training c. To randomly drop data points during training d. To randomly drop layers during training 6. What is the purpose of scaling activations by ‘p’ during testing with dropout?\na. To prevent overfitting b. To ensure the outputs of neurons at test time are similar to their expected outputs at training time c. To speed up convergence d. To reduce the number of parameters in a network 7. Which of the following is NOT a common pitfall in data preprocessing?\na. Computing the mean across the entire dataset before splitting into train/val/test sets b. Using the same preprocessing parameters for train, validation, and test sets c. Not normalizing the data d. Using PCA for dimensionality reduction 8. What is the recommended initialization for ReLU neurons?\na. w = np.random.randn(n) / sqrt(n) b. w = np.random.randn(n) * sqrt(2.0/n) c. w = np.random.randn(n) * 0.01 d. w = np.zeros(n) 9. Which of the following is a common way to control the capacity of a neural network?\na. Batch Normalization b. Weight Initialization c. Regularization d. All of the above 10. What is the main difference between vanilla dropout and inverted dropout?\na. Vanilla dropout scales activations at test time, while inverted dropout scales them at train time. b. Vanilla dropout drops connections, while inverted dropout drops neurons. c. Vanilla dropout is more effective than inverted dropout. d. Vanilla dropout is easier to implement than inverted dropout. Answer Key:\nc. Mean subtraction c. To break symmetry and allow neurons to learn different features c. To force activations to have a unit gaussian distribution b. L1 regularization b. To randomly drop neurons during training b. To ensure the outputs of neurons at test time are similar to their expected outputs at training time b. Using the same preprocessing parameters for train, validation, and test sets b. w = np.random.randn(n) * sqrt(2.0/n) d. All of the above a. Vanilla dropout scales activations at test time, while inverted dropout scales them at train time. CS231n: Neural Networks 3 - Quiz Instructions: Answer the following questions based on the provided text from the CS231n website.\nMultiple Choice:\nWhich formula for numerical gradient calculation is recommended?\na. \\(df(x)/dx = (f(x + h) - f(x))/h\\) b. \\(df(x)/dx = (f(x + h) - f(x - h))/(2h)\\) c. \\(df(x)/dx = (f(x) - f(x - h))/h\\) d. \\(df(x)/dx = (f(x + h) + f(x - h))/(2h)\\) Which of the following is NOT a recommended practice for gradient checking?\na. Using double precision floating point b. Checking only a few dimensions of the gradient c. Using a large number of datapoints d. Turning off dropout and data augmentations What does the “ratio of weights:updates” metric tell us about the learning process?\na. The amount of overfitting in the model b. The effectiveness of the regularization technique c. The appropriateness of the learning rate d. The accuracy of the gradient calculation Which type of learning rate decay involves reducing the learning rate by a constant factor every few epochs?\na. Exponential decay b. 1/t decay c. Step decay d. Linear decay What is the main advantage of second-order optimization methods over first-order methods?\na. They are less computationally expensive b. They require fewer hyperparameters c. They are less prone to getting stuck in local minima d. They are more robust to noisy data True/False:\nThe centered difference formula for numerical gradient calculation is more precise than the standard formula. (True/False)\nIt is always a good idea to perform gradient checking on the full dataset. (True/False)\nA high “ratio of weights:updates” indicates that the learning rate is too low. (True/False)\nNesterov Momentum is a variation of the Momentum update that uses a “lookahead” approach. (True/False)\nSecond-order optimization methods are generally preferred over first-order methods due to their computational efficiency. (True/False)\nShort Answer:\nBriefly explain the concept of “kinks” in the objective function and how they can affect gradient checking.\nDescribe two sanity checks that should be performed before training a neural network.\nWhat are the three common types of learning rate decay?\nWhy are second-order optimization methods often impractical for deep learning applications?\nBonus:\nWhat are some common signs of an incorrect initialization in a neural network? Answer Key:\nMultiple Choice:\nb c c c b True/False:\nTrue False False True False Short Answer:\nKinks refer to non-differentiable points in the objective function, often introduced by functions like ReLU or SVM loss. When evaluating the numerical gradient across a kink, the result can be inaccurate due to the sudden change in the function’s behavior.\nTwo sanity checks are:\nChecking the loss at chance performance: Ensure the initial loss matches the expected value for a randomly initialized network. Overfitting a tiny subset of data: Verify that the model can achieve zero cost on a small, representative sample of the data. The three common types of learning rate decay are:\nStep decay Exponential decay 1/t decay Second-order optimization methods require computing and inverting the Hessian matrix, which is computationally expensive and memory-intensive, especially for large neural networks.\nBonus:\nCommon signs of incorrect initialization include: Noisy or unusual activation/gradient distributions in the network layers Slow or stalled learning progress Unrealistic or unstable loss function behavior This quiz provides a comprehensive assessment of the key concepts discussed in the provided text from the CS231n website. It covers topics like gradient checking, sanity checks, learning rate decay, and optimization methods, helping students solidify their understanding of these important aspects of neural network training.\nCS231n: Neural Networks Case Study - Quiz Instructions: Answer the following questions based on the provided text from the CS231n website.\nMultiple Choice:\nWhat type of dataset is used in this case study?\na. MNIST b. CIFAR-10 c. Spiral dataset d. ImageNet What is the primary reason for using the spiral dataset in this case study?\na. It is a simple dataset to understand. b. It is easily linearly separable. c. It is a challenging dataset that requires a non-linear classifier. d. It is a commonly used benchmark dataset for evaluating classifiers. What is the main difference between the linear classifier and the neural network in this case study?\na. The neural network uses a non-linear activation function. b. The neural network has more parameters. c. The neural network uses a different loss function. d. The neural network uses a different optimization algorithm. What is the purpose of the ReLU activation function in the neural network?\na. To introduce non-linearity into the model. b. To prevent overfitting. c. To improve the efficiency of the optimization process. d. To normalize the output of the hidden layer. What is the main advantage of using a neural network over a linear classifier for this dataset?\na. Neural networks are faster to train. b. Neural networks are more robust to noisy data. c. Neural networks can learn complex, non-linear relationships. d. Neural networks are easier to implement. True/False:\nThe spiral dataset is easily linearly separable. (True/False)\nThe cross-entropy loss function is used for both the linear classifier and the neural network. (True/False)\nThe gradient of the ReLU activation function is always 1. (True/False)\nThe backpropagation algorithm is used to update the parameters of the neural network. (True/False)\nThe regularization term in the loss function helps to prevent overfitting. (True/False)\nShort Answer:\nBriefly explain the concept of “backpropagation” in the context of neural networks.\nDescribe the two main components of the loss function used in this case study.\nWhat is the role of the “step size” parameter in the gradient descent algorithm?\nHow does the neural network improve the classification accuracy compared to the linear classifier on the spiral dataset?\nBonus:\nWhat are some other common activation functions used in neural networks besides ReLU? Answer Key:\nMultiple Choice:\nc. c. a. a. c. True/False:\nFalse True False True True Short Answer:\nBackpropagation is a method for computing the gradient of the loss function with respect to the parameters of a neural network. It involves propagating the gradient backwards through the network, starting from the output layer and working its way back to the input layer.\nThe loss function consists of two components:\nData loss: Measures the difference between the predicted and actual class labels. Regularization loss: Penalizes large weights, helping to prevent overfitting. The step size parameter controls the size of the steps taken during gradient descent. A larger step size can lead to faster convergence but also a greater risk of overshooting the optimal solution. A smaller step size can lead to slower convergence but may be more accurate.\nThe neural network improves the classification accuracy by introducing non-linearity through the ReLU activation function. This allows the model to learn complex, non-linear relationships between the input features and the output classes, which is necessary to accurately classify the spiral dataset.\nBonus:\nSome other common activation functions used in neural networks besides ReLU include: Sigmoid Tanh Leaky ReLU ELU This quiz provides a comprehensive assessment of the key concepts discussed in the provided text from the CS231n website. It covers topics like data generation, linear classification, neural networks, activation functions, backpropagation, and loss functions, helping students solidify their understanding of these important aspects of neural network training.\n","wordCount":"5139","inLanguage":"en","image":"https://csc-bo.github.io/favicon.ico","datePublished":"2024-05-21T11:59:27+08:00","dateModified":"2024-05-21T11:59:27+08:00","author":{"@type":"Person","name":"Bo Liu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://csc-bo.github.io/posts/cs231_part1_quiz/"},"publisher":{"@type":"Organization","name":"Bo's Log | What I cannot create, I do not understand","logo":{"@type":"ImageObject","url":"https://csc-bo.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://csc-bo.github.io/ accesskey=h title="Bo's Log (Alt + H)"><img src=https://csc-bo.github.io/apple-touch-icon.png alt aria-label=logo height=35>Bo's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://csc-bo.github.io/ title=Home><span>Home</span></a></li><li><a href=https://csc-bo.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://csc-bo.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://csc-bo.github.io/experience/ title=Experience><span>Experience</span></a></li><li><a href=https://csc-bo.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://csc-bo.github.io/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://csc-bo.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://csc-bo.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">CS231_part1_quiz</h1><div class=post-meta><span title='2024-05-21 11:59:27 +0800 CST'>May 21, 2024</span>&nbsp;·&nbsp;25 min&nbsp;·&nbsp;5139 words&nbsp;·&nbsp;Bo Liu&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/cs231_part1_quiz.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><ul><li><a href=#cs231n-linear-classification-quiz>CS231n Linear Classification Quiz</a></li><li><a href=#cs231n-optimization-quiz>CS231n Optimization Quiz:</a></li><li><a href=#cs231n-optimization-2-quiz>CS231n Optimization 2 Quiz</a></li><li><a href=#cs231n-neural-networks-quiz>CS231n Neural Networks: Quiz</a></li><li><a href=#cs231n-neural-networks-3---quiz>CS231n: Neural Networks 3 - Quiz</a></li><li><a href=#cs231n-neural-networks-case-study---quiz>CS231n: Neural Networks Case Study - Quiz</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>I used Gemini-1.5-flash to generate all the quizzes and answers. Enjoy!"</p><p>It covers topics like dataset, linear classification, neural networks, activation functions, backpropagation, and loss functions, helping us understand neural network training.</p><p><strong>Quiz: Image Classification</strong></p><p><strong>Multiple Choice:</strong></p><ol><li><p><strong>What is the primary task of image classification?</strong></p><ul><li>a. To identify all the objects present in an image.</li><li>b. To segment an image into different regions.</li><li>c. To create a new image based on an input image.</li><li>d. To assign a label from a fixed set of categories to an input image.</li></ul></li><li><p><strong>What is the data-driven approach to image classification?</strong></p><ul><li>a. Manually specifying the features of each category in code.</li><li>b. Providing the computer with many examples of each class and letting it learn from them.</li><li>c. Using a predefined set of rules to classify images.</li><li>d. Using a pre-trained model to classify images.</li></ul></li><li><p><strong>What is a hyperparameter?</strong></p><ul><li>a. A parameter that is learned from the data during training.</li><li>b. A parameter that is set before training and affects the model&rsquo;s behavior.</li><li>c. A parameter that is used to evaluate the model&rsquo;s performance.</li><li>d. A parameter that is used to visualize the model&rsquo;s predictions.</li></ul></li><li><p><strong>Why is it important to use a validation set when tuning hyperparameters?</strong></p><ul><li>a. To avoid overfitting to the test set.</li><li>b. To ensure that the model is generalizing well to unseen data.</li><li>c. To improve the model&rsquo;s performance on the training set.</li><li>d. To reduce the computational cost of training.</li></ul></li><li><p><strong>What is the main drawback of the Nearest Neighbor classifier for image classification?</strong></p><ul><li>a. It is difficult to implement.</li><li>b. It is computationally expensive to evaluate on a test image.</li><li>c. It does not generalize well to unseen data.</li><li>d. It is not robust to variations in image data.</li></ul></li><li><p><strong>What is the L1 distance?</strong></p><ul><li>a. The sum of the absolute differences between two vectors.</li><li>b. The square root of the sum of the squared differences between two vectors.</li><li>c. The dot product of two vectors.</li><li>d. The maximum difference between two vectors.</li></ul></li><li><p><strong>What is the purpose of k-Nearest Neighbor classifier?</strong></p><ul><li>a. To find the single closest training example to a test example.</li><li>b. To find the k closest training examples to a test example and have them vote on the label.</li><li>c. To learn a linear model that separates the classes.</li><li>d. To learn a non-linear model that separates the classes.</li></ul></li><li><p><strong>What is cross-validation?</strong></p><ul><li>a. A technique for splitting the training data into folds and using different folds as validation sets to tune hyperparameters.</li><li>b. A technique for evaluating the model&rsquo;s performance on the test set.</li><li>c. A technique for visualizing the model&rsquo;s predictions.</li><li>d. A technique for reducing the computational cost of training.</li></ul></li><li><p><strong>Why are pixel-based distances often inadequate for comparing images?</strong></p><ul><li>a. They are computationally expensive.</li><li>b. They do not capture perceptual or semantic similarity.</li><li>c. They are not robust to variations in image data.</li><li>d. They are not suitable for high-dimensional data.</li></ul></li><li><p><strong>What is t-SNE?</strong></p><ul><li>a. A technique for reducing the dimensionality of data while preserving local distances.</li><li>b. A technique for learning a linear model that separates the classes.</li><li>c. A technique for evaluating the model&rsquo;s performance.</li><li>d. A technique for visualizing the model&rsquo;s predictions.</li></ul></li></ol><p><strong>Fill-in-the-blank:</strong></p><ol start=11><li><p>The image classification pipeline consists of three steps: input, __________, and evaluation.</p></li><li><p>When choosing a dataset for image classification, it&rsquo;s important to consider factors like its __________, __________, and __________.</p></li><li><p>The L2 distance is also known as the __________ distance.</p></li><li><p>A higher value of k in k-Nearest Neighbor classifier has a __________ effect that makes the classifier more resistant to outliers.</p></li><li><p>The __________ set is used to tune hyperparameters, while the __________ set is used to evaluate the final model&rsquo;s performance.</p></li><li><p>A __________ layer in a Convolutional Neural Network (CNN) applies a filter to the input image to extract features.</p></li><li><p>__________ is a technique used to prevent overfitting in CNNs by randomly dropping out units during training.</p></li></ol><p><strong>Short Answer:</strong></p><ol start=18><li><p>Briefly describe two challenges of image classification.</p></li><li><p>Explain the difference between L1 and L2 distances in the context of image classification.</p></li><li><p>What is the purpose of a validation set in hyperparameter tuning?</p></li><li><p>Why is the Nearest Neighbor classifier often not a good choice for image classification?</p></li><li><p>What are some advantages of using t-SNE for visualizing high-dimensional data?</p></li><li><p>What are some common evaluation metrics used in image classification besides accuracy?</p></li><li><p>Explain the concept of overfitting in image classification.</p></li><li><p>Briefly describe the role of feature extraction in image classification.</p></li><li><p>What are some popular feature extraction techniques used in image classification?</p></li><li><p>How do convolutional layers work in a CNN?</p></li><li><p>What are some real-world applications of image classification?</p></li><li><p>Explain how a CNN can learn to recognize different objects in an image.</p></li><li><p>What are some of the limitations of CNNs?</p></li></ol><p><strong>Answer Key:</strong></p><ol><li>d</li><li>b</li><li>b</li><li>a</li><li>b</li><li>a</li><li>b</li><li>a</li><li>b</li><li>a</li><li>learning</li><li>size, diversity, and relevance to the task.</li><li>Euclidean</li><li>smoothing</li><li>validation, test</li><li>Convolutional</li><li>Dropout</li><li>Two challenges of image classification are:<ul><li><strong>Viewpoint variation:</strong> Objects can be oriented in many ways with respect to the camera.</li><li><strong>Illumination conditions:</strong> Lighting can drastically affect the appearance of an object.</li></ul></li><li>L1 distance is the sum of absolute differences between two vectors, while L2 distance is the square root of the sum of squared differences. L1 is more forgiving to outliers, while L2 is more sensitive to large differences.</li><li>A validation set is used to tune hyperparameters to prevent overfitting to the training set and ensure the model generalizes well to unseen data.</li><li>The Nearest Neighbor classifier is often not a good choice for image classification because it is computationally expensive to evaluate on a test image and it does not generalize well to unseen data.</li><li>Advantages of using t-SNE for visualizing high-dimensional data include:<ul><li>Reducing the dimensionality of data while preserving local distances.</li><li>Creating visually appealing and informative representations of complex data.</li><li>Helping to identify clusters and patterns in the data.</li></ul></li><li>Some common evaluation metrics used in image classification besides accuracy include:<ul><li>Precision</li><li>Recall</li><li>F1-score</li><li>Loss functions (e.g., cross-entropy loss)</li></ul></li><li>Overfitting in image classification occurs when a model learns the training data too well and fails to generalize to unseen data. This can happen if the model is too complex or if the training data is not representative of the real-world data.</li><li>Feature extraction in image classification involves extracting meaningful features from images that can be used to distinguish between different classes. These features can be based on various aspects of the image, such as shape, texture, color, or edges.</li><li>Some popular feature extraction techniques used in image classification include:<ul><li>SIFT (Scale-Invariant Feature Transform)</li><li>HOG (Histogram of Oriented Gradients)</li><li>Color histograms</li><li>Edge detectors</li></ul></li><li>Convolutional layers in a CNN apply a filter to the input image, sliding it across the image and performing a dot product at each location. This process extracts features from the image, such as edges, textures, and patterns.</li><li>Some real-world applications of image classification include:<ul><li>Object detection in self-driving cars</li><li>Facial recognition in security systems</li><li>Medical image analysis for disease diagnosis</li><li>Image retrieval in search engines</li><li>Content moderation on social media platforms</li></ul></li><li>A CNN learns to recognize different objects in an image by using convolutional layers to extract features, pooling layers to reduce dimensionality, and fully connected layers to classify the features into different categories. The network learns the weights of these layers during training by minimizing a loss function that measures the difference between the predicted and actual labels.</li><li>Some limitations of CNNs include:<ul><li>They can be computationally expensive to train and deploy.</li><li>They may struggle with recognizing objects that are not present in the training data.</li><li>They can be susceptible to adversarial attacks, where small changes to the input image can drastically change the output.</li></ul></li></ol><h2 id=cs231n-linear-classification-quiz>CS231n Linear Classification Quiz<a hidden class=anchor aria-hidden=true href=#cs231n-linear-classification-quiz>#</a></h2><p><strong>Instructions:</strong> Answer the following questions based on the provided content from the CS231n website.</p><p><strong>1. What are the two major components of the approach to image classification discussed in the article?</strong></p><p><strong>2. What is the simplest form of the score function used in linear classification, and what are its parameters?</strong></p><p><strong>3. Explain the bias trick and how it simplifies the score function.</strong></p><p><strong>4. What is the purpose of the loss function in image classification?</strong></p><p><strong>5. Describe the Multiclass Support Vector Machine (SVM) loss function. What is the role of the margin (Δ) in this loss function?</strong></p><p><strong>6. What is the purpose of regularization in the SVM loss function?</strong></p><p><strong>7. What is the difference between the hinge loss and the squared hinge loss?</strong></p><p><strong>8. How does the magnitude of the weights (W) affect the score differences?</strong></p><p><strong>9. Explain the relationship between the hyperparameters Δ and λ in the SVM loss function.</strong></p><p><strong>10. How can the linear classifier be interpreted as template matching?</strong></p><p><strong>Bonus Question:</strong> What is the main advantage of using a linear classifier over the k-Nearest Neighbor (kNN) classifier?</p><p><strong>Answer Key:</strong></p><p><strong>1.</strong> The two major components are:
* <strong>Score function:</strong> Maps raw image pixels to class scores.
* <strong>Loss function:</strong> Quantifies the agreement between predicted scores and ground truth labels.</p><p><strong>2.</strong> The simplest score function is a linear mapping:
* \(f(x_i, W, b) = W x_i + b\)
* Parameters:
* <strong>W:</strong> Weight matrix (size [K x D])
* <strong>b:</strong> Bias vector (size [K x 1])</p><p><strong>3.</strong> The bias trick combines the weight matrix (W) and bias vector (b) into a single matrix. It involves adding an extra dimension to the input vector \(x_i\) that always holds the constant 1 (bias dimension). This allows the score function to be expressed as a single matrix multiplication:
* \(f(x_i, W) = W x_i\)</p><p><strong>4.</strong> The loss function measures how well the predicted scores from the score function match the ground truth labels in the training data. A lower loss indicates better classification performance.</p><p><strong>5.</strong> The Multiclass SVM loss aims to ensure that the correct class score is higher than the incorrect class scores by a fixed margin (Δ). It calculates the loss for each example as the sum of the maximum of zero and the difference between the score of each incorrect class and the score of the correct class, minus the margin.</p><p><strong>6.</strong> Regularization in the SVM loss function aims to prevent overfitting by penalizing large weights. It encourages the classifier to consider all input dimensions to small amounts rather than a few dimensions with very strong influence.</p><p><strong>7.</strong> Both hinge loss and squared hinge loss are used in SVMs. The hinge loss penalizes violated margins linearly, while the squared hinge loss penalizes them quadratically, meaning it penalizes larger violations more strongly.</p><p><strong>8.</strong> Larger weights lead to larger score differences, while smaller weights lead to smaller score differences.</p><p><strong>9.</strong> Both Δ and λ control the trade-off between the data loss and the regularization loss. While Δ appears to control the margin, the actual trade-off is determined by the magnitude of the weights, which is influenced by λ.</p><p><strong>10.</strong> Each row of the weight matrix (W) can be interpreted as a template for a specific class. The score for each class is calculated by comparing the image to each template using an inner product. This process is similar to template matching, where the templates are learned from the training data.</p><p><strong>Bonus Question:</strong> The main advantage of using a linear classifier over kNN is that it is much faster for classifying new images. It only requires a single matrix multiplication and addition, while kNN needs to compare the test image to all training images. Additionally, linear classifiers can be trained efficiently and can be used to classify new images without storing the entire training set.</p><h2 id=cs231n-optimization-quiz>CS231n Optimization Quiz:<a hidden class=anchor aria-hidden=true href=#cs231n-optimization-quiz>#</a></h2><p>This quiz is based on the content of the CS231n Optimization section (<a href=https://cs231n.github.io/optimization-1/)>https://cs231n.github.io/optimization-1/)</a>.</p><p><strong>Multiple Choice:</strong></p><ol><li><p><strong>What is the goal of optimization in the context of machine learning?</strong></p><ul><li>a. To find the best set of parameters that minimize the loss function.</li><li>b. To maximize the accuracy of the model on the training data.</li><li>c. To find the most complex model that can fit the training data perfectly.</li><li>d. To ensure that the model generalizes well to unseen data.</li></ul></li><li><p><strong>Which of the following is NOT a strategy for optimization?</strong></p><ul><li>a. Random Search</li><li>b. Random Local Search</li><li>c. Following the Gradient</li><li>d. Backpropagation</li></ul></li><li><p><strong>What is the main advantage of using the gradient to optimize the loss function?</strong></p><ul><li>a. It is computationally less expensive than random search.</li><li>b. It guarantees finding the global minimum of the loss function.</li><li>c. It is more accurate than numerical gradient computation.</li><li>d. It can be easily implemented without calculus.</li></ul></li><li><p><strong>What is the difference between numerical gradient computation and analytic gradient computation?</strong></p><ul><li>a. Numerical gradient is approximate, while analytic gradient is exact.</li><li>b. Numerical gradient is faster, while analytic gradient is slower.</li><li>c. Numerical gradient requires calculus, while analytic gradient does not.</li><li>d. Numerical gradient is more error-prone, while analytic gradient is less error-prone.</li></ul></li><li><p><strong>What is the term for the process of repeatedly evaluating the gradient and updating the parameters?</strong></p><ul><li>a. Backpropagation</li><li>b. Gradient Descent</li><li>c. Stochastic Gradient Descent</li><li>d. Mini-batch Gradient Descent</li></ul></li><li><p><strong>What is the main advantage of using mini-batch gradient descent over full gradient descent?</strong></p><ul><li>a. It is computationally less expensive.</li><li>b. It is more accurate.</li><li>c. It is less prone to getting stuck in local minima.</li><li>d. It is easier to implement.</li></ul></li><li><p><strong>What is the term for using a single example to compute the gradient in gradient descent?</strong></p><ul><li>a. Full gradient descent</li><li>b. Mini-batch gradient descent</li><li>c. Stochastic gradient descent</li><li>d. Batch gradient descent</li></ul></li></ol><p><strong>True or False:</strong></p><ol><li>The SVM loss function is a convex function.</li><li>The numerical gradient computation is always more accurate than the analytic gradient computation.</li><li>The step size (learning rate) is a hyperparameter that needs to be carefully tuned.</li><li>The size of the mini-batch is usually set to a power of 2 for computational efficiency.</li><li>Backpropagation is a technique used to compute the gradient of the loss function.</li></ol><p><strong>Short Answer:</strong></p><ol><li>Briefly explain the blindfolded hiker analogy for optimization.</li><li>What are the two main components of the loss function?</li><li>Why is it important to check the correctness of the analytic gradient computation?</li></ol><p><strong>Answer Key:</strong></p><p><strong>Multiple Choice:</strong></p><ol><li>a To find the best set of parameters that minimize the loss function.</li><li>d Backpropagation</li><li>a It is computationally less expensive than random search.</li><li>a Numerical gradient is approximate, while analytic gradient is exact.</li><li>b Gradient Descent</li><li>a It is computationally less expensive.</li><li>c Stochastic gradient descent</li></ol><p><strong>True or False:</strong></p><ol><li>True</li><li>False</li><li>True</li><li>True</li><li>True</li></ol><p><strong>Short Answer:</strong></p><ol><li><p>The blindfolded hiker analogy compares the optimization process to a blindfolded hiker trying to find the bottom of a hilly terrain. The hiker represents the optimization algorithm, the terrain represents the loss function, and the height of the terrain represents the loss value. The goal is to find the lowest point on the terrain, which corresponds to the minimum loss value.</p></li><li><p>The two main components of the loss function are the data loss and the regularization loss. The data loss measures how well the model&rsquo;s predictions match the ground truth labels, while the regularization loss penalizes complex models and helps prevent overfitting.</p></li><li><p>It is important to check the correctness of the analytic gradient computation because it is more error-prone to implement than the numerical gradient. By comparing the analytic gradient to the numerical gradient, we can ensure that our implementation is correct and avoid errors in the optimization process.</p></li></ol><h2 id=cs231n-optimization-2-quiz>CS231n Optimization 2 Quiz<a hidden class=anchor aria-hidden=true href=#cs231n-optimization-2-quiz>#</a></h2><p>This quiz covers the concepts discussed in the CS231n course page on backpropagation: <a href=https://cs231n.github.io/optimization-2/>https://cs231n.github.io/optimization-2/</a></p><p><strong>Instructions:</strong> Answer the following questions to the best of your ability.</p><p><strong>Multiple Choice:</strong></p><ol><li><p><strong>What is the primary reason we are interested in computing gradients in the context of neural networks?</strong></p><ul><li>a. To understand the sensitivity of the loss function to its inputs.</li><li>b. To update the weights and biases of the network.</li><li>c. To visualize the network&rsquo;s internal representations.</li><li>d. All of the above.</li></ul></li><li><p><strong>What does the derivative of a function tell us?</strong></p><ul><li>a. The rate of change of the function at a specific point.</li><li>b. The slope of the function at a specific point.</li><li>c. The sensitivity of the function to small changes in its input.</li><li>d. All of the above.</li></ul></li><li><p><strong>What is the chain rule used for in backpropagation?</strong></p><ul><li>a. To compute the gradient of a composite function.</li><li>b. To propagate gradients backwards through the circuit.</li><li>c. To multiply local gradients to compute the overall gradient.</li><li>d. All of the above.</li></ul></li><li><p><strong>Which of the following gates has a local gradient of +1.0 for all its inputs?</strong></p><ul><li>a. Add gate</li><li>b. Multiply gate</li><li>c. Max gate</li><li>d. Sigmoid gate</li></ul></li><li><p><strong>What is the key difference between the add gate and the max gate in terms of how they distribute gradients?</strong></p><ul><li>a. The add gate distributes the gradient equally to all inputs, while the max gate routes the gradient to the highest input.</li><li>b. The add gate routes the gradient to the highest input, while the max gate distributes the gradient equally to all inputs.</li><li>c. The add gate distributes the gradient proportionally to the input values, while the max gate routes the gradient to the highest input.</li><li>d. The add gate routes the gradient to the highest input, while the max gate distributes the gradient proportionally to the input values.</li></ul></li><li><p><strong>Why is it important to use += instead of = when accumulating gradients in backpropagation?</strong></p><ul><li>a. To prevent overwriting the gradient on variables that are used multiple times in the forward pass.</li><li>b. To ensure that the gradient is propagated correctly through the circuit.</li><li>c. To account for the fact that gradients add up at forks in the circuit.</li><li>d. All of the above.</li></ul></li><li><p><strong>What is the main challenge in computing the gradient for matrix-matrix multiplication?</strong></p><ul><li>a. The need to use the chain rule multiple times.</li><li>b. The need to handle the transpose operation correctly.</li><li>c. The need to account for the dimensions of the matrices.</li><li>d. All of the above.</li></ul></li></ol><p><strong>Short Answer:</strong></p><ol><li>Explain the concept of &ldquo;backward flow&rdquo; in backpropagation.</li><li>Describe how the scale of input data can affect the magnitude of gradients in a linear classifier.</li><li>Briefly explain the importance of staged computation in backpropagation.</li></ol><p><strong>Bonus:</strong></p><ol><li>Derive the gradient of the sigmoid function with respect to its input.</li><li>Write Python code to implement the forward and backward pass for the following function: $f(x,y) = (x + y)^2 / (x * y)$</li></ol><p>This quiz is designed to test your understanding of the concepts covered in the CS231n course page on backpropagation. Good luck!</p><p>Here&rsquo;s a quiz based on the CS231n content you provided, covering the key concepts discussed:https://cs231n.github.io/neural-networks-1/</p><p><strong>Multiple Choice:</strong></p><ol><li><p><strong>Which of the following is NOT a commonly used activation function in neural networks?</strong></p><ul><li>a. Sigmoid</li><li>b. Tanh</li><li>c. ReLU</li><li>d. Exponential</li><li>e. Leaky ReLU</li></ul></li><li><p><strong>What is the primary advantage of using the ReLU activation function over the sigmoid function?</strong></p><ul><li>a. ReLU outputs are zero-centered.</li><li>b. ReLU does not suffer from saturation.</li><li>c. ReLU is computationally more efficient.</li><li>d. Both b and c</li><li>e. All of the above</li></ul></li><li><p><strong>What does the universal approximation theorem state about neural networks?</strong></p><ul><li>a. Neural networks can approximate any function with a finite number of neurons.</li><li>b. Neural networks with at least one hidden layer can approximate any continuous function.</li><li>c. Neural networks are always better than other machine learning models.</li><li>d. Neural networks are the only type of model that can achieve perfect accuracy.</li></ul></li><li><p><strong>Which of the following is NOT a factor to consider when setting the number of layers and their sizes in a neural network?</strong></p><ul><li>a. The complexity of the task</li><li>b. The amount of training data available</li><li>c. The computational resources available</li><li>d. The type of activation function used</li><li>e. The number of parameters in the network</li></ul></li><li><p><strong>What is the primary function of the output layer in a neural network?</strong></p><ul><li>a. To perform the final non-linear transformation.</li><li>b. To represent the class scores or target values.</li><li>c. To provide the input to the next hidden layer.</li><li>d. To determine the learning rate for the network.</li></ul></li></ol><p><strong>True/False:</strong></p><ol start=6><li><p><strong>A single neuron can be used to implement a binary classifier.</strong> (True/False)</p></li><li><p><strong>The tanh activation function is a scaled version of the sigmoid function.</strong> (True/False)</p></li><li><p><strong>The &ldquo;dying ReLU&rdquo; problem occurs when a ReLU neuron becomes permanently inactive during training.</strong> (True/False)</p></li><li><p><strong>Neural networks with more layers are always better than networks with fewer layers.</strong> (True/False)</p></li><li><p><strong>Regularization techniques are used to prevent overfitting in neural networks.</strong> (True/False)</p></li></ol><p><strong>Answers:</strong></p><ol><li>d. Exponential</li><li>d. Both b and c</li><li>b. Neural networks with at least one hidden layer can approximate any continuous function.</li><li>e. The number of parameters in the network</li><li>b. To represent the class scores or target values.</li><li>True</li><li>True</li><li>True</li><li>False</li><li>True</li></ol><p><strong>Bonus Question:</strong></p><p><strong>Explain the concept of &ldquo;gradual forgetting&rdquo; in the context of biological neurons and how it relates to regularization in neural networks.</strong></p><p>This quiz covers the key concepts from the provided text, including activation functions, network architectures, representational power, and the role of regularization. It should help you assess your understanding of these fundamental concepts in neural networks.</p><h2 id=cs231n-neural-networks-quiz>CS231n Neural Networks: Quiz<a hidden class=anchor aria-hidden=true href=#cs231n-neural-networks-quiz>#</a></h2><p><strong>Instructions:</strong> Choose the best answer for each question.</p><p><strong>1. What is the most common form of data preprocessing for neural networks?</strong></p><ul><li>a. Normalization</li><li>b. Whitening</li><li>c. Mean subtraction</li><li>d. PCA</li></ul><p><strong>2. Why is it important to initialize weights with small random numbers instead of all zeros?</strong></p><ul><li>a. To prevent overfitting</li><li>b. To ensure all neurons have the same output distribution</li><li>c. To break symmetry and allow neurons to learn different features</li><li>d. To speed up convergence</li></ul><p><strong>3. What is the purpose of Batch Normalization?</strong></p><ul><li>a. To reduce the number of parameters in a network</li><li>b. To prevent overfitting</li><li>c. To force activations to have a unit gaussian distribution</li><li>d. To improve the performance of dropout</li></ul><p><strong>4. Which regularization technique encourages sparse weight vectors?</strong></p><ul><li>a. L2 regularization</li><li>b. L1 regularization</li><li>c. Max norm constraints</li><li>d. Dropout</li></ul><p><strong>5. What is the main idea behind dropout?</strong></p><ul><li>a. To randomly drop connections between neurons during training</li><li>b. To randomly drop neurons during training</li><li>c. To randomly drop data points during training</li><li>d. To randomly drop layers during training</li></ul><p><strong>6. What is the purpose of scaling activations by &lsquo;p&rsquo; during testing with dropout?</strong></p><ul><li>a. To prevent overfitting</li><li>b. To ensure the outputs of neurons at test time are similar to their expected outputs at training time</li><li>c. To speed up convergence</li><li>d. To reduce the number of parameters in a network</li></ul><p><strong>7. Which of the following is NOT a common pitfall in data preprocessing?</strong></p><ul><li>a. Computing the mean across the entire dataset before splitting into train/val/test sets</li><li>b. Using the same preprocessing parameters for train, validation, and test sets</li><li>c. Not normalizing the data</li><li>d. Using PCA for dimensionality reduction</li></ul><p><strong>8. What is the recommended initialization for ReLU neurons?</strong></p><ul><li>a. w = np.random.randn(n) / sqrt(n)</li><li>b. w = np.random.randn(n) * sqrt(2.0/n)</li><li>c. w = np.random.randn(n) * 0.01</li><li>d. w = np.zeros(n)</li></ul><p><strong>9. Which of the following is a common way to control the capacity of a neural network?</strong></p><ul><li>a. Batch Normalization</li><li>b. Weight Initialization</li><li>c. Regularization</li><li>d. All of the above</li></ul><p><strong>10. What is the main difference between vanilla dropout and inverted dropout?</strong></p><ul><li>a. Vanilla dropout scales activations at test time, while inverted dropout scales them at train time.</li><li>b. Vanilla dropout drops connections, while inverted dropout drops neurons.</li><li>c. Vanilla dropout is more effective than inverted dropout.</li><li>d. Vanilla dropout is easier to implement than inverted dropout.</li></ul><p><strong>Answer Key:</strong></p><ol><li>c. Mean subtraction</li><li>c. To break symmetry and allow neurons to learn different features</li><li>c. To force activations to have a unit gaussian distribution</li><li>b. L1 regularization</li><li>b. To randomly drop neurons during training</li><li>b. To ensure the outputs of neurons at test time are similar to their expected outputs at training time</li><li>b. Using the same preprocessing parameters for train, validation, and test sets</li><li>b. w = np.random.randn(n) * sqrt(2.0/n)</li><li>d. All of the above</li><li>a. Vanilla dropout scales activations at test time, while inverted dropout scales them at train time.</li></ol><h2 id=cs231n-neural-networks-3---quiz>CS231n: Neural Networks 3 - Quiz<a hidden class=anchor aria-hidden=true href=#cs231n-neural-networks-3---quiz>#</a></h2><p><strong>Instructions:</strong> Answer the following questions based on the provided text from the CS231n website.</p><p><strong>Multiple Choice:</strong></p><ol><li><p><strong>Which formula for numerical gradient calculation is recommended?</strong></p><ul><li>a. \(df(x)/dx = (f(x + h) - f(x))/h\)</li><li>b. \(df(x)/dx = (f(x + h) - f(x - h))/(2h)\)</li><li>c. \(df(x)/dx = (f(x) - f(x - h))/h\)</li><li>d. \(df(x)/dx = (f(x + h) + f(x - h))/(2h)\)</li></ul></li><li><p><strong>Which of the following is NOT a recommended practice for gradient checking?</strong></p><ul><li>a. Using double precision floating point</li><li>b. Checking only a few dimensions of the gradient</li><li>c. Using a large number of datapoints</li><li>d. Turning off dropout and data augmentations</li></ul></li><li><p><strong>What does the &ldquo;ratio of weights:updates&rdquo; metric tell us about the learning process?</strong></p><ul><li>a. The amount of overfitting in the model</li><li>b. The effectiveness of the regularization technique</li><li>c. The appropriateness of the learning rate</li><li>d. The accuracy of the gradient calculation</li></ul></li><li><p><strong>Which type of learning rate decay involves reducing the learning rate by a constant factor every few epochs?</strong></p><ul><li>a. Exponential decay</li><li>b. 1/t decay</li><li>c. Step decay</li><li>d. Linear decay</li></ul></li><li><p><strong>What is the main advantage of second-order optimization methods over first-order methods?</strong></p><ul><li>a. They are less computationally expensive</li><li>b. They require fewer hyperparameters</li><li>c. They are less prone to getting stuck in local minima</li><li>d. They are more robust to noisy data</li></ul></li></ol><p><strong>True/False:</strong></p><ol start=6><li><p><strong>The centered difference formula for numerical gradient calculation is more precise than the standard formula.</strong> (True/False)</p></li><li><p><strong>It is always a good idea to perform gradient checking on the full dataset.</strong> (True/False)</p></li><li><p><strong>A high &ldquo;ratio of weights:updates&rdquo; indicates that the learning rate is too low.</strong> (True/False)</p></li><li><p><strong>Nesterov Momentum is a variation of the Momentum update that uses a &ldquo;lookahead&rdquo; approach.</strong> (True/False)</p></li><li><p><strong>Second-order optimization methods are generally preferred over first-order methods due to their computational efficiency.</strong> (True/False)</p></li></ol><p><strong>Short Answer:</strong></p><ol start=11><li><p>Briefly explain the concept of &ldquo;kinks&rdquo; in the objective function and how they can affect gradient checking.</p></li><li><p>Describe two sanity checks that should be performed before training a neural network.</p></li><li><p>What are the three common types of learning rate decay?</p></li><li><p>Why are second-order optimization methods often impractical for deep learning applications?</p></li></ol><p><strong>Bonus:</strong></p><ol start=15><li>What are some common signs of an incorrect initialization in a neural network?</li></ol><p><strong>Answer Key:</strong></p><p><strong>Multiple Choice:</strong></p><ol><li>b</li><li>c</li><li>c</li><li>c</li><li>b</li></ol><p><strong>True/False:</strong></p><ol start=6><li>True</li><li>False</li><li>False</li><li>True</li><li>False</li></ol><p><strong>Short Answer:</strong></p><ol start=11><li><p>Kinks refer to non-differentiable points in the objective function, often introduced by functions like ReLU or SVM loss. When evaluating the numerical gradient across a kink, the result can be inaccurate due to the sudden change in the function&rsquo;s behavior.</p></li><li><p>Two sanity checks are:</p><ul><li><strong>Checking the loss at chance performance:</strong> Ensure the initial loss matches the expected value for a randomly initialized network.</li><li><strong>Overfitting a tiny subset of data:</strong> Verify that the model can achieve zero cost on a small, representative sample of the data.</li></ul></li><li><p>The three common types of learning rate decay are:</p><ul><li>Step decay</li><li>Exponential decay</li><li>1/t decay</li></ul></li><li><p>Second-order optimization methods require computing and inverting the Hessian matrix, which is computationally expensive and memory-intensive, especially for large neural networks.</p></li></ol><p><strong>Bonus:</strong></p><ol start=15><li>Common signs of incorrect initialization include:<ul><li>Noisy or unusual activation/gradient distributions in the network layers</li><li>Slow or stalled learning progress</li><li>Unrealistic or unstable loss function behavior</li></ul></li></ol><p>This quiz provides a comprehensive assessment of the key concepts discussed in the provided text from the CS231n website. It covers topics like gradient checking, sanity checks, learning rate decay, and optimization methods, helping students solidify their understanding of these important aspects of neural network training.</p><h2 id=cs231n-neural-networks-case-study---quiz>CS231n: Neural Networks Case Study - Quiz<a hidden class=anchor aria-hidden=true href=#cs231n-neural-networks-case-study---quiz>#</a></h2><p><strong>Instructions:</strong> Answer the following questions based on the provided text from the CS231n website.</p><p><strong>Multiple Choice:</strong></p><ol><li><p><strong>What type of dataset is used in this case study?</strong></p><ul><li>a. MNIST</li><li>b. CIFAR-10</li><li>c. Spiral dataset</li><li>d. ImageNet</li></ul></li><li><p><strong>What is the primary reason for using the spiral dataset in this case study?</strong></p><ul><li>a. It is a simple dataset to understand.</li><li>b. It is easily linearly separable.</li><li>c. It is a challenging dataset that requires a non-linear classifier.</li><li>d. It is a commonly used benchmark dataset for evaluating classifiers.</li></ul></li><li><p><strong>What is the main difference between the linear classifier and the neural network in this case study?</strong></p><ul><li>a. The neural network uses a non-linear activation function.</li><li>b. The neural network has more parameters.</li><li>c. The neural network uses a different loss function.</li><li>d. The neural network uses a different optimization algorithm.</li></ul></li><li><p><strong>What is the purpose of the ReLU activation function in the neural network?</strong></p><ul><li>a. To introduce non-linearity into the model.</li><li>b. To prevent overfitting.</li><li>c. To improve the efficiency of the optimization process.</li><li>d. To normalize the output of the hidden layer.</li></ul></li><li><p><strong>What is the main advantage of using a neural network over a linear classifier for this dataset?</strong></p><ul><li>a. Neural networks are faster to train.</li><li>b. Neural networks are more robust to noisy data.</li><li>c. Neural networks can learn complex, non-linear relationships.</li><li>d. Neural networks are easier to implement.</li></ul></li></ol><p><strong>True/False:</strong></p><ol start=6><li><p>The spiral dataset is easily linearly separable. (True/False)</p></li><li><p>The cross-entropy loss function is used for both the linear classifier and the neural network. (True/False)</p></li><li><p>The gradient of the ReLU activation function is always 1. (True/False)</p></li><li><p>The backpropagation algorithm is used to update the parameters of the neural network. (True/False)</p></li><li><p>The regularization term in the loss function helps to prevent overfitting. (True/False)</p></li></ol><p><strong>Short Answer:</strong></p><ol start=11><li><p>Briefly explain the concept of &ldquo;backpropagation&rdquo; in the context of neural networks.</p></li><li><p>Describe the two main components of the loss function used in this case study.</p></li><li><p>What is the role of the &ldquo;step size&rdquo; parameter in the gradient descent algorithm?</p></li><li><p>How does the neural network improve the classification accuracy compared to the linear classifier on the spiral dataset?</p></li></ol><p><strong>Bonus:</strong></p><ol start=15><li>What are some other common activation functions used in neural networks besides ReLU?</li></ol><p><strong>Answer Key:</strong></p><p><strong>Multiple Choice:</strong></p><ol><li>c.</li><li>c.</li><li>a.</li><li>a.</li><li>c.</li></ol><p><strong>True/False:</strong></p><ol start=6><li>False</li><li>True</li><li>False</li><li>True</li><li>True</li></ol><p><strong>Short Answer:</strong></p><ol start=11><li><p>Backpropagation is a method for computing the gradient of the loss function with respect to the parameters of a neural network. It involves propagating the gradient backwards through the network, starting from the output layer and working its way back to the input layer.</p></li><li><p>The loss function consists of two components:</p><ul><li><strong>Data loss:</strong> Measures the difference between the predicted and actual class labels.</li><li><strong>Regularization loss:</strong> Penalizes large weights, helping to prevent overfitting.</li></ul></li><li><p>The step size parameter controls the size of the steps taken during gradient descent. A larger step size can lead to faster convergence but also a greater risk of overshooting the optimal solution. A smaller step size can lead to slower convergence but may be more accurate.</p></li><li><p>The neural network improves the classification accuracy by introducing non-linearity through the ReLU activation function. This allows the model to learn complex, non-linear relationships between the input features and the output classes, which is necessary to accurately classify the spiral dataset.</p></li></ol><p><strong>Bonus:</strong></p><ol start=15><li>Some other common activation functions used in neural networks besides ReLU include:<ul><li>Sigmoid</li><li>Tanh</li><li>Leaky ReLU</li><li>ELU</li></ul></li></ol><p>This quiz provides a comprehensive assessment of the key concepts discussed in the provided text from the CS231n website. It covers topics like data generation, linear classification, neural networks, activation functions, backpropagation, and loss functions, helping students solidify their understanding of these important aspects of neural network training.</p></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://csc-bo.github.io/posts/cs231p3/><span class=title>Next »</span><br><span>Notes for CS231n: Part 3</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://csc-bo.github.io/>Bo's Log | What I cannot create, I do not understand</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script></body></html>