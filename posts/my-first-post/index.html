<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Notes for MIT 6.S191 | Bo's Log</title>
<meta name=keywords content><meta name=description content="1. Introduction to Deep Learning After watch one of Feifei Li and Geffery Hinton&rsquo;s video on Youtube, I start to gain interest in DeepLearning, then I found this MIT course online that is a very comprehensive introduction for the Deep Learning which perfect for beginners like myself. This blog post marks the beginning of my journey into the world of machine learning, serving as a collection for my course notes and insights from related materials."><meta name=author content="Bo Liu"><link rel=canonical href=https://csc-bo.github.io/posts/my-first-post/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://csc-bo.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://csc-bo.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://csc-bo.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://csc-bo.github.io/apple-touch-icon.png><link rel=mask-icon href=https://csc-bo.github.io/android-chrome-192x192.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://csc-bo.github.io/posts/my-first-post/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Notes for MIT 6.S191"><meta property="og:description" content="1. Introduction to Deep Learning After watch one of Feifei Li and Geffery Hinton&rsquo;s video on Youtube, I start to gain interest in DeepLearning, then I found this MIT course online that is a very comprehensive introduction for the Deep Learning which perfect for beginners like myself. This blog post marks the beginning of my journey into the world of machine learning, serving as a collection for my course notes and insights from related materials."><meta property="og:type" content="article"><meta property="og:url" content="https://csc-bo.github.io/posts/my-first-post/"><meta property="og:image" content="https://csc-bo.github.io/favicon.ico"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-01T21:52:19+08:00"><meta property="article:modified_time" content="2024-05-01T21:52:19+08:00"><meta property="og:site_name" content="Bo's Log"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://csc-bo.github.io/favicon.ico"><meta name=twitter:title content="Notes for MIT 6.S191"><meta name=twitter:description content="1. Introduction to Deep Learning After watch one of Feifei Li and Geffery Hinton&rsquo;s video on Youtube, I start to gain interest in DeepLearning, then I found this MIT course online that is a very comprehensive introduction for the Deep Learning which perfect for beginners like myself. This blog post marks the beginning of my journey into the world of machine learning, serving as a collection for my course notes and insights from related materials."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://csc-bo.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Notes for MIT 6.S191","item":"https://csc-bo.github.io/posts/my-first-post/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Notes for MIT 6.S191","name":"Notes for MIT 6.S191","description":"1. Introduction to Deep Learning After watch one of Feifei Li and Geffery Hinton\u0026rsquo;s video on Youtube, I start to gain interest in DeepLearning, then I found this MIT course online that is a very comprehensive introduction for the Deep Learning which perfect for beginners like myself. This blog post marks the beginning of my journey into the world of machine learning, serving as a collection for my course notes and insights from related materials.","keywords":[],"articleBody":"1. Introduction to Deep Learning After watch one of Feifei Li and Geffery Hinton’s video on Youtube, I start to gain interest in DeepLearning, then I found this MIT course online that is a very comprehensive introduction for the Deep Learning which perfect for beginners like myself. This blog post marks the beginning of my journey into the world of machine learning, serving as a collection for my course notes and insights from related materials.\n2. Course Content Introductin\nThe Perceptron\nPerceptron: Simplified Activation Functions is to introduce non-linearites into the network Sigmoid, Hyperbolic Tangent, Rectified Linear Unit(ReLU) We can implement this graph on code very easily Forward Propagation:\nor foward pass refers to the calculation and storage of intermediate variables (including outputs) for a neural network in order from the input to the output layer.\nDense Layer: all inputs area densely connected to all outputs.\nWhat does it do?\nEssentially, a perceptron learns a linear decision boundary that separates the input space into two regions, each corresponding to a different class. One perceptron: Draws a line to separate data. binary classification. Multiple perceptrons in layers: Can create complex curves and shapes to separate data The Neural Networks\nNeural Network: Stacking Perceptrons to form neural network -\u003e MLP: Multi Layer Perceptron Loss: The loss of our network measures the cost incurred from incorrect predictions Empirical Loss $$ L_e(f) = \\frac{1}{n} \\sum_{i=1}^{n} L(y_i, f(x_i)) $$ Cross Entropy Loos $$ L(y, p) = - (y * log(p) + (1 - y) * log(1 - p)) $$ Mean Squared Error Loss $$ L(y, \\hat{y}) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 $$ Gradient Descent Backpropagation:\nBackpropagation is the application of gradient descent in deep learning, used to compute gradients of weights in neural networks. It employs the chain rule from calculus to efficiently propagate errors backwards, guiding updates to each layer’s parameters. By calculating the partial derivatives of the loss function with respect to every weight, it determines how each weight should be adjusted to minimize the loss.\nAlgorithm\nLearning Rate\nMomentum or Adaptive Learning Rates: For improved convergence, incorporate momentum or adaptive learning rate techniques like Nesterov Accelerated Gradient (NAG), RMSprop, or Adam.\nMini-Batch Gradient Descent\nInstead of using all data points at once, use a mini-batch of\n$( B ) samples: [ \\theta_i \\leftarrow \\theta_i - \\frac{\\alpha}{B} \\sum_{j=1}^{B} \\frac{\\partial L_j}{\\partial \\theta_i} ]$\n$ where ( L_j )$ is the loss for the $( j )-th$ sample in the mini-batch.\nOptimization Algorithm\nSGD(Stochastic Gradient Descent) Adam Adadelta Adagrad RMSProp Real World Technique\nMini-batches Fitting underfitting: Model does not have capacity to fully learn the data ideal fit overfitting: Too complex, extra parameters, does not generalize well Regularization Dropout\nDuring training, randomly set some activations to 0 Typically ‘drop’ 50% of activations in layer Forces network to not rely on any 1 node Early Stopping\nStop training before we have a chance to overfit Deep Sequence Modeling\nDeep Computer Vision\nDeep Generative Modeling\nDeep Reinforcement Learning\nLimitation and New Frontiers\nModule Summaries: Briefly summarize each module of the course, highlighting key concepts and algorithms covered. Lecture Notes: You can include your personal notes from lectures, focusing on important points and areas of difficulty. Assignments and Projects: Discuss the assignments and projects assigned in the course, sharing your approach and solutions. 3. Resources Share any additional resources you found helpful while taking the course, such as:\nOnline Tutorials: Links to relevant online tutorials or articles that provide further explanation on specific topics.\nResearch Papers: References to important research papers in the field of deep learning.\nSoftware Libraries: Information about deep learning libraries used in the course, such as TensorFlow or PyTorch.\n4. Personal Insights and Reflections Share your personal thoughts and reflections on the course, including:\nChallenges Faced: Discuss any challenges you encountered while learning the material and how you overcame them. Key Takeaways: Highlight the most important things you learned from the course. Future Applications: Explore potential applications of deep learning that you find interesting. 5. Conclusion Conclude your post by summarizing your experience with the course and expressing your thoughts on the field of deep learning as a whole.\n","wordCount":"686","inLanguage":"en","image":"https://csc-bo.github.io/favicon.ico","datePublished":"2024-05-01T21:52:19+08:00","dateModified":"2024-05-01T21:52:19+08:00","author":{"@type":"Person","name":"Bo Liu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://csc-bo.github.io/posts/my-first-post/"},"publisher":{"@type":"Organization","name":"Bo's Log","logo":{"@type":"ImageObject","url":"https://csc-bo.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://csc-bo.github.io/ accesskey=h title="Bo's Log (Alt + H)"><img src=https://csc-bo.github.io/apple-touch-icon.png alt aria-label=logo height=35>Bo's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://csc-bo.github.io/ title=Home><span>Home</span></a></li><li><a href=https://csc-bo.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://csc-bo.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://csc-bo.github.io/experience/ title=Experience><span>Experience</span></a></li><li><a href=https://csc-bo.github.io/search/ title=Search><span>Search</span></a></li><li><a href=https://csc-bo.github.io/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://csc-bo.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://csc-bo.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Notes for MIT 6.S191</h1><div class=post-meta><span title='2024-05-01 21:52:19 +0800 CST'>May 1, 2024</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;686 words&nbsp;·&nbsp;Bo Liu&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/my-first-post.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h2 id=1-introduction-to-deep-learning>1. Introduction to Deep Learning<a hidden class=anchor aria-hidden=true href=#1-introduction-to-deep-learning>#</a></h2><p>After watch one of Feifei Li and Geffery Hinton&rsquo;s video on Youtube, I start to gain interest in DeepLearning, then I found this MIT course online that is a very comprehensive introduction for the Deep Learning which perfect for beginners like myself. This blog post marks the beginning of my journey into the world of machine learning, serving as a collection for my course notes and insights from related materials.</p><h2 id=2-course-content>2. Course Content<a hidden class=anchor aria-hidden=true href=#2-course-content>#</a></h2><ol><li><p>Introductin</p><ul><li><p>The Perceptron</p><ol><li><p>Perceptron: Simplified
<img loading=lazy src=/MIT6.S191/perceptron.png alt="alt text"></p><ul><li>Activation Functions is to introduce <strong>non-linearites</strong> into the network</li><li>Sigmoid, Hyperbolic Tangent, Rectified Linear Unit(ReLU)</li><li>We can implement this graph on code very easily</li></ul></li><li><p><strong>Forward Propagation</strong>:</p><p>or foward pass refers to the calculation and storage of intermediate variables (including outputs) for a neural network in order from the input to the output layer.</p></li><li><p>Dense Layer: all inputs area densely connected to all outputs.</p></li><li><p>What does it do?</p><ul><li>Essentially, a perceptron learns a linear decision boundary that separates the input space into two regions, each corresponding to a different class.</li><li><strong>One perceptron</strong>: Draws a line to separate data. binary classification.</li><li><strong>Multiple perceptrons</strong> in layers: Can create complex curves and shapes to separate data</li></ul></li></ol></li><li><p>The Neural Networks</p><ol><li>Neural Network: Stacking Perceptrons to form neural network
<img loading=lazy src=/MIT6.S191/SLNN.png alt="alt text"><ul><li>-> MLP: Multi Layer Perceptron</li></ul></li><li><strong>Loss</strong>: The <strong>loss</strong> of our network measures the cost incurred from incorrect predictions<ul><li>Empirical Loss
$$
L_e(f) = \frac{1}{n} \sum_{i=1}^{n} L(y_i, f(x_i))
$$</li><li>Cross Entropy Loos
$$
L(y, p) = - (y * log(p) + (1 - y) * log(1 - p))
$$</li><li>Mean Squared Error Loss
$$
L(y, \hat{y}) = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$</li></ul></li><li><strong>Gradient Descent</strong><ol><li><p><strong>Backpropagation</strong>:</p><p>Backpropagation is the application of gradient descent in deep learning, used to compute gradients of weights in neural networks. It employs the <strong>chain rule</strong> from calculus to efficiently propagate errors backwards, guiding updates to each layer&rsquo;s parameters. By calculating the partial derivatives of the loss function with respect to every weight, it determines how each weight should be adjusted to minimize the loss.</p></li><li><p>Algorithm</p></li><li><p><strong>Learning Rate</strong></p><p>Momentum or Adaptive Learning Rates: For improved convergence, incorporate momentum or adaptive learning rate techniques like Nesterov Accelerated Gradient (NAG), RMSprop, or Adam.</p></li><li><p><strong>Mini-Batch Gradient Descent</strong></p><p>Instead of using all data points at once, use a mini-batch of</p><p>$( B ) samples: [ \theta_i \leftarrow \theta_i - \frac{\alpha}{B} \sum_{j=1}^{B} \frac{\partial L_j}{\partial \theta_i} ]$</p><p>$ where ( L_j )$ is the loss for the $( j )-th$ sample in the mini-batch.</p></li><li><p>Optimization Algorithm</p><ul><li>SGD(Stochastic Gradient Descent)</li><li>Adam</li><li>Adadelta</li><li>Adagrad</li><li>RMSProp</li></ul></li></ol></li></ol></li><li><p>Real World Technique</p><ol><li>Mini-batches</li><li>Fitting<ul><li>underfitting: Model does not have capacity to fully learn the data</li><li>ideal fit</li><li>overfitting: Too complex, extra parameters, does not generalize well</li></ul></li><li>Regularization<ol><li><p>Dropout</p><ul><li>During training, randomly set some activations to 0<ul><li>Typically &lsquo;drop&rsquo; 50% of activations in layer</li><li>Forces network to not rely on any 1 node</li></ul></li></ul></li><li><p>Early Stopping</p><ul><li>Stop training before we have a chance to overfit</li></ul></li></ol></li></ol></li></ul></li><li><p>Deep Sequence Modeling</p></li><li><p>Deep Computer Vision</p></li><li><p>Deep Generative Modeling</p></li><li><p>Deep Reinforcement Learning</p></li><li><p>Limitation and New Frontiers</p></li></ol><ul><li>Module Summaries: Briefly summarize each module of the course, highlighting key concepts and algorithms covered.</li><li>Lecture Notes: You can include your personal notes from lectures, focusing on important points and areas of difficulty.</li></ul><ul><li>Assignments and Projects: Discuss the assignments and projects assigned in the course, sharing your approach and solutions.</li></ul><h2 id=3-resources>3. Resources<a hidden class=anchor aria-hidden=true href=#3-resources>#</a></h2><ul><li><p>Share any additional resources you found helpful while taking the course, such as:</p></li><li><p>Online Tutorials: Links to relevant online tutorials or articles that provide further explanation on specific topics.</p></li><li><p>Research Papers: References to important research papers in the field of deep learning.</p></li><li><p>Software Libraries: Information about deep learning libraries used in the course, such as TensorFlow or PyTorch.</p></li></ul><h2 id=4-personal-insights-and-reflections>4. Personal Insights and Reflections<a hidden class=anchor aria-hidden=true href=#4-personal-insights-and-reflections>#</a></h2><p>Share your personal thoughts and reflections on the course, including:</p><ul><li>Challenges Faced: Discuss any challenges you encountered while learning the material and how you overcame them.</li><li>Key Takeaways: Highlight the most important things you learned from the course.
Future Applications: Explore potential applications of deep learning that you find interesting. 5. Conclusion</li></ul><p>Conclude your post by summarizing your experience with the course and expressing your thoughts on the field of deep learning as a whole.</p></div><footer class=post-footer><ul class=post-tags></ul></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://csc-bo.github.io/>Bo's Log</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>