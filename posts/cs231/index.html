<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Notes for CS231n: Deep Learning for Computer Vision | Bo's Log | What I cannot create, I do not understand</title>
<meta name=keywords content><meta name=description content="Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch & Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Deep Learning Basics Lecture 1: Computer vision overview
Lecture 2: Image Classification with linear Classifiers
The data-driven approch What is data-driven approaches linear classification & kNN means? Data-Driven Approach: 1."><meta name=author content="Bo Liu"><link rel=canonical href=https://csc-bo.github.io/posts/cs231/><link crossorigin=anonymous href=/assets/css/stylesheet.css rel="preload stylesheet" as=style><link rel=icon href=https://csc-bo.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://csc-bo.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://csc-bo.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://csc-bo.github.io/apple-touch-icon.png><link rel=mask-icon href=https://csc-bo.github.io/android-chrome-192x192.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://csc-bo.github.io/posts/cs231/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:title" content="Notes for CS231n: Deep Learning for Computer Vision"><meta property="og:description" content="Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch & Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Deep Learning Basics Lecture 1: Computer vision overview
Lecture 2: Image Classification with linear Classifiers
The data-driven approch What is data-driven approaches linear classification & kNN means? Data-Driven Approach: 1."><meta property="og:type" content="article"><meta property="og:url" content="https://csc-bo.github.io/posts/cs231/"><meta property="og:image" content="https://csc-bo.github.io/favicon.ico"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-05-06T16:30:19+08:00"><meta property="article:modified_time" content="2024-05-06T16:30:19+08:00"><meta property="og:site_name" content="Bo's Log"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://csc-bo.github.io/favicon.ico"><meta name=twitter:title content="Notes for CS231n: Deep Learning for Computer Vision"><meta name=twitter:description content="Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch & Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Deep Learning Basics Lecture 1: Computer vision overview
Lecture 2: Image Classification with linear Classifiers
The data-driven approch What is data-driven approaches linear classification & kNN means? Data-Driven Approach: 1."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://csc-bo.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Notes for CS231n: Deep Learning for Computer Vision","item":"https://csc-bo.github.io/posts/cs231/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Notes for CS231n: Deep Learning for Computer Vision","name":"Notes for CS231n: Deep Learning for Computer Vision","description":"Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch \u0026amp; Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Deep Learning Basics Lecture 1: Computer vision overview\nLecture 2: Image Classification with linear Classifiers\nThe data-driven approch What is data-driven approaches linear classification \u0026amp; kNN means? Data-Driven Approach: 1.","keywords":[],"articleBody":"Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch \u0026 Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Deep Learning Basics Lecture 1: Computer vision overview\nLecture 2: Image Classification with linear Classifiers\nThe data-driven approch What is data-driven approaches linear classification \u0026 kNN means? Data-Driven Approach: 1. Colloect a dataset of images and labels. 2. Use Machine Learning algorithms to train a classifier. 3. Evaluate the classifier on new images. There is two basic data-driven aprroaches to image classification\nK-nearest neighbor (kNN) What is kNN?\nkNN is a non-parametric algorithm, meaning it does’t make any assumptions about the underlying data distribution. It doesn’t build a model until a new instance is presented to it. How does kNN work? Data Preparation: The dataset is divided into a training set and a testing set(or validation set). Distance Calculation: When a new instance(query point) is presented to the model, the algorithm calculates the distance between the new query point and each instance in the training set. The most common distance metrics used in kNN are Euclidean distance(L2 norm), Manhattan distance(L1 norm), and Minkowski distance(Lp norm). K-nearest Neighbor Selection: The algorithm selects the K most similar instances (nearest neighbors) to the query point based on the calculated distances. The value of K is a hyperparameter that needs to be set. Majority vote (classification): In classification problems, the algorithm assigns the query point to the class that is most common among its K nearest neighbors. This is done by taking a majority vote among the classes of the nearest neighbors. Average value (regression): In regression problems, the algorithm predicts the value of the query point by taking the average of the values of its K nearest neighbors. Prediction: The final prediction is made based on the majority vote (classification) or average value (regression). Key takeways In image classification we start with a training set of images and labels, and must predict labels on the test set The K-Nearest Neighbors classifier predicts labels based on the K nearest training examples Distance metric and K are hyperparameters Choose hyperparameters using the validation set Only run on the test set once at the very end! K value: The choice of K is crucial, as it affects the performance of the algorithm. A small value of K can lead to overfitting, while a large value can lead to underfitting. Distance metric: The choice of distance metric can impact the performance of the algorithm, especially when dealing with high-dimensional data. Computational complexity: KNN can be computationally expensive, especially for large datasets, since it requires calculating distances between all instances. Distance Metric L1 (Manhattan)distance $$d_1(I_1,I_2) = \\sum_{p} |I_1^p - I_2^p|$$ L2 (Euclidean) distance $$d_2(I_1,I_2) = \\sqrt{\\sum_{p} (I_1^p - I_2^p)^2}$$ Linear classifiers What is a linear classifier?\nLinear classifier is a function $$f(x,W) = Wx + b$$ W is parameters or weights Algebraic/Visual/ Geometric veiwpoints\nSVM loss What is Multiclass SVM loss?\nGiven an example $(x_i, y_i)$ where $x_i$ is the image and where $y_i$ is the (integer) label, and using the shorthand for the scores vector: $s=f(x_i, W)$ the SVM loss has the form: $$\rL_i = ∑_{j≠y_i}\r\\begin{cases}\r0 \u0026 \\text{if } s_{y_i} ≥ s_j + 1 \\\\\rs_j - s_{y_i} + 1 \u0026 \\text{otherwise}\r\\end{cases}\\\\\r= ∑_{j≠y_i} max(0, s_j - s_{y_i} + 1)\r$$ $S_{y_i} - s_j$ is the difference in scores between correct and incorrect class calculation Q1: What happens to loss if car scores decrease by 0.5 for this training example? To determine the effect of decreasing the car score by 0.5, we need to look at the multiclass SVM loss formula:\n$ L*i = \\sum*{j \\neq y*i} \\max(0, s_j - s*{y_i} + 1) $\nFor this training example, the scores are:\ncat: 1.3 car: 4.9 frog: 2.0 Assuming the correct class is “car” (i.e., $ y_i = \\text{car} $), the loss is calculated as:\n$ L_i = \\max(0, 1.3 - 4.9 + 1) + \\max(0, 2.0 - 4.9 + 1) $ $ L_i = \\max(0, -2.6) + \\max(0, -1.9) $ $ L_i = 0 + 0 = 0 $\nNow, if the car score decreases by 0.5, the new scores are:\ncat: 1.3 car: 4.4 frog: 2.0 The new loss is:\n$ L_i = \\max(0, 1.3 - 4.4 + 1) + \\max(0, 2.0 - 4.4 + 1) $ $ L_i = \\max(0, -2.1) + \\max(0, -1.4) $ $ L_i = 0 + 0 = 0 $\nThus, the loss remains the same at 0.\nQ2: What is the min/max possible SVM loss $ L_i $? Minimum SVM Loss: The minimum SVM loss occurs when the model perfectly classifies the example with a margin of at least 1. Hence, for all incorrect classes $ j $:\n$ s*j - s*{y_i} + 1 \\leq 0 $\nIn this case, the loss for these classes will be 0. Therefore, the minimum SVM loss $ L_i $ is 0.\nMaximum SVM Loss: The maximum SVM loss occurs when the scores for all incorrect classes $ j $ are infinitely higher than the score for the correct class $ y_i $. As the scores for incorrect classes increase without bound, the loss also increases without bound. Therefore, the maximum SVM loss $ L_i $ is infinity.\nQ3: At initialization $ W $ is small so all $ s \\approx 0 $. What is the loss $ L_i $, assuming $ N $ examples and $ C $ classes? At initialization, if all scores $ s \\approx 0 $:\nThe multiclass SVM loss for a single example is:\n$ L*i = \\sum*{j \\neq y*i} \\max(0, s_j - s*{y_i} + 1) $\nSince $ s_j \\approx 0 $ for all $ j $:\n$ L*i = \\sum*{j \\neq y*i} \\max(0, 0 - 0 + 1) $ $ L_i = \\sum*{j \\neq y_i} 1 $ $ L_i = C - 1 $\n(where $ C $ is the number of classes)\nFor $ N $ examples, the total loss is:\n$ L = \\sum*{i=1}^{N} L_i = \\sum*{i=1}^{N} (C - 1) $ $ L = N(C - 1) $\nSo, the loss $ L_i $ for each example is $ C - 1 $, and the total loss for $ N $ examples is $ N(C - 1) $.\nQ4: What if the sum was over all classes (including $ j = y_i $)? If the sum was over all classes, including $ j = y_i $, the loss formula would be:\n$ L*i = \\sum*{j} \\max(0, s*j - s*{y_i} + 1) $\nSince the term for $ j = y_i $ is included, we have:\n$ \\max(0, s*{y_i} - s*{y_i} + 1) = \\max(0, 1) = 1 $\nSo, the loss would be:\n$ L*i = \\sum*{j \\neq y*i} \\max(0, s_j - s*{y_i} + 1) + 1 $\nThus, it includes an additional $ 1 $ term compared to the standard SVM loss for each example.\nSoftmax loss What is Softmax Classifier(Multinomial Logistic Regression) - Want to interpret raw classifier scores as probabilities $$ s = f(x*i; W) P(Y=k|X=x_i) = \\frac{e^{s_k}}{\\sum*{j=1}^{K} e^{s*j}}$$ Softmax Function\n$L_i = - log P(Y=y_i|X=x_i)$ Kullback-Leibler divergence $$ D*{KL}(P||Q) = \\sum*{y\\in \\mathcal{Y}} P(y) log \\frac{P(y)}{Q(y)}$$ Cross-Entropy $$ H(P,Q)=H(p)+D*{KL}(P||Q)$$ Questions\nQ1: What is the min/max possible softmax loss $ L_i $? Minimum Softmax Loss: The minimum softmax loss occurs when the model is perfectly confident and correct. In this case, the probability $ P(Y = y_i | X = x_i) $ for the correct class is 1. Therefore, the loss is:\n$ L_i = -\\log(1) = 0 $\nThus, the minimum possible softmax loss $ L_i $ is 0.\nMaximum Softmax Loss: The maximum softmax loss occurs when the model is infinitely wrong, meaning it assigns a probability of 0 to the correct class. In practical terms, this happens when the score for the correct class is much lower than the scores for all other classes. As the probability approaches 0, the loss approaches infinity:\n$ L_i = -\\log(0) = \\infty $\nThus, the maximum possible softmax loss $ L_i $ is infinity.\nQ2: At initialization, all $ s_j $ will be approximately equal; what is the softmax loss $ L_i $, assuming $ C $ classes? At initialization, if all scores $ s_j $ are approximately equal, the softmax probabilities for each class will be evenly distributed. For $ C $ classes, each class will have a probability of $ \\frac{1}{C} $.\nThe softmax loss for the correct class $ y_i $ is:\n$ L_i = -\\log\\left(\\frac{1}{C}\\right) = \\log(C) $\nTherefore, the softmax loss $ L_i $ at initialization, assuming $ C $ classes, is $ \\log(C) $.\nQ3: If all scores are small random values, what is the loss? $$-log(\\frac{1}{C})\\\\\rlog(10)\\approx 2.3$$ Cross-Entropy vs SVM Loss Q: What is cross-entropy loss? What is SVM loss? Cross-entropy loss \u003e 0, SVM loss = 0\nQ: What happens to each loss if I slightly change the scores of the last datapoint? Cross-entropy loss will change; SVM loss will stay the same\nSummary Lecture 3: Regularization and Optimization\nRegularization Stochastic Gradient Descent Momentum, AdaGrad, Adam Learning rate schedules Lecture 4: Neural Networks and Backpropagation\nMulti-layer Perceptron Backpropagation Perceiving and Understanding the Visual World Lecture 5: Image Classification with CNNs History Higher-level representations, image features Convolution and pooling Lecture 6: CNN Architectures Batch Normalization Transfer learning AlexNet, VGG, GoogLeNet, ResNet Lecture 7: Recurrent Neural Networks RNN, LSTM, GRU Language modeling Image captioning Sequence-to-sequence Lecture 8: Attention and Transformers Self-Attention Transformers Lecture 9: Object Detection and Image Segmentation Single-stage detectors Two-stage detectors Semantic/Instance/Panoptic segmentation Lecture 10: Video Understanding Video classification 3D CNNs Two-stream networks Multimodal video understanding Lecture 11: Visualizing and Understanding Feature visualization and inversion Adversarial examples DeepDream and style transfer Generative and Interactive Visual Intelligence Lecture 12: Self-supervised Learning Pretext tasks Contrastive learning Multisensory supervision Lecture 13: Generative Models Generative Adversarial Network Diffusion models Autoregressive models Lecture 14: OpenAI Sora Diffusion models Lecture 15: Robot Learning Deep Reinforcement Learning Model Learning Robotic Manipulation Lecture 16: Human-Centered Artificial Intelligence Lecture 17: Guest Lecture by Prof. Serena Yeung-Levy Lecture 18: 3D Vision 3D shape representations Shape reconstruction Neural implicit representations ","wordCount":"1701","inLanguage":"en","image":"https://csc-bo.github.io/favicon.ico","datePublished":"2024-05-06T16:30:19+08:00","dateModified":"2024-05-06T16:30:19+08:00","author":{"@type":"Person","name":"Bo Liu"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://csc-bo.github.io/posts/cs231/"},"publisher":{"@type":"Organization","name":"Bo's Log | What I cannot create, I do not understand","logo":{"@type":"ImageObject","url":"https://csc-bo.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://csc-bo.github.io/ accesskey=h title="Bo's Log (Alt + H)"><img src=https://csc-bo.github.io/apple-touch-icon.png alt aria-label=logo height=35>Bo's Log</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://csc-bo.github.io/ title=Home><span>Home</span></a></li><li><a href=https://csc-bo.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://csc-bo.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://csc-bo.github.io/experience/ title=Experience><span>Experience</span></a></li><li><a href=https://csc-bo.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li><li><a href=https://csc-bo.github.io/faq/ title=FAQ><span>FAQ</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://csc-bo.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://csc-bo.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Notes for CS231n: Deep Learning for Computer Vision</h1><div class=post-meta><span title='2024-05-06 16:30:19 +0800 CST'>May 6, 2024</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;1701 words&nbsp;·&nbsp;Bo Liu&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/cs231.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#assignments>Assignments</a></li><li><a href=#deep-learning-basics>Deep Learning Basics</a><ul><li><ul><li><a href=#q1-what-happens-to-loss-if-car-scores-decrease-by-05-for-this-training-example>Q1: What happens to loss if car scores decrease by 0.5 for this training example?</a></li><li><a href=#q2-what-is-the-minmax-possible-svm-loss--l_i->Q2: What is the min/max possible SVM loss ?</a></li><li><a href=#q3-at-initialization--w--is-small-so-all--s-approx-0--what-is-the-loss--l_i--assuming--n--examples-and--c--classes>Q3: At initialization is small so all . What is the loss , assuming examples and classes?</a></li><li><a href=#q4-what-if-the-sum-was-over-all-classes-including--j--y_i->Q4: What if the sum was over all classes (including )?</a></li><li><a href=#q1-what-is-the-minmax-possible-softmax-loss--l_i->Q1: What is the min/max possible softmax loss ?</a></li><li><a href=#q2-at-initialization-all--s_j--will-be-approximately-equal-what-is-the-softmax-loss--l_i--assuming--c--classes>Q2: At initialization, all will be approximately equal; what is the softmax loss , assuming classes?</a></li><li><a href=#q3-if-all-scores-are-small-random-values-what-is-the-loss>Q3: If all scores are small random values, what is the loss?</a></li><li><a href=#q-what-is-cross-entropy-loss-what-is-svm-loss>Q: What is cross-entropy loss? What is SVM loss?</a></li><li><a href=#q-what-happens-to-each-loss-if-i-slightly-change-the-scores-of-the-last-datapoint>Q: What happens to each loss if I slightly change the scores of the last datapoint?</a></li></ul></li></ul></li><li><a href=#perceiving-and-understanding-the-visual-world>Perceiving and Understanding the Visual World</a></li><li><a href=#generative-and-interactive-visual-intelligence>Generative and Interactive Visual Intelligence</a></li></ul></nav></div></details></div><div class=post-content><h1 id=assignments>Assignments<a hidden class=anchor aria-hidden=true href=#assignments>#</a></h1><ul><li><input disabled type=checkbox> Assignment #1
Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network</li><li><input disabled type=checkbox> Assignment #2
Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch & Network Visualization</li><li><input disabled type=checkbox> Assignment #3
Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning</li></ul><h1 id=deep-learning-basics>Deep Learning Basics<a hidden class=anchor aria-hidden=true href=#deep-learning-basics>#</a></h1><ul><li><p><input checked disabled type=checkbox> Lecture 1: Computer vision overview</p></li><li><p><input disabled type=checkbox> <strong>Lecture 2: Image Classification with linear Classifiers</strong></p><ul><li><p><input checked disabled type=checkbox> The data-driven approch
What is data-driven approaches linear classification & kNN means?
Data-Driven Approach: 1. Colloect a dataset of images and labels. 2. Use Machine Learning algorithms to train a classifier. 3. Evaluate the classifier on new images.
There is two basic data-driven aprroaches to image classification</p></li><li><p><input disabled type=checkbox> K-nearest neighbor (kNN)
What is kNN?</p><ul><li>kNN is a non-parametric algorithm, meaning it does&rsquo;t make any assumptions about the underlying data distribution. It doesn&rsquo;t build a model until a new instance is presented to it.
How does kNN work?</li></ul><ol><li>Data Preparation: The dataset is divided into a training set and a testing set(or validation set).
<img loading=lazy src=image-5.png alt="alt text">
<img loading=lazy src=image-6.png alt="alt text"></li><li>Distance Calculation: When a new instance(query point) is presented to the model, the algorithm calculates the distance between the new query point and each instance in the training set. The most common distance metrics used in kNN are Euclidean distance(L2 norm), Manhattan distance(L1 norm), and Minkowski distance(Lp norm).</li><li>K-nearest Neighbor Selection: The algorithm selects the K most similar instances (nearest neighbors) to the query point based on the calculated distances. The value of K is a hyperparameter that needs to be set.</li><li>Majority vote (classification): In classification problems, the algorithm assigns the query point to the class that is most common among its K nearest neighbors. This is done by taking a majority vote among the classes of the nearest neighbors.</li><li>Average value (regression): In regression problems, the algorithm predicts the value of the query point by taking the average of the values of its K nearest neighbors.</li><li>Prediction: The final prediction is made based on the majority vote (classification) or average value (regression).</li></ol><ul><li>Key takeways<ul><li>In image classification we start with a training set of images and labels, and must predict labels on the test set</li><li>The K-Nearest Neighbors classifier predicts labels based on the K nearest training examples</li><li>Distance metric and K are hyperparameters</li><li>Choose hyperparameters using the validation set</li><li>Only run on the test set once at the very end!</li><li><strong>K value</strong>: The choice of K is crucial, as it affects the performance of the algorithm. A small value of K can lead to overfitting, while a large value can lead to underfitting.</li><li><strong>Distance metric</strong>: The choice of distance metric can impact the performance of the algorithm, especially when dealing with high-dimensional data.</li><li>Computational complexity: KNN can be computationally expensive, especially for large datasets, since it requires calculating distances between all instances.</li></ul></li><li>Distance Metric</li><li>L1 (Manhattan)distance $$d_1(I_1,I_2) = \sum_{p} |I_1^p - I_2^p|$$</li><li>L2 (Euclidean) distance $$d_2(I_1,I_2) = \sqrt{\sum_{p} (I_1^p - I_2^p)^2}$$</li></ul></li><li><p><input disabled type=checkbox> Linear classifiers
What is a linear classifier?</p><ul><li>Linear classifier is a function $$f(x,W) = Wx + b$$</li><li>W is parameters or weights</li></ul></li><li><p><input disabled type=checkbox> Algebraic/Visual/ Geometric veiwpoints</p></li><li><p><input disabled type=checkbox> SVM loss
What is Multiclass SVM loss?</p><ul><li>Given an example $(x_i, y_i)$ where $x_i$ is the image and where $y_i$ is the (integer) label, and using the shorthand for the scores vector: $s=f(x_i, W)$</li><li>the SVM loss has the form:
$$
L_i = ∑_{j≠y_i}
\begin{cases}
0 & \text{if } s_{y_i} ≥ s_j + 1 \\
s_j - s_{y_i} + 1 & \text{otherwise}
      \end{cases}\\
= ∑_{j≠y_i} max(0, s_j - s_{y_i} + 1)
$$</li><li>$S_{y_i} - s_j$ is the difference in scores between correct and incorrect class
<img loading=lazy src=image-7.png alt="alt text"></li><li>calculation
<img loading=lazy src=image-8.png alt="alt text">
<img loading=lazy src=image-9.png alt="alt text"></li></ul><h3 id=q1-what-happens-to-loss-if-car-scores-decrease-by-05-for-this-training-example>Q1: What happens to loss if car scores decrease by 0.5 for this training example?<a hidden class=anchor aria-hidden=true href=#q1-what-happens-to-loss-if-car-scores-decrease-by-05-for-this-training-example>#</a></h3><p>To determine the effect of decreasing the car score by 0.5, we need to look at the multiclass SVM loss formula:</p><p>$ L*i = \sum*{j \neq y*i} \max(0, s_j - s*{y_i} + 1) $</p><p>For this training example, the scores are:</p><ul><li>cat: 1.3</li><li>car: 4.9</li><li>frog: 2.0</li></ul><p>Assuming the correct class is &ldquo;car&rdquo; (i.e., $ y_i = \text{car} $), the loss is calculated as:</p><p>$ L_i = \max(0, 1.3 - 4.9 + 1) + \max(0, 2.0 - 4.9 + 1) $
$ L_i = \max(0, -2.6) + \max(0, -1.9) $
$ L_i = 0 + 0 = 0 $</p><p>Now, if the car score decreases by 0.5, the new scores are:</p><ul><li>cat: 1.3</li><li>car: 4.4</li><li>frog: 2.0</li></ul><p>The new loss is:</p><p>$ L_i = \max(0, 1.3 - 4.4 + 1) + \max(0, 2.0 - 4.4 + 1) $
$ L_i = \max(0, -2.1) + \max(0, -1.4) $
$ L_i = 0 + 0 = 0 $</p><p>Thus, the loss remains the same at <strong>0</strong>.</p><h3 id=q2-what-is-the-minmax-possible-svm-loss--l_i->Q2: What is the min/max possible SVM loss $ L_i $?<a hidden class=anchor aria-hidden=true href=#q2-what-is-the-minmax-possible-svm-loss--l_i->#</a></h3><h4 id=minimum-svm-loss>Minimum SVM Loss:<a hidden class=anchor aria-hidden=true href=#minimum-svm-loss>#</a></h4><p>The minimum SVM loss occurs when the model perfectly classifies the example with a margin of at least 1. Hence, for all incorrect classes $ j $:</p><p>$ s*j - s*{y_i} + 1 \leq 0 $</p><p>In this case, the loss for these classes will be 0. Therefore, the minimum SVM loss $ L_i $ is <strong>0</strong>.</p><h4 id=maximum-svm-loss>Maximum SVM Loss:<a hidden class=anchor aria-hidden=true href=#maximum-svm-loss>#</a></h4><p>The maximum SVM loss occurs when the scores for all incorrect classes $ j $ are infinitely higher than the score for the correct class $ y_i $. As the scores for incorrect classes increase without bound, the loss also increases without bound. Therefore, the maximum SVM loss $ L_i $ is <strong>infinity</strong>.</p><h3 id=q3-at-initialization--w--is-small-so-all--s-approx-0--what-is-the-loss--l_i--assuming--n--examples-and--c--classes>Q3: At initialization $ W $ is small so all $ s \approx 0 $. What is the loss $ L_i $, assuming $ N $ examples and $ C $ classes?<a hidden class=anchor aria-hidden=true href=#q3-at-initialization--w--is-small-so-all--s-approx-0--what-is-the-loss--l_i--assuming--n--examples-and--c--classes>#</a></h3><p>At initialization, if all scores $ s \approx 0 $:</p><p>The multiclass SVM loss for a single example is:</p><p>$ L*i = \sum*{j \neq y*i} \max(0, s_j - s*{y_i} + 1) $</p><p>Since $ s_j \approx 0 $ for all $ j $:</p><p>$ L*i = \sum*{j \neq y*i} \max(0, 0 - 0 + 1) $
$ L_i = \sum*{j \neq y_i} 1 $
$ L_i = C - 1 $</p><p>(where $ C $ is the number of classes)</p><p>For $ N $ examples, the total loss is:</p><p>$ L = \sum*{i=1}^{N} L_i = \sum*{i=1}^{N} (C - 1) $
$ L = N(C - 1) $</p><p>So, the loss $ L_i $ for each example is $ C - 1 $, and the total loss for $ N $ examples is $ N(C - 1) $.</p><h3 id=q4-what-if-the-sum-was-over-all-classes-including--j--y_i->Q4: What if the sum was over all classes (including $ j = y_i $)?<a hidden class=anchor aria-hidden=true href=#q4-what-if-the-sum-was-over-all-classes-including--j--y_i->#</a></h3><p>If the sum was over all classes, including $ j = y_i $, the loss formula would be:</p><p>$ L*i = \sum*{j} \max(0, s*j - s*{y_i} + 1) $</p><p>Since the term for $ j = y_i $ is included, we have:</p><p>$ \max(0, s*{y_i} - s*{y_i} + 1) = \max(0, 1) = 1 $</p><p>So, the loss would be:</p><p>$ L*i = \sum*{j \neq y*i} \max(0, s_j - s*{y_i} + 1) + 1 $</p><p>Thus, it includes an additional $ 1 $ term compared to the standard SVM loss for each example.</p><ul><li><p><input disabled type=checkbox> Softmax loss
What is Softmax Classifier(Multinomial Logistic Regression) - Want to interpret raw classifier scores as probabilities</p>$$ s = f(x*i; W) P(Y=k|X=x_i) = \frac{e^{s_k}}{\sum*{j=1}^{K} e^{s*j}}$$</li><li><p>Softmax Function</p></li><li><p>$L_i = - log P(Y=y_i|X=x_i)$
<img loading=lazy src=image-10.png alt="alt text">
<img loading=lazy src=image-11.png alt="alt text">
<img loading=lazy src=image-12.png alt="alt text"></p></li><li><p>Kullback-Leibler divergence</p>$$ D*{KL}(P||Q) = \sum*{y\in \mathcal{Y}} P(y) log \frac{P(y)}{Q(y)}$$</li><li><p>Cross-Entropy</p>$$ H(P,Q)=H(p)+D*{KL}(P||Q)$$</li><li><p>Questions</p><h3 id=q1-what-is-the-minmax-possible-softmax-loss--l_i->Q1: What is the min/max possible softmax loss $ L_i $?<a hidden class=anchor aria-hidden=true href=#q1-what-is-the-minmax-possible-softmax-loss--l_i->#</a></h3><h4 id=minimum-softmax-loss>Minimum Softmax Loss:<a hidden class=anchor aria-hidden=true href=#minimum-softmax-loss>#</a></h4><p>The minimum softmax loss occurs when the model is perfectly confident and correct. In this case, the probability $ P(Y = y_i | X = x_i) $ for the correct class is 1. Therefore, the loss is:</p><p>$ L_i = -\log(1) = 0 $</p><p>Thus, the minimum possible softmax loss $ L_i $ is <strong>0</strong>.</p><h4 id=maximum-softmax-loss>Maximum Softmax Loss:<a hidden class=anchor aria-hidden=true href=#maximum-softmax-loss>#</a></h4><p>The maximum softmax loss occurs when the model is infinitely wrong, meaning it assigns a probability of 0 to the correct class. In practical terms, this happens when the score for the correct class is much lower than the scores for all other classes. As the probability approaches 0, the loss approaches infinity:</p><p>$ L_i = -\log(0) = \infty $</p><p>Thus, the maximum possible softmax loss $ L_i $ is <strong>infinity</strong>.</p><h3 id=q2-at-initialization-all--s_j--will-be-approximately-equal-what-is-the-softmax-loss--l_i--assuming--c--classes>Q2: At initialization, all $ s_j $ will be approximately equal; what is the softmax loss $ L_i $, assuming $ C $ classes?<a hidden class=anchor aria-hidden=true href=#q2-at-initialization-all--s_j--will-be-approximately-equal-what-is-the-softmax-loss--l_i--assuming--c--classes>#</a></h3><p>At initialization, if all scores $ s_j $ are approximately equal, the softmax probabilities for each class will be evenly distributed. For $ C $ classes, each class will have a probability of $ \frac{1}{C} $.</p><p>The softmax loss for the correct class $ y_i $ is:</p><p>$ L_i = -\log\left(\frac{1}{C}\right) = \log(C) $</p><p>Therefore, the softmax loss $ L_i $ at initialization, assuming $ C $ classes, is <strong>$ \log(C) $</strong>.</p><h3 id=q3-if-all-scores-are-small-random-values-what-is-the-loss>Q3: If all scores are small random values, what is the loss?<a hidden class=anchor aria-hidden=true href=#q3-if-all-scores-are-small-random-values-what-is-the-loss>#</a></h3>$$-log(\frac{1}{C})\\
log(10)\approx 2.3$$<ul><li>Cross-Entropy vs SVM Loss</li></ul><h3 id=q-what-is-cross-entropy-loss-what-is-svm-loss>Q: What is cross-entropy loss? What is SVM loss?<a hidden class=anchor aria-hidden=true href=#q-what-is-cross-entropy-loss-what-is-svm-loss>#</a></h3><p>Cross-entropy loss > 0, SVM loss = 0</p><h3 id=q-what-happens-to-each-loss-if-i-slightly-change-the-scores-of-the-last-datapoint>Q: What happens to each loss if I slightly change the scores of the last datapoint?<a hidden class=anchor aria-hidden=true href=#q-what-happens-to-each-loss-if-i-slightly-change-the-scores-of-the-last-datapoint>#</a></h3><p>Cross-entropy loss will change; SVM loss will stay the same</p><ul><li>Summary
<img loading=lazy src=image-14.png alt="alt text">
<img loading=lazy src=image-13.png alt="alt text"></li></ul></li></ul></li></ul></li><li><p><input disabled type=checkbox> <strong>Lecture 3: Regularization and Optimization</strong></p><ul><li><input disabled type=checkbox> Regularization</li><li><input disabled type=checkbox> Stochastic Gradient Descent</li><li><input disabled type=checkbox> Momentum, AdaGrad, Adam</li><li><input disabled type=checkbox> Learning rate schedules</li></ul></li><li><p><input disabled type=checkbox> <strong>Lecture 4: Neural Networks and Backpropagation</strong></p><ul><li><input disabled type=checkbox> Multi-layer Perceptron</li><li><input disabled type=checkbox> Backpropagation</li></ul></li></ul><h1 id=perceiving-and-understanding-the-visual-world>Perceiving and Understanding the Visual World<a hidden class=anchor aria-hidden=true href=#perceiving-and-understanding-the-visual-world>#</a></h1><ul><li><input disabled type=checkbox> <strong>Lecture 5: Image Classification with CNNs</strong><ul><li><input disabled type=checkbox> History</li><li><input disabled type=checkbox> Higher-level representations, image features</li><li><input disabled type=checkbox> Convolution and pooling</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 6: CNN Architectures</strong><ul><li><input disabled type=checkbox> Batch Normalization</li><li><input disabled type=checkbox> Transfer learning</li><li><input disabled type=checkbox> AlexNet, VGG, GoogLeNet, ResNet</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 7: Recurrent Neural Networks</strong><ul><li><input disabled type=checkbox> RNN, LSTM, GRU</li><li><input disabled type=checkbox> Language modeling</li><li><input disabled type=checkbox> Image captioning</li><li><input disabled type=checkbox> Sequence-to-sequence</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 8: Attention and Transformers</strong><ul><li><input disabled type=checkbox> Self-Attention</li><li><input disabled type=checkbox> Transformers</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 9: Object Detection and Image Segmentation</strong><ul><li><input disabled type=checkbox> Single-stage detectors</li><li><input disabled type=checkbox> Two-stage detectors</li><li><input disabled type=checkbox> Semantic/Instance/Panoptic segmentation</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 10: Video Understanding</strong><ul><li><input disabled type=checkbox> Video classification</li><li><input disabled type=checkbox> 3D CNNs</li><li><input disabled type=checkbox> Two-stream networks</li><li><input disabled type=checkbox> Multimodal video understanding</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 11: Visualizing and Understanding</strong><ul><li><input disabled type=checkbox> Feature visualization and inversion</li><li><input disabled type=checkbox> Adversarial examples</li><li><input disabled type=checkbox> DeepDream and style transfer</li></ul></li></ul><h1 id=generative-and-interactive-visual-intelligence>Generative and Interactive Visual Intelligence<a hidden class=anchor aria-hidden=true href=#generative-and-interactive-visual-intelligence>#</a></h1><ul><li><input disabled type=checkbox> <strong>Lecture 12: Self-supervised Learning</strong><ul><li><input disabled type=checkbox> Pretext tasks</li><li><input disabled type=checkbox> Contrastive learning</li><li><input disabled type=checkbox> Multisensory supervision</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 13: Generative Models</strong><ul><li><input disabled type=checkbox> Generative Adversarial Network</li><li><input disabled type=checkbox> Diffusion models</li><li><input disabled type=checkbox> Autoregressive models</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 14: OpenAI Sora</strong><ul><li><input disabled type=checkbox> Diffusion models</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 15: Robot Learning</strong><ul><li><input disabled type=checkbox> Deep Reinforcement Learning</li><li><input disabled type=checkbox> Model Learning</li><li><input disabled type=checkbox> Robotic Manipulation</li></ul></li><li><input disabled type=checkbox> <strong>Lecture 16: Human-Centered Artificial Intelligence</strong></li><li><input disabled type=checkbox> <strong>Lecture 17: Guest Lecture by Prof. Serena Yeung-Levy</strong></li><li><input disabled type=checkbox> <strong>Lecture 18: 3D Vision</strong><ul><li><input disabled type=checkbox> 3D shape representations</li><li><input disabled type=checkbox> Shape reconstruction</li><li><input disabled type=checkbox> Neural implicit representations</li></ul></li></ul></div><footer class=post-footer><ul class=post-tags></ul><nav class=paginav><a class=next href=https://csc-bo.github.io/posts/roadmap_ml/><span class=title>Next »</span><br><span>Roadmap for machine lerning</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://csc-bo.github.io/>Bo's Log | What I cannot create, I do not understand</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js></script><script>MathJax={tex:{displayMath:[["\\[","\\]"],["$$","$$"]],inlineMath:[["\\(","\\)"],["$","$"]]}}</script></body></html>