+++
title = 'Notes for CS231n: Deep Learning for Computer Vision'
date = 2024-05-06T16:30:19+08:00
draft = false
+++

# Assignments

- [ ] Assignment #1
      Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network
- [ ] Assignment #2
      Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch & Network Visualization
- [ ] Assignment #3
      Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning

# Deep Learning Basics

- [x] Lecture 1: Computer vision overview
- [ ] **Lecture 2: Image Classification with linear Classifiers**

  - [x] The data-driven approch
        What is data-driven approaches linear classification & kNN means?
        Data-Driven Approach: 1. Colloect a dataset of images and labels. 2. Use Machine Learning algorithms to train a classifier. 3. Evaluate the classifier on new images.
        There is two basic data-driven aprroaches to image classification

  - [ ] K-nearest neighbor (kNN)
        What is kNN?

    - kNN is a non-parametric algorithm, meaning it does't make any assumptions about the underlying data distribution. It doesn't build a model until a new instance is presented to it.
      How does kNN work?

    1. Data Preparation: The dataset is divided into a training set and a testing set(or validation set).
       ![alt text](image-5.png)
       ![alt text](image-6.png)
    2. Distance Calculation: When a new instance(query point) is presented to the model, the algorithm calculates the distance between the new query point and each instance in the training set. The most common distance metrics used in kNN are Euclidean distance(L2 norm), Manhattan distance(L1 norm), and Minkowski distance(Lp norm).
    3. K-nearest Neighbor Selection: The algorithm selects the K most similar instances (nearest neighbors) to the query point based on the calculated distances. The value of K is a hyperparameter that needs to be set.
    4. Majority vote (classification): In classification problems, the algorithm assigns the query point to the class that is most common among its K nearest neighbors. This is done by taking a majority vote among the classes of the nearest neighbors.
    5. Average value (regression): In regression problems, the algorithm predicts the value of the query point by taking the average of the values of its K nearest neighbors.
    6. Prediction: The final prediction is made based on the majority vote (classification) or average value (regression).

    - Key takeways
      - In image classification we start with a training set of images and labels, and must predict labels on the test set
      - The K-Nearest Neighbors classifier predicts labels based on the K nearest training examples
      - Distance metric and K are hyperparameters
      - Choose hyperparameters using the validation set
      - Only run on the test set once at the very end!
      - **K value**: The choice of K is crucial, as it affects the performance of the algorithm. A small value of K can lead to overfitting, while a large value can lead to underfitting.
      - **Distance metric**: The choice of distance metric can impact the performance of the algorithm, especially when dealing with high-dimensional data.
      - Computational complexity: KNN can be computationally expensive, especially for large datasets, since it requires calculating distances between all instances.
    - Distance Metric
    - L1 (Manhattan)distance $$d_1(I_1,I_2) = \sum_{p} |I_1^p - I_2^p|$$
    - L2 (Euclidean) distance $$d_2(I_1,I_2) = \sqrt{\sum_{p} (I_1^p - I_2^p)^2}$$

  - [ ] Linear classifiers
        What is a linear classifier?
    - Linear classifier is a function $$f(x,W) = Wx + b$$
    - W is parameters or weights
  - [ ] Algebraic/Visual/ Geometric veiwpoints
  - [ ] SVM loss
        What is Multiclass SVM loss?
    - Given an example $(x_i, y_i)$ where $x_i$ is the image and where $y_i$ is the (integer) label, and using the shorthand for the scores vector: $s=f(x_i, W)$
    - the SVM loss has the form:
      $$
      L_i = ∑_{j≠y_i}
      \begin{cases}
      0 & \text{if } s_{y_i} ≥ s_j + 1 \\
      s_j - s_{y_i} + 1 & \text{otherwise}
      \end{cases}\\
      = ∑_{j≠y_i} max(0, s_j - s_{y_i} + 1)
      $$
    - $S_{y_i} - s_j$ is the difference in scores between correct and incorrect class
      ![alt text](image-7.png)
    - calculation
      ![alt text](image-8.png)
      ![alt text](image-9.png)
    ### Q1: What happens to loss if car scores decrease by 0.5 for this training example?

    To determine the effect of decreasing the car score by 0.5, we need to look at the multiclass SVM loss formula:

    $ L*i = \sum*{j \neq y*i} \max(0, s_j - s*{y_i} + 1) $

    For this training example, the scores are:

    - cat: 1.3
    - car: 4.9
    - frog: 2.0

    Assuming the correct class is "car" (i.e., $ y_i = \text{car} $), the loss is calculated as:

    $ L_i = \max(0, 1.3 - 4.9 + 1) + \max(0, 2.0 - 4.9 + 1) $
    $ L_i = \max(0, -2.6) + \max(0, -1.9) $
    $ L_i = 0 + 0 = 0 $

    Now, if the car score decreases by 0.5, the new scores are:

    - cat: 1.3
    - car: 4.4
    - frog: 2.0

    The new loss is:

    $ L_i = \max(0, 1.3 - 4.4 + 1) + \max(0, 2.0 - 4.4 + 1) $
    $ L_i = \max(0, -2.1) + \max(0, -1.4) $
    $ L_i = 0 + 0 = 0 $

    Thus, the loss remains the same at **0**.

    ### Q2: What is the min/max possible SVM loss $ L_i $?

    #### Minimum SVM Loss:

    The minimum SVM loss occurs when the model perfectly classifies the example with a margin of at least 1. Hence, for all incorrect classes $ j $:

    $ s*j - s*{y_i} + 1 \leq 0 $

    In this case, the loss for these classes will be 0. Therefore, the minimum SVM loss $ L_i $ is **0**.

    #### Maximum SVM Loss:

    The maximum SVM loss occurs when the scores for all incorrect classes $ j $ are infinitely higher than the score for the correct class $ y_i $. As the scores for incorrect classes increase without bound, the loss also increases without bound. Therefore, the maximum SVM loss $ L_i $ is **infinity**.

    ### Q3: At initialization $ W $ is small so all $ s \approx 0 $. What is the loss $ L_i $, assuming $ N $ examples and $ C $ classes?

    At initialization, if all scores $ s \approx 0 $:

    The multiclass SVM loss for a single example is:

    $ L*i = \sum*{j \neq y*i} \max(0, s_j - s*{y_i} + 1) $

    Since $ s_j \approx 0 $ for all $ j $:

    $ L*i = \sum*{j \neq y*i} \max(0, 0 - 0 + 1) $
    $ L_i = \sum*{j \neq y_i} 1 $
    $ L_i = C - 1 $

    (where $ C $ is the number of classes)

    For $ N $ examples, the total loss is:

    $ L = \sum*{i=1}^{N} L_i = \sum*{i=1}^{N} (C - 1) $
    $ L = N(C - 1) $

    So, the loss $ L_i $ for each example is $ C - 1 $, and the total loss for $ N $ examples is $ N(C - 1) $.

    ### Q4: What if the sum was over all classes (including $ j = y_i $)?

    If the sum was over all classes, including $ j = y_i $, the loss formula would be:

    $ L*i = \sum*{j} \max(0, s*j - s*{y_i} + 1) $

    Since the term for $ j = y_i $ is included, we have:

    $ \max(0, s*{y_i} - s*{y_i} + 1) = \max(0, 1) = 1 $

    So, the loss would be:

    $ L*i = \sum*{j \neq y*i} \max(0, s_j - s*{y_i} + 1) + 1 $

    Thus, it includes an additional $ 1 $ term compared to the standard SVM loss for each example. 
    
    - [ ] Softmax loss
    What is Softmax Classifier(Multinomial Logistic Regression) - Want to interpret raw classifier scores as probabilities
    $$ s = f(x*i; W) P(Y=k|X=x_i) = \frac{e^{s_k}}{\sum*{j=1}^{K} e^{s*j}}$$ 
    - Softmax Function 
    - $L_i = - log P(Y=y_i|X=x_i)$
    ![alt text](image-10.png)
    ![alt text](image-11.png)
    ![alt text](image-12.png) 

    - Kullback-Leibler divergence
    $$ D*{KL}(P||Q) = \sum*{y\in \mathcal{Y}} P(y) log \frac{P(y)}{Q(y)}$$
        
    - Cross-Entropy
    $$ H(P,Q)=H(p)+D*{KL}(P||Q)$$ 
    - Questions 
        ### Q1: What is the min/max possible softmax loss $ L_i $?
        #### Minimum Softmax Loss:
        The minimum softmax loss occurs when the model is perfectly confident and correct. In this case, the probability $ P(Y = y_i | X = x_i) $ for the correct class is 1. Therefore, the loss is:

        $ L_i = -\log(1) = 0 $

        Thus, the minimum possible softmax loss $ L_i $ is **0**.

        #### Maximum Softmax Loss:
        The maximum softmax loss occurs when the model is infinitely wrong, meaning it assigns a probability of 0 to the correct class. In practical terms, this happens when the score for the correct class is much lower than the scores for all other classes. As the probability approaches 0, the loss approaches infinity:

        $ L_i = -\log(0) = \infty $

        Thus, the maximum possible softmax loss $ L_i $ is **infinity**.

        ### Q2: At initialization, all $ s_j $ will be approximately equal; what is the softmax loss $ L_i $, assuming $ C $ classes?

        At initialization, if all scores $ s_j $ are approximately equal, the softmax probabilities for each class will be evenly distributed. For $ C $ classes, each class will have a probability of $ \frac{1}{C} $.

        The softmax loss for the correct class $ y_i $ is:

        $ L_i = -\log\left(\frac{1}{C}\right) = \log(C) $

        Therefore, the softmax loss $ L_i $ at initialization, assuming $ C $ classes, is **$ \log(C) $**.
        ### Q3: If all scores are small random values, what is the loss?

        $$-log(\frac{1}{C})\\
        log(10)\approx 2.3$$

        - Cross-Entropy vs SVM Loss
        ### Q: What is cross-entropy loss? What is SVM loss?

        Cross-entropy loss > 0, SVM loss = 0

        ### Q: What happens to each loss if I slightly change the scores of the last datapoint?

        Cross-entropy loss will change; SVM loss will stay the same

        - Summary
        ![alt text](image-14.png)
        ![alt text](image-13.png)

- [ ] **Lecture 3: Regularization and Optimization**
  - [ ] Regularization
  - [ ] Stochastic Gradient Descent
  - [ ] Momentum, AdaGrad, Adam
  - [ ] Learning rate schedules
- [ ] **Lecture 4: Neural Networks and Backpropagation**
  - [ ] Multi-layer Perceptron
  - [ ] Backpropagation

# Perceiving and Understanding the Visual World

- [ ] **Lecture 5: Image Classification with CNNs**
  - [ ] History
  - [ ] Higher-level representations, image features
  - [ ] Convolution and pooling
- [ ] **Lecture 6: CNN Architectures**
  - [ ] Batch Normalization
  - [ ] Transfer learning
  - [ ] AlexNet, VGG, GoogLeNet, ResNet
- [ ] **Lecture 7: Recurrent Neural Networks**
  - [ ] RNN, LSTM, GRU
  - [ ] Language modeling
  - [ ] Image captioning
  - [ ] Sequence-to-sequence
- [ ] **Lecture 8: Attention and Transformers**
  - [ ] Self-Attention
  - [ ] Transformers
- [ ] **Lecture 9: Object Detection and Image Segmentation**
  - [ ] Single-stage detectors
  - [ ] Two-stage detectors
  - [ ] Semantic/Instance/Panoptic segmentation
- [ ] **Lecture 10: Video Understanding**
  - [ ] Video classification
  - [ ] 3D CNNs
  - [ ] Two-stream networks
  - [ ] Multimodal video understanding
- [ ] **Lecture 11: Visualizing and Understanding**
  - [ ] Feature visualization and inversion
  - [ ] Adversarial examples
  - [ ] DeepDream and style transfer

# Generative and Interactive Visual Intelligence

- [ ] **Lecture 12: Self-supervised Learning**
  - [ ] Pretext tasks
  - [ ] Contrastive learning
  - [ ] Multisensory supervision
- [ ] **Lecture 13: Generative Models**
  - [ ] Generative Adversarial Network
  - [ ] Diffusion models
  - [ ] Autoregressive models
- [ ] **Lecture 14: OpenAI Sora**
  - [ ] Diffusion models
- [ ] **Lecture 15: Robot Learning**
  - [ ] Deep Reinforcement Learning
  - [ ] Model Learning
  - [ ] Robotic Manipulation
- [ ] **Lecture 16: Human-Centered Artificial Intelligence**
- [ ] **Lecture 17: Guest Lecture by Prof. Serena Yeung-Levy**
- [ ] **Lecture 18: 3D Vision**
  - [ ] 3D shape representations
  - [ ] Shape reconstruction
  - [ ] Neural implicit representations
