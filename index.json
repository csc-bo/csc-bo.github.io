[{"content":"I used Gemini-1.5-flash to generate all the quizzes and answers. Enjoy!\u0026quot;\nMultiple Choice:\nWhat is the primary function of a convolutional layer in a ConvNet?\na) To perform matrix multiplication on the input data. b) To extract features from the input volume by applying learnable filters. c) To classify the input data into different categories. d) To reduce the dimensionality of the input data. What does \u0026ldquo;local connectivity\u0026rdquo; mean in the context of convolutional layers?\na) Each neuron is connected to all neurons in the previous layer. b) Each neuron is connected to a small region of the input volume. c) Neurons in the same depth slice share the same weights. d) The layer\u0026rsquo;s output is a single vector of class scores. Which of the following is NOT a hyperparameter that controls the size of the output volume in a convolutional layer?\na) Depth b) Stride c) Receptive field size d) Zero padding What is the purpose of parameter sharing in convolutional layers?\na) To reduce the number of parameters in the network. b) To ensure that the network is translationally invariant. c) To allow the network to learn more complex features. d) All of the above. What is a \u0026ldquo;depth slice\u0026rdquo; in a convolutional layer?\na) A 2D slice of the output volume, where all neurons share the same weights. b) A 3D volume representing the input to the convolutional layer. c) A single neuron in the convolutional layer. d) The set of all neurons connected to a specific region of the input volume. True/False:\nConvolutional layers are only used in image processing tasks. Zero padding is always necessary to preserve the spatial size of the input volume. The stride hyperparameter determines the size of the receptive field. A convolutional layer can be implemented as a matrix multiplication operation. Short Answer:\nExplain the concept of a \u0026ldquo;filter\u0026rdquo; in a convolutional layer. How does the stride hyperparameter affect the size of the output volume? Give an example of how parameter sharing contributes to translation invariance in a ConvNet. Answer Key:\nMultiple Choice:\nb) To extract features from the input volume by applying learnable filters. b) Each neuron is connected to a small region of the input volume. c) Receptive field size d) All of the above. a) A 2D slice of the output volume, where all neurons share the same weights. True/False:\nFalse False False True Short Answer:\nA filter in a convolutional layer is a set of weights that is applied to a small region of the input volume. The filter learns to detect a specific feature, such as an edge or a texture. A larger stride results in a smaller output volume, as the filter skips over more pixels in the input volume. If a ConvNet learns to detect a horizontal edge in one part of the image, parameter sharing ensures that it can also detect that edge in other parts of the image without needing to learn separate weights for each location. This makes the network translationally invariant. This quiz is designed to test your understanding of the core concepts related to convolutional layers. Remember that the CS231n website provides a wealth of information and resources to further explore these topics.\nQuiz: Visualizing ConvNets This quiz focuses on understanding how to visualize and interpret what Convolutional Neural Networks (ConvNets) learn, based on the information from the CS231n website.\nMultiple Choice:\nWhich of the following is NOT a common technique for visualizing ConvNets?\na) Visualizing activations of different layers. b) Visualizing the weights of filters. c) Occluding parts of the image to see how the prediction changes. d) Using a fully connected layer to classify the input image. What can be inferred from seeing noisy patterns in the visualized weights of a ConvNet?\na) The network has been trained for a long time. b) The network is overfitting the training data. c) The network is performing well on unseen data. d) The network has a high regularization strength. What is a potential drawback of visualizing the images that maximally activate a neuron?\na) It can be difficult to understand the meaning of a single neuron in isolation. b) It doesn\u0026rsquo;t provide information about the spatial relationships between features. c) It requires a large dataset of images for visualization. d) All of the above. What is the purpose of using t-SNE to embed CNN codes?\na) To visualize the high-dimensional representation space of a ConvNet in two dimensions. b) To understand the spatial relationships between features learned by the network. c) To determine the optimal number of filters for a convolutional layer. d) To reduce the number of parameters in the network. What information can be obtained by occluding parts of an image and observing the change in classification probability?\na) Which parts of the image are most important for the network\u0026rsquo;s prediction. b) The specific features that the network is learning to detect. c) The size of the receptive field for each neuron in the network. d) The number of hidden layers required for accurate classification. True/False:\nVisualizing activations can help identify \u0026ldquo;dead filters\u0026rdquo; in a ConvNet. t-SNE is the only method for visualizing high-dimensional data in a low-dimensional space. Occlusion methods can be used to understand the spatial relationships between features learned by the network. Short Answer:\nExplain why visualizing the weights of the first convolutional layer is often more interpretable than visualizing weights in deeper layers. Describe one limitation of using t-SNE to visualize CNN codes. What is the difference between visualizing activations and visualizing weights in a ConvNet? Answer Key:\nMultiple Choice:\nd) Using a fully connected layer to classify the input image. b) The network is overfitting the training data. d) All of the above. a) To visualize the high-dimensional representation space of a ConvNet in two dimensions. a) Which parts of the image are most important for the network\u0026rsquo;s prediction. True/False:\nTrue False True Short Answer:\nThe first convolutional layer operates directly on raw pixel data, making the filters easier to interpret as they learn simple features like edges, textures, and colors. Deeper layers learn more abstract and complex features, making their weights harder to understand visually. t-SNE can sometimes distort the relationships between points in the high-dimensional space, especially when dealing with very large datasets. It can also be sensitive to the choice of hyperparameters. Visualizing activations shows the output of each neuron in a layer for a given input image. Visualizing weights shows the filters learned by the convolutional layer, which represent the patterns the network is looking for in the input data. This quiz is designed to help you understand the different techniques for visualizing and interpreting ConvNets. Remember that the CS231n website provides a wealth of information and resources to further explore these topics.\nTransfer Learning in ConvNets: A Quiz This quiz focuses on the concepts of transfer learning in convolutional neural networks, as explained on the CS231n website.\nMultiple Choice:\nWhy is it uncommon to train a ConvNet from scratch?\na) It requires a large amount of data. b) It is computationally expensive. c) It is difficult to find good initial weights. d) All of the above. What is the primary benefit of using a pretrained ConvNet as a fixed feature extractor?\na) It allows for faster training times. b) It improves the performance of the network on the new dataset. c) It reduces the risk of overfitting. d) It enables the use of smaller datasets. What are \u0026ldquo;CNN codes\u0026rdquo;?\na) The weights of the convolutional layers in a pretrained ConvNet. b) The activations of the hidden layers in a pretrained ConvNet. c) The output of the classifier layer in a pretrained ConvNet. d) The input images used to train a pretrained ConvNet. Which of the following is NOT a factor to consider when deciding whether to fine-tune a pretrained ConvNet?\na) The size of the new dataset. b) The similarity of the new dataset to the original dataset. c) The number of convolutional layers in the pretrained network. d) The learning rate used for training the new classifier. What is the main advantage of using a pretrained ConvNet as an initialization for a new dataset?\na) It allows for faster training times. b) It improves the performance of the network on the new dataset. c) It reduces the risk of overfitting. d) All of the above. True/False:\nIt is always better to fine-tune a pretrained ConvNet than to use it as a fixed feature extractor. Fine-tuning all layers of a pretrained ConvNet is always the best approach. The learning rate used for fine-tuning should be higher than the learning rate used for training the new classifier. Short Answer:\nExplain why it is generally better to use a smaller learning rate for fine-tuning a pretrained ConvNet. Describe a scenario where it might be beneficial to train a linear classifier on CNN codes from an earlier layer in the pretrained network. How can you convert a fully connected layer in a ConvNet to a convolutional layer? Answer Key:\nMultiple Choice:\nd) All of the above. b) It improves the performance of the network on the new dataset. b) The activations of the hidden layers in a pretrained ConvNet. c) The number of convolutional layers in the pretrained network. d) All of the above. True/False:\nFalse False False Short Answer:\nThe weights of a pretrained ConvNet are already relatively good, so using a smaller learning rate prevents them from being distorted too quickly during fine-tuning. This helps to preserve the valuable features learned during the initial training. If the new dataset is very different from the original dataset, the features learned in the later layers of the pretrained network may not be relevant. Training a linear classifier on CNN codes from an earlier layer, which contains more generic features, can improve performance in this scenario. A fully connected layer can be converted to a convolutional layer by setting the filter size to the size of the input volume and applying it with zero padding. This effectively replicates the functionality of the fully connected layer as a convolution operation. This quiz is designed to test your understanding of the concepts of transfer learning in ConvNets. Remember that the CS231n website provides a wealth of information and resources to further explore these topics.\n","permalink":"https://csc-bo.github.io/posts/cs231_part2_quiz/","summary":"I used Gemini-1.5-flash to generate all the quizzes and answers. Enjoy!\u0026quot;\nMultiple Choice:\nWhat is the primary function of a convolutional layer in a ConvNet?\na) To perform matrix multiplication on the input data. b) To extract features from the input volume by applying learnable filters. c) To classify the input data into different categories. d) To reduce the dimensionality of the input data. What does \u0026ldquo;local connectivity\u0026rdquo; mean in the context of convolutional layers?","title":"Cs231_part2_quiz"},{"content":"I used Gemini-1.5-flash to generate all the quizzes and answers. Enjoy!\u0026quot;\nIt covers topics like dataset, linear classification, neural networks, activation functions, backpropagation, and loss functions, helping us understand neural network training.\nQuiz: Image Classification\nMultiple Choice:\nWhat is the primary task of image classification?\na. To identify all the objects present in an image. b. To segment an image into different regions. c. To create a new image based on an input image. d. To assign a label from a fixed set of categories to an input image. What is the data-driven approach to image classification?\na. Manually specifying the features of each category in code. b. Providing the computer with many examples of each class and letting it learn from them. c. Using a predefined set of rules to classify images. d. Using a pre-trained model to classify images. What is a hyperparameter?\na. A parameter that is learned from the data during training. b. A parameter that is set before training and affects the model\u0026rsquo;s behavior. c. A parameter that is used to evaluate the model\u0026rsquo;s performance. d. A parameter that is used to visualize the model\u0026rsquo;s predictions. Why is it important to use a validation set when tuning hyperparameters?\na. To avoid overfitting to the test set. b. To ensure that the model is generalizing well to unseen data. c. To improve the model\u0026rsquo;s performance on the training set. d. To reduce the computational cost of training. What is the main drawback of the Nearest Neighbor classifier for image classification?\na. It is difficult to implement. b. It is computationally expensive to evaluate on a test image. c. It does not generalize well to unseen data. d. It is not robust to variations in image data. What is the L1 distance?\na. The sum of the absolute differences between two vectors. b. The square root of the sum of the squared differences between two vectors. c. The dot product of two vectors. d. The maximum difference between two vectors. What is the purpose of k-Nearest Neighbor classifier?\na. To find the single closest training example to a test example. b. To find the k closest training examples to a test example and have them vote on the label. c. To learn a linear model that separates the classes. d. To learn a non-linear model that separates the classes. What is cross-validation?\na. A technique for splitting the training data into folds and using different folds as validation sets to tune hyperparameters. b. A technique for evaluating the model\u0026rsquo;s performance on the test set. c. A technique for visualizing the model\u0026rsquo;s predictions. d. A technique for reducing the computational cost of training. Why are pixel-based distances often inadequate for comparing images?\na. They are computationally expensive. b. They do not capture perceptual or semantic similarity. c. They are not robust to variations in image data. d. They are not suitable for high-dimensional data. What is t-SNE?\na. A technique for reducing the dimensionality of data while preserving local distances. b. A technique for learning a linear model that separates the classes. c. A technique for evaluating the model\u0026rsquo;s performance. d. A technique for visualizing the model\u0026rsquo;s predictions. Fill-in-the-blank:\nThe image classification pipeline consists of three steps: input, __________, and evaluation.\nWhen choosing a dataset for image classification, it\u0026rsquo;s important to consider factors like its __________, __________, and __________.\nThe L2 distance is also known as the __________ distance.\nA higher value of k in k-Nearest Neighbor classifier has a __________ effect that makes the classifier more resistant to outliers.\nThe __________ set is used to tune hyperparameters, while the __________ set is used to evaluate the final model\u0026rsquo;s performance.\nA __________ layer in a Convolutional Neural Network (CNN) applies a filter to the input image to extract features.\n__________ is a technique used to prevent overfitting in CNNs by randomly dropping out units during training.\nShort Answer:\nBriefly describe two challenges of image classification.\nExplain the difference between L1 and L2 distances in the context of image classification.\nWhat is the purpose of a validation set in hyperparameter tuning?\nWhy is the Nearest Neighbor classifier often not a good choice for image classification?\nWhat are some advantages of using t-SNE for visualizing high-dimensional data?\nWhat are some common evaluation metrics used in image classification besides accuracy?\nExplain the concept of overfitting in image classification.\nBriefly describe the role of feature extraction in image classification.\nWhat are some popular feature extraction techniques used in image classification?\nHow do convolutional layers work in a CNN?\nWhat are some real-world applications of image classification?\nExplain how a CNN can learn to recognize different objects in an image.\nWhat are some of the limitations of CNNs?\nAnswer Key:\nd b b a b a b a b a learning size, diversity, and relevance to the task. Euclidean smoothing validation, test Convolutional Dropout Two challenges of image classification are: Viewpoint variation: Objects can be oriented in many ways with respect to the camera. Illumination conditions: Lighting can drastically affect the appearance of an object. L1 distance is the sum of absolute differences between two vectors, while L2 distance is the square root of the sum of squared differences. L1 is more forgiving to outliers, while L2 is more sensitive to large differences. A validation set is used to tune hyperparameters to prevent overfitting to the training set and ensure the model generalizes well to unseen data. The Nearest Neighbor classifier is often not a good choice for image classification because it is computationally expensive to evaluate on a test image and it does not generalize well to unseen data. Advantages of using t-SNE for visualizing high-dimensional data include: Reducing the dimensionality of data while preserving local distances. Creating visually appealing and informative representations of complex data. Helping to identify clusters and patterns in the data. Some common evaluation metrics used in image classification besides accuracy include: Precision Recall F1-score Loss functions (e.g., cross-entropy loss) Overfitting in image classification occurs when a model learns the training data too well and fails to generalize to unseen data. This can happen if the model is too complex or if the training data is not representative of the real-world data. Feature extraction in image classification involves extracting meaningful features from images that can be used to distinguish between different classes. These features can be based on various aspects of the image, such as shape, texture, color, or edges. Some popular feature extraction techniques used in image classification include: SIFT (Scale-Invariant Feature Transform) HOG (Histogram of Oriented Gradients) Color histograms Edge detectors Convolutional layers in a CNN apply a filter to the input image, sliding it across the image and performing a dot product at each location. This process extracts features from the image, such as edges, textures, and patterns. Some real-world applications of image classification include: Object detection in self-driving cars Facial recognition in security systems Medical image analysis for disease diagnosis Image retrieval in search engines Content moderation on social media platforms A CNN learns to recognize different objects in an image by using convolutional layers to extract features, pooling layers to reduce dimensionality, and fully connected layers to classify the features into different categories. The network learns the weights of these layers during training by minimizing a loss function that measures the difference between the predicted and actual labels. Some limitations of CNNs include: They can be computationally expensive to train and deploy. They may struggle with recognizing objects that are not present in the training data. They can be susceptible to adversarial attacks, where small changes to the input image can drastically change the output. CS231n Linear Classification Quiz Instructions: Answer the following questions based on the provided content from the CS231n website.\n1. What are the two major components of the approach to image classification discussed in the article?\n2. What is the simplest form of the score function used in linear classification, and what are its parameters?\n3. Explain the bias trick and how it simplifies the score function.\n4. What is the purpose of the loss function in image classification?\n5. Describe the Multiclass Support Vector Machine (SVM) loss function. What is the role of the margin (Δ) in this loss function?\n6. What is the purpose of regularization in the SVM loss function?\n7. What is the difference between the hinge loss and the squared hinge loss?\n8. How does the magnitude of the weights (W) affect the score differences?\n9. Explain the relationship between the hyperparameters Δ and λ in the SVM loss function.\n10. How can the linear classifier be interpreted as template matching?\nBonus Question: What is the main advantage of using a linear classifier over the k-Nearest Neighbor (kNN) classifier?\nAnswer Key:\n1. The two major components are: * Score function: Maps raw image pixels to class scores. * Loss function: Quantifies the agreement between predicted scores and ground truth labels.\n2. The simplest score function is a linear mapping: * \\(f(x_i, W, b) = W x_i + b\\) * Parameters: * W: Weight matrix (size [K x D]) * b: Bias vector (size [K x 1])\n3. The bias trick combines the weight matrix (W) and bias vector (b) into a single matrix. It involves adding an extra dimension to the input vector \\(x_i\\) that always holds the constant 1 (bias dimension). This allows the score function to be expressed as a single matrix multiplication: * \\(f(x_i, W) = W x_i\\)\n4. The loss function measures how well the predicted scores from the score function match the ground truth labels in the training data. A lower loss indicates better classification performance.\n5. The Multiclass SVM loss aims to ensure that the correct class score is higher than the incorrect class scores by a fixed margin (Δ). It calculates the loss for each example as the sum of the maximum of zero and the difference between the score of each incorrect class and the score of the correct class, minus the margin.\n6. Regularization in the SVM loss function aims to prevent overfitting by penalizing large weights. It encourages the classifier to consider all input dimensions to small amounts rather than a few dimensions with very strong influence.\n7. Both hinge loss and squared hinge loss are used in SVMs. The hinge loss penalizes violated margins linearly, while the squared hinge loss penalizes them quadratically, meaning it penalizes larger violations more strongly.\n8. Larger weights lead to larger score differences, while smaller weights lead to smaller score differences.\n9. Both Δ and λ control the trade-off between the data loss and the regularization loss. While Δ appears to control the margin, the actual trade-off is determined by the magnitude of the weights, which is influenced by λ.\n10. Each row of the weight matrix (W) can be interpreted as a template for a specific class. The score for each class is calculated by comparing the image to each template using an inner product. This process is similar to template matching, where the templates are learned from the training data.\nBonus Question: The main advantage of using a linear classifier over kNN is that it is much faster for classifying new images. It only requires a single matrix multiplication and addition, while kNN needs to compare the test image to all training images. Additionally, linear classifiers can be trained efficiently and can be used to classify new images without storing the entire training set.\nCS231n Optimization Quiz: This quiz is based on the content of the CS231n Optimization section (https://cs231n.github.io/optimization-1/).\nMultiple Choice:\nWhat is the goal of optimization in the context of machine learning?\na. To find the best set of parameters that minimize the loss function. b. To maximize the accuracy of the model on the training data. c. To find the most complex model that can fit the training data perfectly. d. To ensure that the model generalizes well to unseen data. Which of the following is NOT a strategy for optimization?\na. Random Search b. Random Local Search c. Following the Gradient d. Backpropagation What is the main advantage of using the gradient to optimize the loss function?\na. It is computationally less expensive than random search. b. It guarantees finding the global minimum of the loss function. c. It is more accurate than numerical gradient computation. d. It can be easily implemented without calculus. What is the difference between numerical gradient computation and analytic gradient computation?\na. Numerical gradient is approximate, while analytic gradient is exact. b. Numerical gradient is faster, while analytic gradient is slower. c. Numerical gradient requires calculus, while analytic gradient does not. d. Numerical gradient is more error-prone, while analytic gradient is less error-prone. What is the term for the process of repeatedly evaluating the gradient and updating the parameters?\na. Backpropagation b. Gradient Descent c. Stochastic Gradient Descent d. Mini-batch Gradient Descent What is the main advantage of using mini-batch gradient descent over full gradient descent?\na. It is computationally less expensive. b. It is more accurate. c. It is less prone to getting stuck in local minima. d. It is easier to implement. What is the term for using a single example to compute the gradient in gradient descent?\na. Full gradient descent b. Mini-batch gradient descent c. Stochastic gradient descent d. Batch gradient descent True or False:\nThe SVM loss function is a convex function. The numerical gradient computation is always more accurate than the analytic gradient computation. The step size (learning rate) is a hyperparameter that needs to be carefully tuned. The size of the mini-batch is usually set to a power of 2 for computational efficiency. Backpropagation is a technique used to compute the gradient of the loss function. Short Answer:\nBriefly explain the blindfolded hiker analogy for optimization. What are the two main components of the loss function? Why is it important to check the correctness of the analytic gradient computation? Answer Key:\nMultiple Choice:\na To find the best set of parameters that minimize the loss function. d Backpropagation a It is computationally less expensive than random search. a Numerical gradient is approximate, while analytic gradient is exact. b Gradient Descent a It is computationally less expensive. c Stochastic gradient descent True or False:\nTrue False True True True Short Answer:\nThe blindfolded hiker analogy compares the optimization process to a blindfolded hiker trying to find the bottom of a hilly terrain. The hiker represents the optimization algorithm, the terrain represents the loss function, and the height of the terrain represents the loss value. The goal is to find the lowest point on the terrain, which corresponds to the minimum loss value.\nThe two main components of the loss function are the data loss and the regularization loss. The data loss measures how well the model\u0026rsquo;s predictions match the ground truth labels, while the regularization loss penalizes complex models and helps prevent overfitting.\nIt is important to check the correctness of the analytic gradient computation because it is more error-prone to implement than the numerical gradient. By comparing the analytic gradient to the numerical gradient, we can ensure that our implementation is correct and avoid errors in the optimization process.\nCS231n Optimization 2 Quiz This quiz covers the concepts discussed in the CS231n course page on backpropagation: https://cs231n.github.io/optimization-2/\nInstructions: Answer the following questions to the best of your ability.\nMultiple Choice:\nWhat is the primary reason we are interested in computing gradients in the context of neural networks?\na. To understand the sensitivity of the loss function to its inputs. b. To update the weights and biases of the network. c. To visualize the network\u0026rsquo;s internal representations. d. All of the above. What does the derivative of a function tell us?\na. The rate of change of the function at a specific point. b. The slope of the function at a specific point. c. The sensitivity of the function to small changes in its input. d. All of the above. What is the chain rule used for in backpropagation?\na. To compute the gradient of a composite function. b. To propagate gradients backwards through the circuit. c. To multiply local gradients to compute the overall gradient. d. All of the above. Which of the following gates has a local gradient of +1.0 for all its inputs?\na. Add gate b. Multiply gate c. Max gate d. Sigmoid gate What is the key difference between the add gate and the max gate in terms of how they distribute gradients?\na. The add gate distributes the gradient equally to all inputs, while the max gate routes the gradient to the highest input. b. The add gate routes the gradient to the highest input, while the max gate distributes the gradient equally to all inputs. c. The add gate distributes the gradient proportionally to the input values, while the max gate routes the gradient to the highest input. d. The add gate routes the gradient to the highest input, while the max gate distributes the gradient proportionally to the input values. Why is it important to use += instead of = when accumulating gradients in backpropagation?\na. To prevent overwriting the gradient on variables that are used multiple times in the forward pass. b. To ensure that the gradient is propagated correctly through the circuit. c. To account for the fact that gradients add up at forks in the circuit. d. All of the above. What is the main challenge in computing the gradient for matrix-matrix multiplication?\na. The need to use the chain rule multiple times. b. The need to handle the transpose operation correctly. c. The need to account for the dimensions of the matrices. d. All of the above. Short Answer:\nExplain the concept of \u0026ldquo;backward flow\u0026rdquo; in backpropagation. Describe how the scale of input data can affect the magnitude of gradients in a linear classifier. Briefly explain the importance of staged computation in backpropagation. Bonus:\nDerive the gradient of the sigmoid function with respect to its input. Write Python code to implement the forward and backward pass for the following function: $f(x,y) = (x + y)^2 / (x * y)$ This quiz is designed to test your understanding of the concepts covered in the CS231n course page on backpropagation. Good luck!\nHere\u0026rsquo;s a quiz based on the CS231n content you provided, covering the key concepts discussed:https://cs231n.github.io/neural-networks-1/\nMultiple Choice:\nWhich of the following is NOT a commonly used activation function in neural networks?\na. Sigmoid b. Tanh c. ReLU d. Exponential e. Leaky ReLU What is the primary advantage of using the ReLU activation function over the sigmoid function?\na. ReLU outputs are zero-centered. b. ReLU does not suffer from saturation. c. ReLU is computationally more efficient. d. Both b and c e. All of the above What does the universal approximation theorem state about neural networks?\na. Neural networks can approximate any function with a finite number of neurons. b. Neural networks with at least one hidden layer can approximate any continuous function. c. Neural networks are always better than other machine learning models. d. Neural networks are the only type of model that can achieve perfect accuracy. Which of the following is NOT a factor to consider when setting the number of layers and their sizes in a neural network?\na. The complexity of the task b. The amount of training data available c. The computational resources available d. The type of activation function used e. The number of parameters in the network What is the primary function of the output layer in a neural network?\na. To perform the final non-linear transformation. b. To represent the class scores or target values. c. To provide the input to the next hidden layer. d. To determine the learning rate for the network. True/False:\nA single neuron can be used to implement a binary classifier. (True/False)\nThe tanh activation function is a scaled version of the sigmoid function. (True/False)\nThe \u0026ldquo;dying ReLU\u0026rdquo; problem occurs when a ReLU neuron becomes permanently inactive during training. (True/False)\nNeural networks with more layers are always better than networks with fewer layers. (True/False)\nRegularization techniques are used to prevent overfitting in neural networks. (True/False)\nAnswers:\nd. Exponential d. Both b and c b. Neural networks with at least one hidden layer can approximate any continuous function. e. The number of parameters in the network b. To represent the class scores or target values. True True True False True Bonus Question:\nExplain the concept of \u0026ldquo;gradual forgetting\u0026rdquo; in the context of biological neurons and how it relates to regularization in neural networks.\nThis quiz covers the key concepts from the provided text, including activation functions, network architectures, representational power, and the role of regularization. It should help you assess your understanding of these fundamental concepts in neural networks.\nCS231n Neural Networks: Quiz Instructions: Choose the best answer for each question.\n1. What is the most common form of data preprocessing for neural networks?\na. Normalization b. Whitening c. Mean subtraction d. PCA 2. Why is it important to initialize weights with small random numbers instead of all zeros?\na. To prevent overfitting b. To ensure all neurons have the same output distribution c. To break symmetry and allow neurons to learn different features d. To speed up convergence 3. What is the purpose of Batch Normalization?\na. To reduce the number of parameters in a network b. To prevent overfitting c. To force activations to have a unit gaussian distribution d. To improve the performance of dropout 4. Which regularization technique encourages sparse weight vectors?\na. L2 regularization b. L1 regularization c. Max norm constraints d. Dropout 5. What is the main idea behind dropout?\na. To randomly drop connections between neurons during training b. To randomly drop neurons during training c. To randomly drop data points during training d. To randomly drop layers during training 6. What is the purpose of scaling activations by \u0026lsquo;p\u0026rsquo; during testing with dropout?\na. To prevent overfitting b. To ensure the outputs of neurons at test time are similar to their expected outputs at training time c. To speed up convergence d. To reduce the number of parameters in a network 7. Which of the following is NOT a common pitfall in data preprocessing?\na. Computing the mean across the entire dataset before splitting into train/val/test sets b. Using the same preprocessing parameters for train, validation, and test sets c. Not normalizing the data d. Using PCA for dimensionality reduction 8. What is the recommended initialization for ReLU neurons?\na. w = np.random.randn(n) / sqrt(n) b. w = np.random.randn(n) * sqrt(2.0/n) c. w = np.random.randn(n) * 0.01 d. w = np.zeros(n) 9. Which of the following is a common way to control the capacity of a neural network?\na. Batch Normalization b. Weight Initialization c. Regularization d. All of the above 10. What is the main difference between vanilla dropout and inverted dropout?\na. Vanilla dropout scales activations at test time, while inverted dropout scales them at train time. b. Vanilla dropout drops connections, while inverted dropout drops neurons. c. Vanilla dropout is more effective than inverted dropout. d. Vanilla dropout is easier to implement than inverted dropout. Answer Key:\nc. Mean subtraction c. To break symmetry and allow neurons to learn different features c. To force activations to have a unit gaussian distribution b. L1 regularization b. To randomly drop neurons during training b. To ensure the outputs of neurons at test time are similar to their expected outputs at training time b. Using the same preprocessing parameters for train, validation, and test sets b. w = np.random.randn(n) * sqrt(2.0/n) d. All of the above a. Vanilla dropout scales activations at test time, while inverted dropout scales them at train time. CS231n: Neural Networks 3 - Quiz Instructions: Answer the following questions based on the provided text from the CS231n website.\nMultiple Choice:\nWhich formula for numerical gradient calculation is recommended?\na. \\(df(x)/dx = (f(x + h) - f(x))/h\\) b. \\(df(x)/dx = (f(x + h) - f(x - h))/(2h)\\) c. \\(df(x)/dx = (f(x) - f(x - h))/h\\) d. \\(df(x)/dx = (f(x + h) + f(x - h))/(2h)\\) Which of the following is NOT a recommended practice for gradient checking?\na. Using double precision floating point b. Checking only a few dimensions of the gradient c. Using a large number of datapoints d. Turning off dropout and data augmentations What does the \u0026ldquo;ratio of weights:updates\u0026rdquo; metric tell us about the learning process?\na. The amount of overfitting in the model b. The effectiveness of the regularization technique c. The appropriateness of the learning rate d. The accuracy of the gradient calculation Which type of learning rate decay involves reducing the learning rate by a constant factor every few epochs?\na. Exponential decay b. 1/t decay c. Step decay d. Linear decay What is the main advantage of second-order optimization methods over first-order methods?\na. They are less computationally expensive b. They require fewer hyperparameters c. They are less prone to getting stuck in local minima d. They are more robust to noisy data True/False:\nThe centered difference formula for numerical gradient calculation is more precise than the standard formula. (True/False)\nIt is always a good idea to perform gradient checking on the full dataset. (True/False)\nA high \u0026ldquo;ratio of weights:updates\u0026rdquo; indicates that the learning rate is too low. (True/False)\nNesterov Momentum is a variation of the Momentum update that uses a \u0026ldquo;lookahead\u0026rdquo; approach. (True/False)\nSecond-order optimization methods are generally preferred over first-order methods due to their computational efficiency. (True/False)\nShort Answer:\nBriefly explain the concept of \u0026ldquo;kinks\u0026rdquo; in the objective function and how they can affect gradient checking.\nDescribe two sanity checks that should be performed before training a neural network.\nWhat are the three common types of learning rate decay?\nWhy are second-order optimization methods often impractical for deep learning applications?\nBonus:\nWhat are some common signs of an incorrect initialization in a neural network? Answer Key:\nMultiple Choice:\nb c c c b True/False:\nTrue False False True False Short Answer:\nKinks refer to non-differentiable points in the objective function, often introduced by functions like ReLU or SVM loss. When evaluating the numerical gradient across a kink, the result can be inaccurate due to the sudden change in the function\u0026rsquo;s behavior.\nTwo sanity checks are:\nChecking the loss at chance performance: Ensure the initial loss matches the expected value for a randomly initialized network. Overfitting a tiny subset of data: Verify that the model can achieve zero cost on a small, representative sample of the data. The three common types of learning rate decay are:\nStep decay Exponential decay 1/t decay Second-order optimization methods require computing and inverting the Hessian matrix, which is computationally expensive and memory-intensive, especially for large neural networks.\nBonus:\nCommon signs of incorrect initialization include: Noisy or unusual activation/gradient distributions in the network layers Slow or stalled learning progress Unrealistic or unstable loss function behavior This quiz provides a comprehensive assessment of the key concepts discussed in the provided text from the CS231n website. It covers topics like gradient checking, sanity checks, learning rate decay, and optimization methods, helping students solidify their understanding of these important aspects of neural network training.\nCS231n: Neural Networks Case Study - Quiz Instructions: Answer the following questions based on the provided text from the CS231n website.\nMultiple Choice:\nWhat type of dataset is used in this case study?\na. MNIST b. CIFAR-10 c. Spiral dataset d. ImageNet What is the primary reason for using the spiral dataset in this case study?\na. It is a simple dataset to understand. b. It is easily linearly separable. c. It is a challenging dataset that requires a non-linear classifier. d. It is a commonly used benchmark dataset for evaluating classifiers. What is the main difference between the linear classifier and the neural network in this case study?\na. The neural network uses a non-linear activation function. b. The neural network has more parameters. c. The neural network uses a different loss function. d. The neural network uses a different optimization algorithm. What is the purpose of the ReLU activation function in the neural network?\na. To introduce non-linearity into the model. b. To prevent overfitting. c. To improve the efficiency of the optimization process. d. To normalize the output of the hidden layer. What is the main advantage of using a neural network over a linear classifier for this dataset?\na. Neural networks are faster to train. b. Neural networks are more robust to noisy data. c. Neural networks can learn complex, non-linear relationships. d. Neural networks are easier to implement. True/False:\nThe spiral dataset is easily linearly separable. (True/False)\nThe cross-entropy loss function is used for both the linear classifier and the neural network. (True/False)\nThe gradient of the ReLU activation function is always 1. (True/False)\nThe backpropagation algorithm is used to update the parameters of the neural network. (True/False)\nThe regularization term in the loss function helps to prevent overfitting. (True/False)\nShort Answer:\nBriefly explain the concept of \u0026ldquo;backpropagation\u0026rdquo; in the context of neural networks.\nDescribe the two main components of the loss function used in this case study.\nWhat is the role of the \u0026ldquo;step size\u0026rdquo; parameter in the gradient descent algorithm?\nHow does the neural network improve the classification accuracy compared to the linear classifier on the spiral dataset?\nBonus:\nWhat are some other common activation functions used in neural networks besides ReLU? Answer Key:\nMultiple Choice:\nc. c. a. a. c. True/False:\nFalse True False True True Short Answer:\nBackpropagation is a method for computing the gradient of the loss function with respect to the parameters of a neural network. It involves propagating the gradient backwards through the network, starting from the output layer and working its way back to the input layer.\nThe loss function consists of two components:\nData loss: Measures the difference between the predicted and actual class labels. Regularization loss: Penalizes large weights, helping to prevent overfitting. The step size parameter controls the size of the steps taken during gradient descent. A larger step size can lead to faster convergence but also a greater risk of overshooting the optimal solution. A smaller step size can lead to slower convergence but may be more accurate.\nThe neural network improves the classification accuracy by introducing non-linearity through the ReLU activation function. This allows the model to learn complex, non-linear relationships between the input features and the output classes, which is necessary to accurately classify the spiral dataset.\nBonus:\nSome other common activation functions used in neural networks besides ReLU include: Sigmoid Tanh Leaky ReLU ELU This quiz provides a comprehensive assessment of the key concepts discussed in the provided text from the CS231n website. It covers topics like data generation, linear classification, neural networks, activation functions, backpropagation, and loss functions, helping students solidify their understanding of these important aspects of neural network training.\n","permalink":"https://csc-bo.github.io/posts/cs231_part1_quiz/","summary":"I used Gemini-1.5-flash to generate all the quizzes and answers. Enjoy!\u0026quot;\nIt covers topics like dataset, linear classification, neural networks, activation functions, backpropagation, and loss functions, helping us understand neural network training.\nQuiz: Image Classification\nMultiple Choice:\nWhat is the primary task of image classification?\na. To identify all the objects present in an image. b. To segment an image into different regions. c. To create a new image based on an input image.","title":"CS231_part1_quiz"},{"content":"Generative and Interactive Visual Intelligence Lecture 12: Self-supervised Learning Pretext tasks Contrastive learning Multisensory supervision Lecture 13: Generative Models Generative Adversarial Network Diffusion models Autoregressive models Lecture 14: OpenAI Sora Diffusion models Lecture 15: Robot Learning Deep Reinforcement Learning Model Learning Robotic Manipulation Lecture 16: Human-Centered Artificial Intelligence Lecture 17: Guest Lecture by Prof. Serena Yeung-Levy Lecture 18: 3D Vision 3D shape representations Shape reconstruction Neural implicit representations ","permalink":"https://csc-bo.github.io/posts/cs231p3/","summary":"Generative and Interactive Visual Intelligence Lecture 12: Self-supervised Learning Pretext tasks Contrastive learning Multisensory supervision Lecture 13: Generative Models Generative Adversarial Network Diffusion models Autoregressive models Lecture 14: OpenAI Sora Diffusion models Lecture 15: Robot Learning Deep Reinforcement Learning Model Learning Robotic Manipulation Lecture 16: Human-Centered Artificial Intelligence Lecture 17: Guest Lecture by Prof. Serena Yeung-Levy Lecture 18: 3D Vision 3D shape representations Shape reconstruction Neural implicit representations ","title":"Notes for CS231n: Part 3"},{"content":"Perceiving and Understanding the Visual World Lecture 5: Image Classification with CNNs History https://zhuanlan.zhihu.com/p/352438848 Higher-level representations, image features Convolution and pooling Lecture 6: CNN Architectures Batch Normalization Transfer learning AlexNet, VGG, GoogLeNet, ResNet Lecture 7: Recurrent Neural Networks RNN, LSTM, GRU Language modeling Image captioning Sequence-to-sequence Lecture 8: Attention and Transformers Self-Attention Transformers Lecture 9: Object Detection and Image Segmentation Single-stage detectors Two-stage detectors Semantic/Instance/Panoptic segmentation Lecture 10: Video Understanding Video classification 3D CNNs Two-stream networks Multimodal video understanding Lecture 11: Visualizing and Understanding Feature visualization and inversion Adversarial examples DeepDream and style transfer ","permalink":"https://csc-bo.github.io/posts/cs231p2/","summary":"Perceiving and Understanding the Visual World Lecture 5: Image Classification with CNNs History https://zhuanlan.zhihu.com/p/352438848 Higher-level representations, image features Convolution and pooling Lecture 6: CNN Architectures Batch Normalization Transfer learning AlexNet, VGG, GoogLeNet, ResNet Lecture 7: Recurrent Neural Networks RNN, LSTM, GRU Language modeling Image captioning Sequence-to-sequence Lecture 8: Attention and Transformers Self-Attention Transformers Lecture 9: Object Detection and Image Segmentation Single-stage detectors Two-stage detectors Semantic/Instance/Panoptic segmentation Lecture 10: Video Understanding Video classification 3D CNNs Two-stream networks Multimodal video understanding Lecture 11: Visualizing and Understanding Feature visualization and inversion Adversarial examples DeepDream and style transfer ","title":"Notes for CS231n: Part 2"},{"content":"Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch \u0026amp; Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Deep Learning Basics Lecture 1: Computer vision overview\nLecture 2: Image Classification with linear Classifiers\nThe data-driven approch What is data-driven approaches linear classification \u0026amp; kNN means? Data-Driven Approach: 1. Colloect a dataset of images and labels. 2. Use Machine Learning algorithms to train a classifier. 3. Evaluate the classifier on new images. There is two basic data-driven aprroaches to image classification\nK-nearest neighbor (kNN) What is kNN?\nkNN is a non-parametric algorithm, meaning it does\u0026rsquo;t make any assumptions about the underlying data distribution. It doesn\u0026rsquo;t build a model until a new instance is presented to it. How does kNN work? Data Preparation: The dataset is divided into a training set and a testing set(or validation set). Distance Calculation: When a new instance(query point) is presented to the model, the algorithm calculates the distance between the new query point and each instance in the training set. The most common distance metrics used in kNN are Euclidean distance(L2 norm), Manhattan distance(L1 norm), and Minkowski distance(Lp norm). K-nearest Neighbor Selection: The algorithm selects the K most similar instances (nearest neighbors) to the query point based on the calculated distances. The value of K is a hyperparameter that needs to be set. Majority vote (classification): In classification problems, the algorithm assigns the query point to the class that is most common among its K nearest neighbors. This is done by taking a majority vote among the classes of the nearest neighbors. Average value (regression): In regression problems, the algorithm predicts the value of the query point by taking the average of the values of its K nearest neighbors. Prediction: The final prediction is made based on the majority vote (classification) or average value (regression). Key takeways In image classification we start with a training set of images and labels, and must predict labels on the test set The K-Nearest Neighbors classifier predicts labels based on the K nearest training examples Distance metric and K are hyperparameters Choose hyperparameters using the validation set Only run on the test set once at the very end! K value: The choice of K is crucial, as it affects the performance of the algorithm. A small value of K can lead to overfitting, while a large value can lead to underfitting. Distance metric: The choice of distance metric can impact the performance of the algorithm, especially when dealing with high-dimensional data. Computational complexity: KNN can be computationally expensive, especially for large datasets, since it requires calculating distances between all instances. Distance Metric L1 (Manhattan)distance $$d_1(I_1,I_2) = \\sum_{p} |I_1^p - I_2^p|$$ L2 (Euclidean) distance $$d_2(I_1,I_2) = \\sqrt{\\sum_{p} (I_1^p - I_2^p)^2}$$ Linear classifiers What is a linear classifier?\nLinear classifier is a function $$f(x,W) = Wx + b$$ W is parameters or weights Algebraic/Visual/ Geometric veiwpoints\nSVM loss What is Multiclass SVM loss?\nGiven an example $(x_i, y_i)$ where $x_i$ is the image and where $y_i$ is the (integer) label, and using the shorthand for the scores vector: $s=f(x_i, W)$ the SVM loss has the form: $$\rL_i = ∑_{j≠y_i}\r\\begin{cases}\r0 \u0026 \\text{if } s_{y_i} ≥ s_j + 1 \\\\\rs_j - s_{y_i} + 1 \u0026 \\text{otherwise}\r\\end{cases}\\\\\r= ∑_{j≠y_i} max(0, s_j - s_{y_i} + 1)\r$$ $S_{y_i} - s_j$ is the difference in scores between correct and incorrect class calculation Q1: What happens to loss if car scores decrease by 0.5 for this training example? To determine the effect of decreasing the car score by 0.5, we need to look at the multiclass SVM loss formula:\n$ L*i = \\sum*{j \\neq y*i} \\max(0, s_j - s*{y_i} + 1) $\nFor this training example, the scores are:\ncat: 1.3 car: 4.9 frog: 2.0 Assuming the correct class is \u0026ldquo;car\u0026rdquo; (i.e., $ y_i = \\text{car} $), the loss is calculated as:\n$ L_i = \\max(0, 1.3 - 4.9 + 1) + \\max(0, 2.0 - 4.9 + 1) $ $ L_i = \\max(0, -2.6) + \\max(0, -1.9) $ $ L_i = 0 + 0 = 0 $\nNow, if the car score decreases by 0.5, the new scores are:\ncat: 1.3 car: 4.4 frog: 2.0 The new loss is:\n$ L_i = \\max(0, 1.3 - 4.4 + 1) + \\max(0, 2.0 - 4.4 + 1) $ $ L_i = \\max(0, -2.1) + \\max(0, -1.4) $ $ L_i = 0 + 0 = 0 $\nThus, the loss remains the same at 0.\nQ2: What is the min/max possible SVM loss $ L_i $? Minimum SVM Loss: The minimum SVM loss occurs when the model perfectly classifies the example with a margin of at least 1. Hence, for all incorrect classes $ j $:\n$ s*j - s*{y_i} + 1 \\leq 0 $\nIn this case, the loss for these classes will be 0. Therefore, the minimum SVM loss $ L_i $ is 0.\nMaximum SVM Loss: The maximum SVM loss occurs when the scores for all incorrect classes $ j $ are infinitely higher than the score for the correct class $ y_i $. As the scores for incorrect classes increase without bound, the loss also increases without bound. Therefore, the maximum SVM loss $ L_i $ is infinity.\nQ3: At initialization $ W $ is small so all $ s \\approx 0 $. What is the loss $ L_i $, assuming $ N $ examples and $ C $ classes? At initialization, if all scores $ s \\approx 0 $:\nThe multiclass SVM loss for a single example is:\n$ L*i = \\sum*{j \\neq y*i} \\max(0, s_j - s*{y_i} + 1) $\nSince $ s_j \\approx 0 $ for all $ j $:\n$ L*i = \\sum*{j \\neq y*i} \\max(0, 0 - 0 + 1) $ $ L_i = \\sum*{j \\neq y_i} 1 $ $ L_i = C - 1 $\n(where $ C $ is the number of classes)\nFor $ N $ examples, the total loss is:\n$ L = \\sum*{i=1}^{N} L_i = \\sum*{i=1}^{N} (C - 1) $ $ L = N(C - 1) $\nSo, the loss $ L_i $ for each example is $ C - 1 $, and the total loss for $ N $ examples is $ N(C - 1) $.\nQ4: What if the sum was over all classes (including $ j = y_i $)? If the sum was over all classes, including $ j = y_i $, the loss formula would be:\n$ L*i = \\sum*{j} \\max(0, s*j - s*{y_i} + 1) $\nSince the term for $ j = y_i $ is included, we have:\n$ \\max(0, s*{y_i} - s*{y_i} + 1) = \\max(0, 1) = 1 $\nSo, the loss would be:\n$ L*i = \\sum*{j \\neq y*i} \\max(0, s_j - s*{y_i} + 1) + 1 $\nThus, it includes an additional $ 1 $ term compared to the standard SVM loss for each example.\nSoftmax loss What is Softmax Classifier(Multinomial Logistic Regression) - Want to interpret raw classifier scores as probabilities $$ s = f(x*i; W) P(Y=k|X=x_i) = \\frac{e^{s_k}}{\\sum*{j=1}^{K} e^{s*j}}$$ Softmax Function\n$L_i = - log P(Y=y_i|X=x_i)$ Kullback-Leibler divergence $$ D*{KL}(P||Q) = \\sum*{y\\in \\mathcal{Y}} P(y) log \\frac{P(y)}{Q(y)}$$ Cross-Entropy $$ H(P,Q)=H(p)+D*{KL}(P||Q)$$ Questions\nQ1: What is the min/max possible softmax loss $ L_i $? Minimum Softmax Loss: The minimum softmax loss occurs when the model is perfectly confident and correct. In this case, the probability $ P(Y = y_i | X = x_i) $ for the correct class is 1. Therefore, the loss is:\n$ L_i = -\\log(1) = 0 $\nThus, the minimum possible softmax loss $ L_i $ is 0.\nMaximum Softmax Loss: The maximum softmax loss occurs when the model is infinitely wrong, meaning it assigns a probability of 0 to the correct class. In practical terms, this happens when the score for the correct class is much lower than the scores for all other classes. As the probability approaches 0, the loss approaches infinity:\n$ L_i = -\\log(0) = \\infty $\nThus, the maximum possible softmax loss $ L_i $ is infinity.\nQ2: At initialization, all $ s_j $ will be approximately equal; what is the softmax loss $ L_i $, assuming $ C $ classes? At initialization, if all scores $ s_j $ are approximately equal, the softmax probabilities for each class will be evenly distributed. For $ C $ classes, each class will have a probability of $ \\frac{1}{C} $.\nThe softmax loss for the correct class $ y_i $ is:\n$ L_i = -\\log\\left(\\frac{1}{C}\\right) = \\log(C) $\nTherefore, the softmax loss $ L_i $ at initialization, assuming $ C $ classes, is $ \\log(C) $.\nQ3: If all scores are small random values, what is the loss? $$-log(\\frac{1}{C})\\\\\rlog(10)\\approx 2.3$$ Cross-Entropy vs SVM Loss Q: What is cross-entropy loss? What is SVM loss? Cross-entropy loss \u0026gt; 0, SVM loss = 0\nQ: What happens to each loss if I slightly change the scores of the last datapoint? Cross-entropy loss will change; SVM loss will stay the same\nSummary\nSoftmax: $L_i = -\\log(\\frac{e^{s_{y_i}}}{\\sum_j e^{s_j}})$ Softmax: A type of loss function used for multi-class classification. It measures the negative log-likelihood of the correct class, encouraging the model to assign higher probabilities to the correct class. L_i: The loss for the i-th training example. s_j: The score for the j-th class. y_i: The true class label for the i-th training example. e^{s_j}: The exponential of the score for the j-th class. ∑_j e^{s_j}: The sum of exponentials of scores for all classes. SVM:$L_i = \\sum_{j \\neq y_i} \\max(0, s_j - s_{y_i} + 1)$\nSVM: A type of loss function used for binary classification. It measures the hinge loss, which encourages the model to have a margin between the scores for the correct and incorrect classes. max(0, s_j - s_{y_i} + 1): The hinge loss, which is zero if the score for the correct class is greater than the score for the incorrect class by at least 1, and otherwise is equal to the difference between the scores plus 1. Lecture 3: Regularization and Optimization\nRegularization\nWhat is regularization? Regularization pushes agiainst fitting the data too well so we dont\u0026rsquo;t fir noise in the data\n$L(W) = \\frac{1}{N} \\sum_{i=1}^{N} L_i(f(x_i, W), y_i) + \\lambda R(W)$ Where:\nL(W) is the overall loss function N is the number of training examples L_i is the loss function for the i-th training example f(x_i, W) is the model\u0026rsquo;s prediction for the i-th training example y_i is the true label for the i-th training example R(W) is the regularization term λ is the regularization strength (hyperparameter)\nL2 regularization: $R(W) = \\sum_k \\sum_l W_{k,l}^2$\nL2 regularization: A type of regularization that penalizes the sum of squared weights. It encourages the model to have smaller weights, and likes to “spread out” the weights which can prevent overfitting. R(W): The regularization term, which is added to the loss function to penalize complex models. W: The weight matrix of the model. k, l: Indices for the rows and columns of the weight matrix. W_{k,l}^2: The square of the weight at row k and column l. L1 regularization: $R(W) = \\sum_k \\sum_l |W_{k,l}|$\nL1 regularization: A type of regularization that penalizes the sum of absolute values of weights. It encourages the model to have sparse weights, meaning that many weights are set to zero. This can help with feature selection and prevent overfitting. |W_{k,l}|: The absolute value of the weight at row k and column l. Elastic net (L1 + L2): $R(W) = \\sum_k \\sum_l \\beta W_{k,l}^2 + |W_{k,l}|$\nElastic net: A type of regularization that combines L1 and L2 regularization. It encourages both sparsity and smaller weights. β: A hyperparameter that controls the balance between L1 and L2 regularization. Dropout, Batch normalization, Stochastic depth, fractinal pooling, etc\nWhy regularize?\nExpress pregerences over weights Make the model simple so it works on test data Improve optimization by adding curvature Stochastic Gradient Descent\nGradient descent is an iterative optimization algorithm used to find the minimum of a function. It\u0026rsquo;s widely used in ML to train models by adjusting their parameters to minimize the loss function. Here\u0026rsquo;s a breakdown:\n1. The Goal:\nImagine you\u0026rsquo;re trying to find the lowest point in a valley. You don\u0026rsquo;t know where it is, but you can see the slope of the ground around you. In ML, the \u0026ldquo;valley\u0026rdquo; is represented by the loss function, which measures how well your model performs. The goal is to find the set of model parameters that minimize this loss. 2. How it Works:\nStart at a random point: You begin with an initial guess for the model parameters (like starting at a random spot in the valley). Calculate the gradient: The gradient is a vector that points in the direction of the steepest ascent of the function. In our valley analogy, it\u0026rsquo;s like looking at the slope of the ground to see which direction is uphill. Take a step in the opposite direction: To move towards the minimum (the lowest point), you take a small step in the direction opposite to the gradient. This is like walking downhill. Repeat: You continue calculating the gradient and taking steps in the opposite direction until you reach a point where the gradient is close to zero. This indicates you\u0026rsquo;ve found a local minimum (a point where the function is lower than its immediate neighbors). 3. Key Concepts:\nLearning rate: This parameter controls how big each step you take is. A large learning rate can lead to overshooting the minimum, while a small learning rate can make the optimization process very slow. Local vs. global minimum: Gradient descent can get stuck in a local minimum, which is a point where the function is lower than its neighbors, but not the absolute lowest point. Finding the global minimum is often difficult, but good initialization and other optimization techniques can help. Cost function: The function that measures how well your model performs. It\u0026rsquo;s the function you\u0026rsquo;re trying to minimize. 4. Applications in Machine Learning:\nTraining neural networks: Gradient descent is the workhorse algorithm for training deep learning models. Linear regression: Finding the best line of fit for a dataset. Logistic regression: Classifying data points into different categories. Many other machine learning algorithms: Gradient descent is a versatile tool used in a wide range of machine learning tasks. Visual Analogy:\nImagine you\u0026rsquo;re trying to find the lowest point in a valley. You can\u0026rsquo;t see the entire valley, but you can see the slope of the ground around you. You start at a random point and take small steps in the direction that is downhill. You keep taking steps until you reach a point where the slope is close to zero, indicating that you\u0026rsquo;ve found a low point in the valley.\nStochastic Gradient Descent. It\u0026rsquo;s a variation of the standard Gradient Descent algorithm, and it\u0026rsquo;s widely used in machine learning, especially for training large models. Here\u0026rsquo;s a breakdown:\n1. The Problem with Standard Gradient Descent:\nStandard Gradient Descent calculates the gradient of the loss function using all training examples. This can be computationally expensive, especially when dealing with large datasets. 2. How SGD Solves the Problem:\nStochasticity: Instead of using all training examples, SGD randomly selects a mini batch of examples (32 / 64 / 128 common ) to calculate the gradient. Faster Updates: Since it\u0026rsquo;s working with a smaller subset of data, SGD can update the model parameters much faster than standard Gradient Descent. Noise: The randomness introduced by using mini-batches can actually help the algorithm escape local minima and find better solutions. 3. Key Concepts:\nMini-batch size: This determines how many examples are used to calculate the gradient in each iteration. A larger mini-batch size reduces noise but slows down training. Epoch: One pass through the entire training dataset. SGD typically runs for multiple epochs. Learning rate: Controls the step size during parameter updates. 4. Advantages of SGD:\nSpeed: Much faster than standard Gradient Descent for large datasets. Escape local minima: The noise introduced by mini-batches can help the algorithm find better solutions. Online learning: SGD can be used for online learning, where new data points arrive continuously. 5. Disadvantages of SGD:\nNoisy updates: The randomness can make the optimization process less stable. Learning rate tuning: Finding the optimal learning rate can be challenging. 6. Applications in Machine Learning:\nTraining deep neural networks: SGD is the most common optimization algorithm used for training deep learning models. Natural language processing: Training language models like BERT and GPT-3. Computer vision: Training image classification and object detection models. In multiple dimensions, the gradient is the vector of (partial derivatives) along each dimension. The slope in any direction is the dot product of the direction with the gradient. The direction of steepest descent is the negative gradient\nMomentum, SGD, SGD+Momentum, RMSProp, Adam, AdamW\n1. Gradient Descent (GD):\nThe foundation: GD iteratively updates model parameters (weights) to minimize a loss function. It calculates the gradient of the loss function with respect to the parameters and takes a step in the opposite direction of the gradient. Issue: Can get stuck in local minima, especially in complex landscapes. It can also oscillate around the minimum if the learning rate is too high. 2. Stochastic Gradient Descent (SGD):\nThe speed boost: SGD uses a random subset of the training data (a mini-batch) to calculate the gradient in each iteration. This makes it significantly faster than GD, especially for large datasets. Issue: The noisy updates from mini-batches can lead to oscillations and slow convergence. 3. SGD with Momentum (SGD+Momentum):\nSmoother steps: Momentum introduces a \u0026ldquo;velocity\u0026rdquo; term that accumulates the gradient over multiple iterations. This helps the algorithm: Escape local minima: The momentum term can help the algorithm \u0026ldquo;roll over\u0026rdquo; local minima. Reduce oscillations: It smooths out the updates, making convergence more stable. 4. RMSprop (Root Mean Square Propagation):\nAdaptive learning rates: RMSprop adapts the learning rate for each parameter individually. It uses a moving average of squared gradients to scale the learning rate. Faster convergence: It can speed up convergence by adjusting the learning rate for each parameter based on the history of its gradients. Less sensitive to learning rate: It\u0026rsquo;s more robust to the choice of learning rate than SGD or SGD+Momentum. 5. Adam (Adaptive Moment Estimation):\nCombining the best: Adam combines the momentum term (like SGD+Momentum) with adaptive learning rates (like RMSprop). It estimates the first and second moments of the gradients to adjust the learning rate for each parameter. Efficient and robust: Adam is often considered a very effective and robust optimization algorithm. 6. AdamW (Adam with Weight Decay):\nRegularization boost: AdamW adds a weight decay term to the Adam optimizer. Weight decay helps prevent overfitting by penalizing large weights, similar to L2 regularization. Improved generalization: AdamW often leads to better generalization performance compared to standard Adam. 7. Nesterov Momentum:\nNesterov Momentum is an improvement over standard Momentum that looks \u0026ldquo;ahead\u0026rdquo; in the direction of the momentum before calculating the gradient. This allows it to anticipate the future direction of the parameter update and potentially avoid sharp turns or oscillations. Faster Convergence: Nesterov Momentum often converges faster than standard Momentum, especially in situations where the loss function has many local minima. Smoother Updates: It helps to smooth out the updates, reducing oscillations and making convergence more stable. 8. Adagrad:\nAdaGrad adapts the learning rate for each parameter individually based on the history of its gradients. It accumulates the squared gradients for each parameter and uses this information to scale the learning rate. Adaptive Learning Rates: AdaGrad automatically adjusts the learning rate for each parameter, making it more robust to different scales and magnitudes of gradients. Less Sensitive to Learning Rate: It\u0026rsquo;s less sensitive to the choice of the initial learning rate than standard Gradient Descent or Momentum. Learning Rate Decay: The learning rate can decay too quickly, especially in the later stages of training, leading to slow convergence. Analogy:\nImagine you\u0026rsquo;re trying to find the lowest point in a valley.\nGD: You carefully measure the slope at your current location and take a small step downhill. SGD: You quickly glance at a few random spots in the valley and take a step based on the average slope you see. SGD+Momentum: You use a running average of the slopes you\u0026rsquo;ve seen, allowing you to take larger steps downhill and escape small dips. RMSprop: You adjust your step size based on how steep the ground is in each direction, taking smaller steps in steeper areas. Adam: You combine the momentum and adaptive learning rate techniques for a more efficient and stable descent. AdamW: You add a mechanism to prevent you from straying too far from the center of the valley, helping you find a more general solution. Nesterov Imagine you\u0026rsquo;re rolling a ball down a hill. Nesterov Momentum is like looking ahead slightly in the direction you\u0026rsquo;re rolling before making a decision about the next step. This helps you avoid sharp turns and find a smoother path to the bottom. AdaGrad Imagine you\u0026rsquo;re walking on a terrain with different types of surfaces. AdaGrad is like adjusting your step size based on how rough or smooth the terrain is under each foot. You take smaller steps on rough terrain to avoid stumbling and larger steps on smooth terrain to move faster. Key Takeaways:\nSGD+Momentum, RMSprop, Adam, and AdamW are all improvements upon the basic Gradient Descent algorithm. They address the limitations of GD and SGD by introducing momentum, adaptive learning rates, and regularization. The choice of optimization algorithm depends on the specific problem and dataset. Learning rate schedules\nlearning rate scheduling is about how to adjust the learning rate of your optimization algorithm (like SGD, Adam, etc.) during training to improve performance.\nThe Basic Idea:\nEarly Stages: You usually start with a relatively high learning rate to quickly find a good region in the parameter space (the space of all possible model weights). Later Stages: As training progresses, you often want to decrease the learning rate. This helps the model fine-tune its parameters and avoid overshooting the optimal solution. Learning Rate Schedules:\nCommon learning rate schedules:\nThe Step: The image suggests a simple approach: reduce the learning rate by a fixed factor (e.g., 0.1) at specific epochs. This is often done for ResNet models, where the learning rate is multiplied by 0.1 after epochs 30, 60, and 90.\nCosine: The learning rate follows a cosine curve, decreasing gradually from the initial learning rate to zero. This schedule often provides a smooth transition and avoids abrupt changes in the learning rate.\nLinear: The learning rate decreases linearly from the initial learning rate to zero. This schedule is simpler to implement but might not be as effective as the cosine schedule.\nInverse Square Root: The learning rate decreases inversely proportional to the square root of the epoch number. This schedule is often used in deep learning, as it provides a gradual decay that is not too fast or too slow.\nLinear Warmup: A warmup period where the learning rate increases linearly from zero to the initial learning rate. This helps the model to start with a small learning rate and gradually increase it as the training progresses.\nLinear warmup is the solution for **High Initial Learning Rates: **Using a high initial learning rate can cause the training process to become unstable. The loss function might explode (increase rapidly) instead of decreasing, making the model difficult to train. Key Terms:\nα₀: Initial learning rate (the starting learning rate). αₜ: Learning rate at epoch t (the learning rate at a specific training epoch). T: Total number of epochs (the total number of times you go through the entire training dataset). Why Use Learning Rate Scheduling?\nImproved Convergence: It helps the model converge to a better solution by adjusting the learning rate based on the training progress. Reduced Oscillations: It can prevent the model from oscillating around the optimal solution, especially in the later stages of training. Better Generalization: It can help the model generalize better to unseen data by preventing overfitting. In practice：\nAdam(W) is a good default choice in many cases; it often works ok even with constant learning rate SGD with Momentum can outperform Adam but may require more tuning of LR and schedule If you can afford to do full batch updates then look beyond 1^st order optimization (2^nd order and beyond), Try out L-BFGS The Problem:\nHessian Inversion: The Hessian matrix has a size proportional to the square of the number of parameters (N^2) in the model, the Newton\u0026rsquo;s method update rule requires inverting the Hessian matrix, which has a computational complexity of O(N^3) (N being the number of parameters). which becomes prohibitively expensive for deep learning models with billions or trillions of parameters. The Quasi-Newton Approach:\nApproximation: Quasi-Newton methods avoid directly inverting the Hessian. Instead, they build an approximation of the inverse Hessian using rank-1 updates over time. This approximation is much more efficient to compute, with a complexity of O(N^2). BFGS (Broyden–Fletcher–Goldfarb–Shanno): The most popular Quasi-Newton method is the BFGS algorithm. It iteratively updates the approximation of the inverse Hessian based on the current gradient and the previous gradient. L-BFGS (Limited Memory BFGS):\nMemory Efficiency: L-BFGS is a variation of BFGS that is even more efficient in terms of memory usage. It doesn\u0026rsquo;t store the full inverse Hessian matrix. Instead, it keeps track of a limited number of past gradient and parameter updates. This makes it suitable for very large models where storing the entire Hessian would be impractical. Key Advantages:\nComputational Efficiency: Quasi-Newton methods are significantly faster than Newton\u0026rsquo;s method, especially for large models. Accuracy: They often achieve better accuracy than first-order methods like SGD or Momentum, as they incorporate information about the curvature of the loss function. In Summary:\nQuasi-Newton methods offer a practical compromise between the accuracy of second-order optimization and the computational efficiency of first-order methods. BFGS and L-BFGS are popular algorithms that effectively approximate the inverse Hessian, making them suitable for training large deep learning models. The Alternative: First-Order Methods: First-order methods like SGD, Adam, etc., are computationally much cheaper because they only require calculating the gradient, which has a complexity of O(N). While they might not be as accurate as second-order methods, their efficiency makes them the preferred choice for training large deep learning models. Lecture 4: Neural Networks and Backpropagation\nMulti-layer Perceptron, (Fully Connected) Neural Networks Backpropagation Let\u0026rsquo;s break down Neural Networks and Backpropagation: 1. Neural Networks: The Basics\nImagine a network of interconnected nodes, like a simplified model of the human brain. These nodes, called \u0026ldquo;neurons,\u0026rdquo; process information and pass it along to other neurons. Here\u0026rsquo;s how it works:\nInput Layer: This layer receives the raw data, like pixels in an image or words in a sentence. Hidden Layers: These layers are the \u0026ldquo;thinking\u0026rdquo; part of the network. They use mathematical functions to transform the input data, extracting features and patterns. Output Layer: This layer produces the final prediction, like a classification label or a numerical value. Key Concepts:\nWeights: Each connection between neurons has a \u0026ldquo;weight\u0026rdquo; associated with it. These weights determine the strength of the connection and influence how information is passed along. Activation Functions: Each neuron applies a \u0026ldquo;non-linear\u0026rdquo; function to its input, adding complexity and allowing the network to learn intricate relationships. Learning: The network learns by adjusting its weights based on the feedback it receives. The goal is to minimize the difference between the network\u0026rsquo;s predictions and the actual values. Why are they useful?\nNeural networks are powerful because they can learn complex patterns from data. They excel at tasks like:\nImage Recognition: Identifying objects in images. Natural Language Processing: Understanding and generating text. Machine Translation: Translating text from one language to another. Speech Recognition: Converting spoken words into text. 2. Backpropagation: The Learning Engine\nBackpropagation is the algorithm that allows neural networks to learn. It\u0026rsquo;s a clever way to calculate how much each weight in the network contributes to the error in the output. Think of it like this:\nForward Pass: Input data is fed through the network, and the output is generated. Error Calculation: The difference between the predicted output and the actual output is calculated. Backward Pass: The error is \u0026ldquo;propagated\u0026rdquo; backwards through the network, adjusting the weights along the way. How it works (simplified):\nCalculate the error: How far off was the network\u0026rsquo;s prediction? Find the \u0026ldquo;blame\u0026rdquo;: Which weights contributed most to the error? Adjust the weights: Slightly change the weights to reduce the error. Key Points:\nGradient Descent: Backpropagation uses a mathematical technique called gradient descent to find the optimal weights. Chain Rule: Backpropagation relies on the chain rule of calculus to calculate the gradients efficiently. Iterative Process: The network learns through many iterations of forward and backward passes, gradually improving its accuracy. In a nutshell:\nNeural networks are powerful tools for learning complex patterns from data. Backpropagation is the algorithm that enables them to learn by adjusting their internal weights based on the error in their predictions.\n","permalink":"https://csc-bo.github.io/posts/cs231/","summary":"Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch \u0026amp; Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Deep Learning Basics Lecture 1: Computer vision overview\nLecture 2: Image Classification with linear Classifiers\nThe data-driven approch What is data-driven approaches linear classification \u0026amp; kNN means? Data-Driven Approach: 1.","title":"Notes for CS231n: Part 1"},{"content":"Machine Learning Courses UofT MAT334 Complex Variables MAT223\u0026amp;MAT224 Linear Algebra I\u0026amp;II MAT236 Vector Calculus STA302H1/STA1001: Methods of Data Analysis I MIT 6.S191 Introduction to Deep Learning Sequence to Sequence Computer Vision Reinforcement Learning Generative Modeling Lab 1: Tensorflow, Tensor, Auto differentiation, GradientTape, RNN Lab 2: MNIST, CNN, VAEs, Debiasing Facial Detection extra lab llm finetune self driving Standford Artificial Intelligence a. CS 221 b. At least four of: CS 223A, 224N, 224S, 224U, 224V, 224W, 228, 229, 231A, 231N, 234, 237A, 237B, 238 c. A total of at least 21 units from (a), (b) and the following: CS 205L, 224R, 225A, 227B, 229M, 230, 233, 235, 236, 239, 246, 257, 270, 271, 273A, 273B, 274, 275, 279, 281, 322, 324, 325B, 326, 327A, 329, 330, 331B, 332, 333, 345, 348N, 361, 368, 371, 375, 377**, 379**, 398, 399**, 428A, 428B, 432; EE 263, 276, 278, 364A, 364B, 378B; ENGR 205, 209A; MS\u0026amp;E 226, 252; PSYCH 209; STATS 202, 315A, 315B [ ] CS229(Required): Machine Learning [ ] CS231n: Deep Learning for Computer Vision Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch \u0026amp; Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Lecture 1: Computer vision overview Deep Learning Basics Lecture 2: Image Classification with linear Classifiers The data-driven approch K-nearest neighbor Linear classifiers Algebraic/Visual/ Geometric veiwpoints SVM and Softmax loss Lecture 3: Regularization and Optimization Regularization Stochastic Gradient Descent Momentum, AdaGrad, Adam Learning rate schedules Lecture 4: Neural Networks and Backpropagation Multi-layer Perceptron Backpropagation Perceiving and Understanding the Visual World Lecture 5: Image Classification with CNNs History Higher-level representations, image features Convolution and pooling Lecture 6: CNN Architectures Batch Normalization Transfer learning AlexNet, VGG, GoogLeNet, ResNet Lecture 7: Recurrent Neural Networks RNN, LSTM, GRU Language modeling Image captioning Sequence-to-sequence Lecture 8: Attention and Transformers Self-Attention Transformers Lecture 9: Object Detection and Image Segmentation Single-stage detectors Two-stage detectors Semantic/Instance/Panoptic segmentation Lecture 10: Video Understanding Video classification 3D CNNs Two-stream networks Multimodal video understanding Lecture 11: Visualizing and Understanding Feature visualization and inversion Adversarial examples DeepDream and style transfer Generative and Interactive Visual Intelligence Lecture 12: Self-supervised Learning Pretext tasks Contrastive learning Multisensory supervision Lecture 13: Generative Models Generative Adversarial Network Diffusion models Autoregressive models Lecture 14: OpenAI Sora Diffusion models Lecture 15: Robot Learning Deep Reinforcement Learning Model Learning Robotic Manipulation Lecture 16: Human-Centered Artificial Intelligence Lecture 17: Guest Lecture by Prof. Serena Yeung-Levy Lecture 18: 3D Vision 3D shape representations Shape reconstruction Neural implicit representations [ ] CS236(*)： Deep Generative Models [ ] CS330(*): Deep Multi-Task and Meta Learning [ ] CS234(*): Reinforcement Learning [ ] CS224n(*): Natural Language Processing with Deep Learning [ ] CS224u(*): Natural Language Understanding [ ] CS224w(*): Machine Learning with Graphs [ ] CS237a: Principles of Robot Autonomy I [ ] CS237b: Principles of Robot Autonomy II [ ] CS326: Topics in Advanced Robotic Manipulation [ ] CS336: Language Modeling from Scratch UCB [ ] CS267 Application of Parallel Computers Online Resources [ ] Pytorch freeCodeCamp: PyTorch for Deep Learning \u0026amp; Machine Learning\nFundamentals: Tensor Exercise Workflow Fundamentals: Data, Model, Trainning, Inference, Save \u0026amp; Load Model Exercise Neural Network Classification: Binary, Multi-class, Multi-label Exercise Computer Vision: CNN Exercise Custom Dataset Going Modular Transfer learning Experiment Tracking Paper Replicating Model Deployment StatQuest：Introduction to Pytorch\nBuild a CNN detect handwrite aphabets\nRNN / LSTM\nTransformer\nViT\n\u0026hellip;\n[ ] Neural Networks: Zero to Hero The spelled-out intro to neural networks and backpropagation: building micrograd micrograd exercise The spelled-out intro to language modeling: building makemore Building makemore Part 2: MLP E1 E2 Building makemore Part 3: Activations \u0026amp; Gradients, BatchNorm the forward pass activations, backward pass gradients Batch Normalization typical diagnostic tools and visualizations Residual connections and the Adam optimizer E1 E2 Building makemore Part 4: Becoming a Backprop Ninja Exercise 1 Exercise 2 Exercise 3 Exercise 4 Building makemore Part 5: Building a WaveNet Let\u0026rsquo;s build GPT: from scratch, in code, spelled out. Let\u0026rsquo;s build the GPT Tokenizer Neural Networks by 3Blue1Brown Neural Networks and Deep Learning Gradient Descent, How machines learn Backpropagation Backpropagation Calculus GPT(Generative Pre-trained Transformer) Essence of linear algebra review my linear algebra Essence of calculus review my calculus Fun Projects Stock Trend/Pricing Prediction LSTM Tranformer MultiModel Face reconginition CNN ","permalink":"https://csc-bo.github.io/posts/roadmap_ml/","summary":"Machine Learning Courses UofT MAT334 Complex Variables MAT223\u0026amp;MAT224 Linear Algebra I\u0026amp;II MAT236 Vector Calculus STA302H1/STA1001: Methods of Data Analysis I MIT 6.S191 Introduction to Deep Learning Sequence to Sequence Computer Vision Reinforcement Learning Generative Modeling Lab 1: Tensorflow, Tensor, Auto differentiation, GradientTape, RNN Lab 2: MNIST, CNN, VAEs, Debiasing Facial Detection extra lab llm finetune self driving Standford Artificial Intelligence a. CS 221 b. At least four of: CS 223A, 224N, 224S, 224U, 224V, 224W, 228, 229, 231A, 231N, 234, 237A, 237B, 238 c.","title":"Roadmap for machine lerning"},{"content":"What is Deep Learning? Deep learning is a subfield of artificial intelligence (AI) that focuses on training artificial neural networks to learn and make intelligent decisions. These networks are inspired by the structure and function of the human brain, and they can learn from vast amounts of data to perform tasks such as image recognition, natural language processing, and machine translation.\nHow does Deep Learning work? Deep learning models are typically composed of multiple layers of interconnected nodes, known as neurons. Each neuron receives input data, performs a simple calculation, and passes the result to the next layer. By adjusting the weights and connections between neurons during training, the model learns to extract meaningful patterns from the data and make accurate predictions.\nWhat are the applications of Deep Learning? Deep learning has a wide range of applications across various industries, including:\nComputer vision: Image and video recognition, object detection, self-driving cars Natural language processing: Machine translation, sentiment analysis, chatbots Healthcare: Medical image analysis, drug discovery, disease prediction Finance: Fraud detection, risk assessment, algorithmic trading Robotics: Robot control, navigation, manipulation What are the challenges of Deep Learning? Despite its impressive capabilities, deep learning also faces several challenges:\nData requirements: Deep learning models often require large amounts of data to train effectively. Computational cost: Training deep learning models can be computationally expensive and time-consuming. Explainability: Understanding how deep learning models make decisions can be difficult, leading to concerns about transparency and bias. Ethical considerations: The use of deep learning raises ethical concerns related to privacy, fairness, and potential misuse. How can I get started with Deep Learning? There are many resources available to help you get started with deep learning:\nOnline courses: Platforms like Coursera, edX, and Udacity offer various deep learning courses. Books and tutorials: Numerous books and online tutorials cover deep learning concepts and techniques. Open-source libraries: Libraries like TensorFlow, PyTorch, and Keras provide tools for building and training deep learning models. Community forums and groups: Online communities offer a platform to connect with other deep learning enthusiasts and experts. ","permalink":"https://csc-bo.github.io/faq/","summary":"What is Deep Learning? Deep learning is a subfield of artificial intelligence (AI) that focuses on training artificial neural networks to learn and make intelligent decisions. These networks are inspired by the structure and function of the human brain, and they can learn from vast amounts of data to perform tasks such as image recognition, natural language processing, and machine translation.\nHow does Deep Learning work? Deep learning models are typically composed of multiple layers of interconnected nodes, known as neurons.","title":"Deep Learning FAQ"},{"content":"1. Introduction to Deep Learning After watch one of Feifei Li and Geffery Hinton\u0026rsquo;s video on Youtube, I start to gain interest in DeepLearning, then I found this MIT course online that is a very comprehensive introduction for the Deep Learning which perfect for beginners like myself. This blog post marks the beginning of my journey into the world of machine learning, serving as a collection for my course notes and insights from related materials. 2. Course Content Leture 1: Introductin\nThe Perceptron\nPerceptron: Simplified Activation Functions is to introduce non-linearites into the network Sigmoid Hyperbolic Tangent Rectified Linear Unit(ReLU) We can implement this graph on code very easily One perceptron: Draws a line to separate data. binary classification. Multiple perceptrons in layers: Can create complex curves and shapes to separate data What does it do? Essentially, a perceptron learns a linear decision boundary that separates the input space into two regions, each corresponding to a different class. Forward Propagation/Forward Pass:\nrefers to the calculation and storage of intermediate variables (including outputs) for a neural network in order from the input to the output layer.\nThe Neural Networks\nNeural Network: Stacking Perceptrons to form neural network -\u0026gt; MLP: Multi Layer Perceptron Loss: The loss of our network measures the cost incurred from incorrect\nEmpirical Loss\nThe empirical loss measures the total loss over our entire dataset. Also know as Objective function, Cost function, Empirical Risk $$\rJ(W) = \\frac{1}{n} \\sum_{i=1}^{n} L(f(x_i;W),y_i)\r$$ Binary Cross Entropy Loos $$\rJ(W) = - \\frac{1}{n} \\sum_{i=1}^{n} \\log(f(x_i;W)) + (1-y_i) log(1-f(x_i;W))\r$$ Mean Squared Error Loss $$\rJ(W) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - f(x_i;W))^2\r$$ Loss Optimization\n$$\r\\begin{aligned}\rW^* = \\arg \\mathop{min}\\limits_{W} \\frac{1}{n} \\sum_{i=1}^{n} L(f(x_i;W),y_i)\\\\\rW^* = \\arg \\mathop{min}\\limits_{W} J(W)\\\\\r\\text {Where as: } W=\\{W_0, W_1, ...\\}\r\\end{aligned}\r$$ We want to find the network weights that achieve the lowest loss\nOur loss is a function of the network weights\nWe can use gradient descent to find the weights that minimize the loss\nGradient Descent\nInitialize weights randomly $\\sim N(0,\\sigma^2)$ Loop until convergence Compute gradients, $ \\frac{\\partial J(W)}{\\partial W}$ Update weights: $ W \\leftarrow W - \\eta \\frac{\\partial J(W)}{\\partial W}$ Return weights Backpropagation:\nBackpropagation is the application of gradient descent in deep learning, used to compute gradients of weights in neural networks. It employs the chain rule from calculus to efficiently propagate errors backwards, guiding updates to each layer\u0026rsquo;s parameters. By calculating the partial derivatives of the loss function with respect to every weight, it determines how each weight should be adjusted to minimize the loss.\nLearning Rate\nMomentum or Adaptive Learning Rates: For improved convergence, incorporate momentum or adaptive learning rate techniques like Nesterov Accelerated Gradient (NAG), RMSprop, or Adam.\nMini-Batch Gradient Descent\nInstead of using all data points at once, use a mini-batch of B data points to compute the gradient. This reduces the computational cost and improves convergence. $$\r\\frac{\\partial J(W)}{\\partial W} = \\frac{1}{B} \\sum_{i=1}^{B} \\frac{\\partial J_k(W)}{\\partial W}\\\\\r$$ $$\r\\text {Then update weights: }W \\leftarrow W - \\eta \\frac{\\partial J(W)}{\\partial W}\r$$ More accurate estimation of gradient Allow for larger learning rates Smoother convergence Fast training! Can parallelize computation and achieve significant speed increases on GPU\u0026rsquo;s Optimization Algorithm SGD(Stochastic Gradient Descent) Adam Adadelta Adagrad RMSProp Real World Technique\nMini-batches Fitting Underfitting: Model does not have capacity to fully learn the data Ideal fit Overfitting: Too complex, extra parameters, does not generalize well Regularization: Improve generalization of our model on unseen data Dropout During training, randomly set some activations to 0\nTypically \u0026lsquo;drop\u0026rsquo; 50% of activations in layer\nForces network to not rely on any 1 node\nEarly Stopping\nStop training before we have a chance to overfit Leture 2: Deep Sequence Modeling\nI\u0026rsquo;m gonna write this note in a more practical way, and from my SDE perspective. Code is nessesary for me to understand the concept.\nRecurrent Neural Networks(RNNs)\nMany to One: Sentiment Classification One to Many: Text Generation, Image Captioning Many to Many: Translation \u0026amp; Forecasting, Music Generation class myRNNCell(tf.keras.layers.Layer):\rdef __init__(self, rnn_units, input_dim, output_dim):\rsuper(myRNNCell, self).__init__()\r# Initialize weight matrices\rself.W_xh = self.add_weight([rnn_units, input_dim])\rself.W_hh = self.add_weight([rnn_units, rnn_units])\rself.W_hy = self.add_weight([output_dim, rnn_units])\r# Initialize hidden state to zeros\rself.h = tf.zeros([rnn_units, 1])\rdef call(self, x):\r# the input x corresponds to x_t in the graph\r# Update the hidden state\r# h_t = tanh(W_hh * h_{t-1} + W_xh * x_t)\rself.h = tf.math.tanh( self.W_hh * self.h + self.W_xh * x)\r# Compute the output, y^hat_t = W_hy * h\routput = self.W_hy * self.h\r# Return the current output and hidden state\rreturn output, self.h\r# tensorflow method\rtf.keras.layers.SimpleRNN(rnn_units) The implementation is highly identical to the graphical representation. This code provided by Professor slide not complete. But we can get the idea. I watched Andrej Karpathy\u0026rsquo;s video, he actully only need an image to implement Neural Networks. Combine the graph and code help me to understand the concept. Endcoding Language for a Neural Network\nEmbbedding: transform indexes into a vector of fixed size. One-hot embedding: One-hot embedding is a way to represent categorical data as binary vectors. Each category is represented by a binary vector, where only one element is 1, and the rest are 0. This representation is useful for representing categorical data, such as words in a language, where each word is represented by a unique index. RNN feedforwar and backpropagation graph RNN Problems\nExploding Gradients: Many values \u0026gt; 1, mutiply many large numbers together lead to unstable training, oscillations in the loss function, and ultimately, the model failing to converge to a good solution. Gradient clipping: A technique used to prevent exploding gradients by clipping the gradients to a maximum value. Vanishing Gradients: Many values \u0026lt; 1, mutiply many small numbers together cause the weights to not update at all. then the bias parameters capture short-term dependencies. result no long-term dependencies. Activation functions: Using ReLU prevents f\u0026rsquo; from shriking the gradients when x \u0026gt; 0 Parameter Initialization: Initializing weights to identity matrix, initialize biases to zero. this prevent the weight from shrinking to zero Gated Cells: use gates to selectively add or remove information within each recurrent unit with gated cell (LSTMs, GRUs) LSTMs(Long Short-Term Memory)\nGated LSTM cells control information flow Key Concepts Maintain a cell state Use gates to control the flow of information Forget gate gets rid of irrelevant information Store relevent information from current input Selectively update cell state Output gate returns a filtered version of the cell state Backpropagation through time with partially uninterrupted gradient flow Limitations of RNNs\nEncoding bottleneck Slow, no parallelization Not long memory Attention is all you need(transformer)\nAttention mechanism: orignal from a paper of RNN, this is like RNN search. Allow network to search relevant information Self-Attention: identify and attend to most important features in input Encode position information Extract query, key, value for search Comput attention weighting Extract Features with high attention These steps form a self-attention head that plug into a larger network. Each head attends to a different part of the input. Self-Attention Applied: Language Model: Transformers, GPT, BERT Biological Sequence: AlphaFold2 Computer Vison: Vision Transformers Tranfomer Structure Leture 3: Deep Computer Vision\nFoundation: Images are numbers 2D image Vector of pixel values Convolution extract features with convolution Steps: Apply a set of weights - a filter - to extract local features Use multiple filters to extract different features Spatially share parameters of each filter CNNs(Convolutional Neural Networks) Convolution: Apply filters to generate feature maps Non-linearity: Often ReLU Pooling: Downsampling operation on each feature map. import tensorflow as tf\rdef generate_model():\rmodel = tf.keras.Sequential([\r# first convolutional layer\rtf.keras.layers.Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;, input_shape=(28, 28, 1)),\rtf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\r# second convolutional layer\rtf.keras.layers.Conv2D(64, (3, 3), activation=\u0026#39;relu\u0026#39;),\rtf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\r# fully connected classifier\rtf.keras.layers.Flatten(),\rtf.keras.layers.Dense(1024, activation=\u0026#39;relu\u0026#39;),\rtf.keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) # 10 outputs\r])\rreturn model Applications Classification Breast Cancer Screening Object Detection R-CNNs alorithm: Find regions that we think have objects. Use CNN to classify Semantic Segmentation: Fully Convolutional Networks(FCN) Biomedical Image Analysis Navigation from Vision End-to-End Autonomos Navigation Leture 4: Deep Generative Modeling\nSupervised/Unsupervised Learning Gnerative Models Debiasing Outlier Detection VAEs (Autoencoders and Variational Autoencoders) Autoencoders GANS (Generative Adversarial Networks) Basic Concepts Applications Leture 5: Deep Reinforcement Learning\nRL Concepts Agent Environment Actions Observations State Reward The Q-function captures the expected total future reward an agent in state,s, can receive by executing a certain action,a RL Learning Algorithms Value Learning Policy Learning DQN (Deep Q-Networks) Concepts Training Summary Downside of Q-learning Complexity Can model scenarios where the action space is discrete and small Cannot handle continuous action spaces Flexibility Policy is deterministically computed from the Q function by maximizing the reward Cannot learn stochastic policies Policy Gradient Methods Key ideas Directly optimize the policy pi(s) Continuouse action spaces Traning Initialize the agent Run a policy until termination Record all states, actions, rewards Decrease probability of actions that resulted in low reward Increase probability of actions that resulted in high reward Applications Alpha Go Lecture 6: Limitation and New Frontiers\nModule Summaries: Briefly summarize each module of the course, highlighting key concepts and algorithms covered. Lecture Notes: You can include your personal notes from lectures, focusing on important points and areas of difficulty. Assignments and Projects: Discuss the assignments and projects assigned in the course, sharing your approach and solutions. 3. Resources Share any additional resources you found helpful while taking the course, such as:\nOnline Tutorials: Links to relevant online tutorials or articles that provide further explanation on specific topics.\nResearch Papers: References to important research papers in the field of deep learning.\nSoftware Libraries: Information about deep learning libraries used in the course, such as TensorFlow or PyTorch.\n4. Personal Insights and Reflections Share your personal thoughts and reflections on the course, including:\nChallenges Faced: Discuss any challenges you encountered while learning the material and how you overcame them. Key Takeaways: Highlight the most important things you learned from the course. Future Applications: Explore potential applications of deep learning that you find interesting. 5. Conclusion Conclude your post by summarizing your experience with the course and expressing your thoughts on the field of deep learning as a whole.\n","permalink":"https://csc-bo.github.io/posts/mit6s191/","summary":"1. Introduction to Deep Learning After watch one of Feifei Li and Geffery Hinton\u0026rsquo;s video on Youtube, I start to gain interest in DeepLearning, then I found this MIT course online that is a very comprehensive introduction for the Deep Learning which perfect for beginners like myself. This blog post marks the beginning of my journey into the world of machine learning, serving as a collection for my course notes and insights from related materials.","title":"Notes for MIT 6.S191"}]