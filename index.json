[{"content":"Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch \u0026amp; Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Deep Learning Basics Lecture 1: Computer vision overview\nLecture 2: Image Classification with linear Classifiers\nThe data-driven approch What is data-driven approaches linear classification \u0026amp; kNN means? Data-Driven Approach: 1. Colloect a dataset of images and labels. 2. Use Machine Learning algorithms to train a classifier. 3. Evaluate the classifier on new images. There is two basic data-driven aprroaches to image classification\nK-nearest neighbor (kNN) What is kNN?\nkNN is a non-parametric algorithm, meaning it does\u0026rsquo;t make any assumptions about the underlying data distribution. It doesn\u0026rsquo;t build a model until a new instance is presented to it. How does kNN work? Data Preparation: The dataset is divided into a training set and a testing set(or validation set). Distance Calculation: When a new instance(query point) is presented to the model, the algorithm calculates the distance between the new query point and each instance in the training set. The most common distance metrics used in kNN are Euclidean distance(L2 norm), Manhattan distance(L1 norm), and Minkowski distance(Lp norm). K-nearest Neighbor Selection: The algorithm selects the K most similar instances (nearest neighbors) to the query point based on the calculated distances. The value of K is a hyperparameter that needs to be set. Majority vote (classification): In classification problems, the algorithm assigns the query point to the class that is most common among its K nearest neighbors. This is done by taking a majority vote among the classes of the nearest neighbors. Average value (regression): In regression problems, the algorithm predicts the value of the query point by taking the average of the values of its K nearest neighbors. Prediction: The final prediction is made based on the majority vote (classification) or average value (regression). Key takeways In image classification we start with a training set of images and labels, and must predict labels on the test set The K-Nearest Neighbors classifier predicts labels based on the K nearest training examples Distance metric and K are hyperparameters Choose hyperparameters using the validation set Only run on the test set once at the very end! K value: The choice of K is crucial, as it affects the performance of the algorithm. A small value of K can lead to overfitting, while a large value can lead to underfitting. Distance metric: The choice of distance metric can impact the performance of the algorithm, especially when dealing with high-dimensional data. Computational complexity: KNN can be computationally expensive, especially for large datasets, since it requires calculating distances between all instances. Distance Metric L1 (Manhattan)distance $$d_1(I_1,I_2) = \\sum_{p} |I_1^p - I_2^p|$$ L2 (Euclidean) distance $$d_2(I_1,I_2) = \\sqrt{\\sum_{p} (I_1^p - I_2^p)^2}$$ Linear classifiers What is a linear classifier?\nLinear classifier is a function $$f(x,W) = Wx + b$$ W is parameters or weights Algebraic/Visual/ Geometric veiwpoints\nSVM loss What is Multiclass SVM loss?\nGiven an example $(x_i, y_i)$ where $x_i$ is the image and where $y_i$ is the (integer) label, and using the shorthand for the scores vector: $s=f(x_i, W)$ the SVM loss has the form: $$\rL_i = ∑_{j≠y_i}\r\\begin{cases}\r0 \u0026 \\text{if } s_{y_i} ≥ s_j + 1 \\\\\rs_j - s_{y_i} + 1 \u0026 \\text{otherwise}\r\\end{cases}\\\\\r= ∑_{j≠y_i} max(0, s_j - s_{y_i} + 1)\r$$ $S_{y_i} - s_j$ is the difference in scores between correct and incorrect class calculation Q1: What happens to loss if car scores decrease by 0.5 for this training example? To determine the effect of decreasing the car score by 0.5, we need to look at the multiclass SVM loss formula:\n$ L*i = \\sum*{j \\neq y*i} \\max(0, s_j - s*{y_i} + 1) $\nFor this training example, the scores are:\ncat: 1.3 car: 4.9 frog: 2.0 Assuming the correct class is \u0026ldquo;car\u0026rdquo; (i.e., $ y_i = \\text{car} $), the loss is calculated as:\n$ L_i = \\max(0, 1.3 - 4.9 + 1) + \\max(0, 2.0 - 4.9 + 1) $ $ L_i = \\max(0, -2.6) + \\max(0, -1.9) $ $ L_i = 0 + 0 = 0 $\nNow, if the car score decreases by 0.5, the new scores are:\ncat: 1.3 car: 4.4 frog: 2.0 The new loss is:\n$ L_i = \\max(0, 1.3 - 4.4 + 1) + \\max(0, 2.0 - 4.4 + 1) $ $ L_i = \\max(0, -2.1) + \\max(0, -1.4) $ $ L_i = 0 + 0 = 0 $\nThus, the loss remains the same at 0.\nQ2: What is the min/max possible SVM loss $ L_i $? Minimum SVM Loss: The minimum SVM loss occurs when the model perfectly classifies the example with a margin of at least 1. Hence, for all incorrect classes $ j $:\n$ s*j - s*{y_i} + 1 \\leq 0 $\nIn this case, the loss for these classes will be 0. Therefore, the minimum SVM loss $ L_i $ is 0.\nMaximum SVM Loss: The maximum SVM loss occurs when the scores for all incorrect classes $ j $ are infinitely higher than the score for the correct class $ y_i $. As the scores for incorrect classes increase without bound, the loss also increases without bound. Therefore, the maximum SVM loss $ L_i $ is infinity.\nQ3: At initialization $ W $ is small so all $ s \\approx 0 $. What is the loss $ L_i $, assuming $ N $ examples and $ C $ classes? At initialization, if all scores $ s \\approx 0 $:\nThe multiclass SVM loss for a single example is:\n$ L*i = \\sum*{j \\neq y*i} \\max(0, s_j - s*{y_i} + 1) $\nSince $ s_j \\approx 0 $ for all $ j $:\n$ L*i = \\sum*{j \\neq y*i} \\max(0, 0 - 0 + 1) $ $ L_i = \\sum*{j \\neq y_i} 1 $ $ L_i = C - 1 $\n(where $ C $ is the number of classes)\nFor $ N $ examples, the total loss is:\n$ L = \\sum*{i=1}^{N} L_i = \\sum*{i=1}^{N} (C - 1) $ $ L = N(C - 1) $\nSo, the loss $ L_i $ for each example is $ C - 1 $, and the total loss for $ N $ examples is $ N(C - 1) $.\nQ4: What if the sum was over all classes (including $ j = y_i $)? If the sum was over all classes, including $ j = y_i $, the loss formula would be:\n$ L*i = \\sum*{j} \\max(0, s*j - s*{y_i} + 1) $\nSince the term for $ j = y_i $ is included, we have:\n$ \\max(0, s*{y_i} - s*{y_i} + 1) = \\max(0, 1) = 1 $\nSo, the loss would be:\n$ L*i = \\sum*{j \\neq y*i} \\max(0, s_j - s*{y_i} + 1) + 1 $\nThus, it includes an additional $ 1 $ term compared to the standard SVM loss for each example.\nSoftmax loss What is Softmax Classifier(Multinomial Logistic Regression) - Want to interpret raw classifier scores as probabilities $$ s = f(x*i; W) P(Y=k|X=x_i) = \\frac{e^{s_k}}{\\sum*{j=1}^{K} e^{s*j}}$$ Softmax Function\n$L_i = - log P(Y=y_i|X=x_i)$ Kullback-Leibler divergence $$ D*{KL}(P||Q) = \\sum*{y\\in \\mathcal{Y}} P(y) log \\frac{P(y)}{Q(y)}$$ Cross-Entropy $$ H(P,Q)=H(p)+D*{KL}(P||Q)$$ Questions\nQ1: What is the min/max possible softmax loss $ L_i $? Minimum Softmax Loss: The minimum softmax loss occurs when the model is perfectly confident and correct. In this case, the probability $ P(Y = y_i | X = x_i) $ for the correct class is 1. Therefore, the loss is:\n$ L_i = -\\log(1) = 0 $\nThus, the minimum possible softmax loss $ L_i $ is 0.\nMaximum Softmax Loss: The maximum softmax loss occurs when the model is infinitely wrong, meaning it assigns a probability of 0 to the correct class. In practical terms, this happens when the score for the correct class is much lower than the scores for all other classes. As the probability approaches 0, the loss approaches infinity:\n$ L_i = -\\log(0) = \\infty $\nThus, the maximum possible softmax loss $ L_i $ is infinity.\nQ2: At initialization, all $ s_j $ will be approximately equal; what is the softmax loss $ L_i $, assuming $ C $ classes? At initialization, if all scores $ s_j $ are approximately equal, the softmax probabilities for each class will be evenly distributed. For $ C $ classes, each class will have a probability of $ \\frac{1}{C} $.\nThe softmax loss for the correct class $ y_i $ is:\n$ L_i = -\\log\\left(\\frac{1}{C}\\right) = \\log(C) $\nTherefore, the softmax loss $ L_i $ at initialization, assuming $ C $ classes, is $ \\log(C) $.\nQ3: If all scores are small random values, what is the loss? $$-log(\\frac{1}{C})\\\\\rlog(10)\\approx 2.3$$ Cross-Entropy vs SVM Loss Q: What is cross-entropy loss? What is SVM loss? Cross-entropy loss \u0026gt; 0, SVM loss = 0\nQ: What happens to each loss if I slightly change the scores of the last datapoint? Cross-entropy loss will change; SVM loss will stay the same\nSummary Lecture 3: Regularization and Optimization\nRegularization Stochastic Gradient Descent Momentum, AdaGrad, Adam Learning rate schedules Lecture 4: Neural Networks and Backpropagation\nMulti-layer Perceptron Backpropagation Perceiving and Understanding the Visual World Lecture 5: Image Classification with CNNs History Higher-level representations, image features Convolution and pooling Lecture 6: CNN Architectures Batch Normalization Transfer learning AlexNet, VGG, GoogLeNet, ResNet Lecture 7: Recurrent Neural Networks RNN, LSTM, GRU Language modeling Image captioning Sequence-to-sequence Lecture 8: Attention and Transformers Self-Attention Transformers Lecture 9: Object Detection and Image Segmentation Single-stage detectors Two-stage detectors Semantic/Instance/Panoptic segmentation Lecture 10: Video Understanding Video classification 3D CNNs Two-stream networks Multimodal video understanding Lecture 11: Visualizing and Understanding Feature visualization and inversion Adversarial examples DeepDream and style transfer Generative and Interactive Visual Intelligence Lecture 12: Self-supervised Learning Pretext tasks Contrastive learning Multisensory supervision Lecture 13: Generative Models Generative Adversarial Network Diffusion models Autoregressive models Lecture 14: OpenAI Sora Diffusion models Lecture 15: Robot Learning Deep Reinforcement Learning Model Learning Robotic Manipulation Lecture 16: Human-Centered Artificial Intelligence Lecture 17: Guest Lecture by Prof. Serena Yeung-Levy Lecture 18: 3D Vision 3D shape representations Shape reconstruction Neural implicit representations ","permalink":"https://csc-bo.github.io/posts/cs231/","summary":"Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch \u0026amp; Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Deep Learning Basics Lecture 1: Computer vision overview\nLecture 2: Image Classification with linear Classifiers\nThe data-driven approch What is data-driven approaches linear classification \u0026amp; kNN means? Data-Driven Approach: 1.","title":"Notes for CS231n: Deep Learning for Computer Vision"},{"content":"Machine Learning Courses UofT MAT334 Complex Variables MAT223\u0026amp;MAT224 Linear Algebra I\u0026amp;II MAT236 Vector Calculus STA302H1/STA1001: Methods of Data Analysis I MIT 6.S191 Introduction to Deep Learning Sequence to Sequence Computer Vision Reinforcement Learning Generative Modeling Lab 1: Tensorflow, Tensor, Auto differentiation, GradientTape, RNN Lab 2: MNIST, CNN, VAEs, Debiasing Facial Detection extra lab llm finetune self driving Standford [ ] CS229: Machine Learning [ ] CS231n: Deep Learning for Computer Vision Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch \u0026amp; Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Lecture 1: Computer vision overview Deep Learning Basics Lecture 2: Image Classification with linear Classifiers The data-driven approch K-nearest neighbor Linear classifiers Algebraic/Visual/ Geometric veiwpoints SVM and Softmax loss Lecture 3: Regularization and Optimization Regularization Stochastic Gradient Descent Momentum, AdaGrad, Adam Learning rate schedules Lecture 4: Neural Networks and Backpropagation Multi-layer Perceptron Backpropagation Perceiving and Understanding the Visual World Lecture 5: Image Classification with CNNs History Higher-level representations, image features Convolution and pooling Lecture 6: CNN Architectures Batch Normalization Transfer learning AlexNet, VGG, GoogLeNet, ResNet Lecture 7: Recurrent Neural Networks RNN, LSTM, GRU Language modeling Image captioning Sequence-to-sequence Lecture 8: Attention and Transformers Self-Attention Transformers Lecture 9: Object Detection and Image Segmentation Single-stage detectors Two-stage detectors Semantic/Instance/Panoptic segmentation Lecture 10: Video Understanding Video classification 3D CNNs Two-stream networks Multimodal video understanding Lecture 11: Visualizing and Understanding Feature visualization and inversion Adversarial examples DeepDream and style transfer Generative and Interactive Visual Intelligence Lecture 12: Self-supervised Learning Pretext tasks Contrastive learning Multisensory supervision Lecture 13: Generative Models Generative Adversarial Network Diffusion models Autoregressive models Lecture 14: OpenAI Sora Diffusion models Lecture 15: Robot Learning Deep Reinforcement Learning Model Learning Robotic Manipulation Lecture 16: Human-Centered Artificial Intelligence Lecture 17: Guest Lecture by Prof. Serena Yeung-Levy Lecture 18: 3D Vision 3D shape representations Shape reconstruction Neural implicit representations [ ] CS224n: Natural Language Processing with Deep Learning [ ] CS224u: Natural Language Understanding [ ] CS224w: Machine Learning with Graphs [ ] CS236： Deep Generative Models UCB [ ] CS267 Application of Parallel Computers Online Resources [ ] Pytorch freeCodeCamp: PyTorch for Deep Learning \u0026amp; Machine Learning\nFundamentals: Tensor Exercise Workflow Fundamentals: Data, Model, Trainning, Inference, Save \u0026amp; Load Model Exercise Neural Network Classification: Binary, Multi-class, Multi-label Exercise Computer Vision: CNN Exercise Custom Dataset Going Modular Transfer learning Experiment Tracking Paper Replicating Model Deployment StatQuest：Introduction to Pytorch\nBuild a CNN detect handwrite aphabets\nRNN / LSTM\nTransformer\nViT\n\u0026hellip;\n[ ] Neural Networks: Zero to Hero The spelled-out intro to neural networks and backpropagation: building micrograd micrograd exercise The spelled-out intro to language modeling: building makemore Building makemore Part 2: MLP E1 E2 Building makemore Part 3: Activations \u0026amp; Gradients, BatchNorm the forward pass activations, backward pass gradients Batch Normalization typical diagnostic tools and visualizations Residual connections and the Adam optimizer E1 E2 Building makemore Part 4: Becoming a Backprop Ninja Exercise 1 Exercise 2 Exercise 3 Exercise 4 Building makemore Part 5: Building a WaveNet Let\u0026rsquo;s build GPT: from scratch, in code, spelled out. Let\u0026rsquo;s build the GPT Tokenizer Neural Networks by 3Blue1Brown Neural Networks and Deep Learning Gradient Descent, How machines learn Backpropagation Backpropagation Calculus GPT(Generative Pre-trained Transformer) Essence of linear algebra review my linear algebra Essence of calculus review my calculus Fun Projects Stock Trend/Pricing Prediction LSTM Tranformer MultiModel Face reconginition CNN ","permalink":"https://csc-bo.github.io/posts/roadmap_ml/","summary":"Machine Learning Courses UofT MAT334 Complex Variables MAT223\u0026amp;MAT224 Linear Algebra I\u0026amp;II MAT236 Vector Calculus STA302H1/STA1001: Methods of Data Analysis I MIT 6.S191 Introduction to Deep Learning Sequence to Sequence Computer Vision Reinforcement Learning Generative Modeling Lab 1: Tensorflow, Tensor, Auto differentiation, GradientTape, RNN Lab 2: MNIST, CNN, VAEs, Debiasing Facial Detection extra lab llm finetune self driving Standford [ ] CS229: Machine Learning [ ] CS231n: Deep Learning for Computer Vision Assignments Assignment #1 Image Classification, kNN, SVM, Softmax, Fully Connected Neural Network Assignment #2 Fully Connected and Convolutional Nets, Batch Normalization, Dropout, Pytorch \u0026amp; Network Visualization Assignment #3 Image Captioning with RNNs and Transformers, Network Visualization, Generative Adversarial Networks, Self-Supervised Contrastive Learning Lecture 1: Computer vision overview Deep Learning Basics Lecture 2: Image Classification with linear Classifiers The data-driven approch K-nearest neighbor Linear classifiers Algebraic/Visual/ Geometric veiwpoints SVM and Softmax loss Lecture 3: Regularization and Optimization Regularization Stochastic Gradient Descent Momentum, AdaGrad, Adam Learning rate schedules Lecture 4: Neural Networks and Backpropagation Multi-layer Perceptron Backpropagation Perceiving and Understanding the Visual World Lecture 5: Image Classification with CNNs History Higher-level representations, image features Convolution and pooling Lecture 6: CNN Architectures Batch Normalization Transfer learning AlexNet, VGG, GoogLeNet, ResNet Lecture 7: Recurrent Neural Networks RNN, LSTM, GRU Language modeling Image captioning Sequence-to-sequence Lecture 8: Attention and Transformers Self-Attention Transformers Lecture 9: Object Detection and Image Segmentation Single-stage detectors Two-stage detectors Semantic/Instance/Panoptic segmentation Lecture 10: Video Understanding Video classification 3D CNNs Two-stream networks Multimodal video understanding Lecture 11: Visualizing and Understanding Feature visualization and inversion Adversarial examples DeepDream and style transfer Generative and Interactive Visual Intelligence Lecture 12: Self-supervised Learning Pretext tasks Contrastive learning Multisensory supervision Lecture 13: Generative Models Generative Adversarial Network Diffusion models Autoregressive models Lecture 14: OpenAI Sora Diffusion models Lecture 15: Robot Learning Deep Reinforcement Learning Model Learning Robotic Manipulation Lecture 16: Human-Centered Artificial Intelligence Lecture 17: Guest Lecture by Prof.","title":"Roadmap for machine lerning"},{"content":"What is Deep Learning? Deep learning is a subfield of artificial intelligence (AI) that focuses on training artificial neural networks to learn and make intelligent decisions. These networks are inspired by the structure and function of the human brain, and they can learn from vast amounts of data to perform tasks such as image recognition, natural language processing, and machine translation.\nHow does Deep Learning work? Deep learning models are typically composed of multiple layers of interconnected nodes, known as neurons. Each neuron receives input data, performs a simple calculation, and passes the result to the next layer. By adjusting the weights and connections between neurons during training, the model learns to extract meaningful patterns from the data and make accurate predictions.\nWhat are the applications of Deep Learning? Deep learning has a wide range of applications across various industries, including:\nComputer vision: Image and video recognition, object detection, self-driving cars Natural language processing: Machine translation, sentiment analysis, chatbots Healthcare: Medical image analysis, drug discovery, disease prediction Finance: Fraud detection, risk assessment, algorithmic trading Robotics: Robot control, navigation, manipulation What are the challenges of Deep Learning? Despite its impressive capabilities, deep learning also faces several challenges:\nData requirements: Deep learning models often require large amounts of data to train effectively. Computational cost: Training deep learning models can be computationally expensive and time-consuming. Explainability: Understanding how deep learning models make decisions can be difficult, leading to concerns about transparency and bias. Ethical considerations: The use of deep learning raises ethical concerns related to privacy, fairness, and potential misuse. How can I get started with Deep Learning? There are many resources available to help you get started with deep learning:\nOnline courses: Platforms like Coursera, edX, and Udacity offer various deep learning courses. Books and tutorials: Numerous books and online tutorials cover deep learning concepts and techniques. Open-source libraries: Libraries like TensorFlow, PyTorch, and Keras provide tools for building and training deep learning models. Community forums and groups: Online communities offer a platform to connect with other deep learning enthusiasts and experts. ","permalink":"https://csc-bo.github.io/faq/","summary":"What is Deep Learning? Deep learning is a subfield of artificial intelligence (AI) that focuses on training artificial neural networks to learn and make intelligent decisions. These networks are inspired by the structure and function of the human brain, and they can learn from vast amounts of data to perform tasks such as image recognition, natural language processing, and machine translation.\nHow does Deep Learning work? Deep learning models are typically composed of multiple layers of interconnected nodes, known as neurons.","title":"Deep Learning FAQ"},{"content":"1. Introduction to Deep Learning After watch one of Feifei Li and Geffery Hinton\u0026rsquo;s video on Youtube, I start to gain interest in DeepLearning, then I found this MIT course online that is a very comprehensive introduction for the Deep Learning which perfect for beginners like myself. This blog post marks the beginning of my journey into the world of machine learning, serving as a collection for my course notes and insights from related materials. 2. Course Content Leture 1: Introductin\nThe Perceptron\nPerceptron: Simplified Activation Functions is to introduce non-linearites into the network Sigmoid Hyperbolic Tangent Rectified Linear Unit(ReLU) We can implement this graph on code very easily One perceptron: Draws a line to separate data. binary classification. Multiple perceptrons in layers: Can create complex curves and shapes to separate data What does it do? Essentially, a perceptron learns a linear decision boundary that separates the input space into two regions, each corresponding to a different class. Forward Propagation/Forward Pass:\nrefers to the calculation and storage of intermediate variables (including outputs) for a neural network in order from the input to the output layer.\nThe Neural Networks\nNeural Network: Stacking Perceptrons to form neural network -\u0026gt; MLP: Multi Layer Perceptron Loss: The loss of our network measures the cost incurred from incorrect\nEmpirical Loss\nThe empirical loss measures the total loss over our entire dataset. Also know as Objective function, Cost function, Empirical Risk $$\rJ(W) = \\frac{1}{n} \\sum_{i=1}^{n} L(f(x_i;W),y_i)\r$$ Binary Cross Entropy Loos $$\rJ(W) = - \\frac{1}{n} \\sum_{i=1}^{n} \\log(f(x_i;W)) + (1-y_i) log(1-f(x_i;W))\r$$ Mean Squared Error Loss $$\rJ(W) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - f(x_i;W))^2\r$$ Loss Optimization\n$$\r\\begin{aligned}\rW^* = \\arg \\mathop{min}\\limits_{W} \\frac{1}{n} \\sum_{i=1}^{n} L(f(x_i;W),y_i)\\\\\rW^* = \\arg \\mathop{min}\\limits_{W} J(W)\\\\\r\\text {Where as: } W=\\{W_0, W_1, ...\\}\r\\end{aligned}\r$$ We want to find the network weights that achieve the lowest loss\nOur loss is a function of the network weights\nWe can use gradient descent to find the weights that minimize the loss\nGradient Descent\nInitialize weights randomly $\\sim N(0,\\sigma^2)$ Loop until convergence Compute gradients, $ \\frac{\\partial J(W)}{\\partial W}$ Update weights: $ W \\leftarrow W - \\eta \\frac{\\partial J(W)}{\\partial W}$ Return weights Backpropagation:\nBackpropagation is the application of gradient descent in deep learning, used to compute gradients of weights in neural networks. It employs the chain rule from calculus to efficiently propagate errors backwards, guiding updates to each layer\u0026rsquo;s parameters. By calculating the partial derivatives of the loss function with respect to every weight, it determines how each weight should be adjusted to minimize the loss.\nLearning Rate\nMomentum or Adaptive Learning Rates: For improved convergence, incorporate momentum or adaptive learning rate techniques like Nesterov Accelerated Gradient (NAG), RMSprop, or Adam.\nMini-Batch Gradient Descent\nInstead of using all data points at once, use a mini-batch of B data points to compute the gradient. This reduces the computational cost and improves convergence. $$\r\\frac{\\partial J(W)}{\\partial W} = \\frac{1}{B} \\sum_{i=1}^{B} \\frac{\\partial J_k(W)}{\\partial W}\\\\\r$$ $$\r\\text {Then update weights: }W \\leftarrow W - \\eta \\frac{\\partial J(W)}{\\partial W}\r$$ More accurate estimation of gradient Allow for larger learning rates Smoother convergence Fast training! Can parallelize computation and achieve significant speed increases on GPU\u0026rsquo;s Optimization Algorithm SGD(Stochastic Gradient Descent) Adam Adadelta Adagrad RMSProp Real World Technique\nMini-batches Fitting Underfitting: Model does not have capacity to fully learn the data Ideal fit Overfitting: Too complex, extra parameters, does not generalize well Regularization: Improve generalization of our model on unseen data Dropout During training, randomly set some activations to 0\nTypically \u0026lsquo;drop\u0026rsquo; 50% of activations in layer\nForces network to not rely on any 1 node\nEarly Stopping\nStop training before we have a chance to overfit Leture 2: Deep Sequence Modeling\nI\u0026rsquo;m gonna write this note in a more practical way, and from my SDE perspective. Code is nessesary for me to understand the concept.\nRecurrent Neural Networks(RNNs)\nMany to One: Sentiment Classification One to Many: Text Generation, Image Captioning Many to Many: Translation \u0026amp; Forecasting, Music Generation class myRNNCell(tf.keras.layers.Layer):\rdef __init__(self, rnn_units, input_dim, output_dim):\rsuper(myRNNCell, self).__init__()\r# Initialize weight matrices\rself.W_xh = self.add_weight([rnn_units, input_dim])\rself.W_hh = self.add_weight([rnn_units, rnn_units])\rself.W_hy = self.add_weight([output_dim, rnn_units])\r# Initialize hidden state to zeros\rself.h = tf.zeros([rnn_units, 1])\rdef call(self, x):\r# the input x corresponds to x_t in the graph\r# Update the hidden state\r# h_t = tanh(W_hh * h_{t-1} + W_xh * x_t)\rself.h = tf.math.tanh( self.W_hh * self.h + self.W_xh * x)\r# Compute the output, y^hat_t = W_hy * h\routput = self.W_hy * self.h\r# Return the current output and hidden state\rreturn output, self.h\r# tensorflow method\rtf.keras.layers.SimpleRNN(rnn_units) The implementation is highly identical to the graphical representation. This code provided by Professor slide not complete. But we can get the idea. I watched Andrej Karpathy\u0026rsquo;s video, he actully only need an image to implement Neural Networks. Combine the graph and code help me to understand the concept. Endcoding Language for a Neural Network\nEmbbedding: transform indexes into a vector of fixed size. One-hot embedding: One-hot embedding is a way to represent categorical data as binary vectors. Each category is represented by a binary vector, where only one element is 1, and the rest are 0. This representation is useful for representing categorical data, such as words in a language, where each word is represented by a unique index. RNN feedforwar and backpropagation graph RNN Problems\nExploding Gradients: Many values \u0026gt; 1, mutiply many large numbers together lead to unstable training, oscillations in the loss function, and ultimately, the model failing to converge to a good solution. Gradient clipping: A technique used to prevent exploding gradients by clipping the gradients to a maximum value. Vanishing Gradients: Many values \u0026lt; 1, mutiply many small numbers together cause the weights to not update at all. then the bias parameters capture short-term dependencies. result no long-term dependencies. Activation functions: Using ReLU prevents f\u0026rsquo; from shriking the gradients when x \u0026gt; 0 Parameter Initialization: Initializing weights to identity matrix, initialize biases to zero. this prevent the weight from shrinking to zero Gated Cells: use gates to selectively add or remove information within each recurrent unit with gated cell (LSTMs, GRUs) LSTMs(Long Short-Term Memory)\nGated LSTM cells control information flow Key Concepts Maintain a cell state Use gates to control the flow of information Forget gate gets rid of irrelevant information Store relevent information from current input Selectively update cell state Output gate returns a filtered version of the cell state Backpropagation through time with partially uninterrupted gradient flow Limitations of RNNs\nEncoding bottleneck Slow, no parallelization Not long memory Attention is all you need(transformer)\nAttention mechanism: orignal from a paper of RNN, this is like RNN search. Allow network to search relevant information Self-Attention: identify and attend to most important features in input Encode position information Extract query, key, value for search Comput attention weighting Extract Features with high attention These steps form a self-attention head that plug into a larger network. Each head attends to a different part of the input. Self-Attention Applied: Language Model: Transformers, GPT, BERT Biological Sequence: AlphaFold2 Computer Vison: Vision Transformers Tranfomer Structure Leture 3: Deep Computer Vision\nFoundation: Images are numbers 2D image Vector of pixel values Convolution extract features with convolution Steps: Apply a set of weights - a filter - to extract local features Use multiple filters to extract different features Spatially share parameters of each filter CNNs(Convolutional Neural Networks) Convolution: Apply filters to generate feature maps Non-linearity: Often ReLU Pooling: Downsampling operation on each feature map. import tensorflow as tf\rdef generate_model():\rmodel = tf.keras.Sequential([\r# first convolutional layer\rtf.keras.layers.Conv2D(32, (3, 3), activation=\u0026#39;relu\u0026#39;, input_shape=(28, 28, 1)),\rtf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\r# second convolutional layer\rtf.keras.layers.Conv2D(64, (3, 3), activation=\u0026#39;relu\u0026#39;),\rtf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\r# fully connected classifier\rtf.keras.layers.Flatten(),\rtf.keras.layers.Dense(1024, activation=\u0026#39;relu\u0026#39;),\rtf.keras.layers.Dense(10, activation=\u0026#39;softmax\u0026#39;) # 10 outputs\r])\rreturn model Applications Classification Breast Cancer Screening Object Detection R-CNNs alorithm: Find regions that we think have objects. Use CNN to classify Semantic Segmentation: Fully Convolutional Networks(FCN) Biomedical Image Analysis Navigation from Vision End-to-End Autonomos Navigation Leture 4: Deep Generative Modeling\nSupervised/Unsupervised Learning Gnerative Models Debiasing Outlier Detection VAEs (Autoencoders and Variational Autoencoders) Autoencoders GANS (Generative Adversarial Networks) Basic Concepts Applications Leture 5: Deep Reinforcement Learning\nRL Concepts Agent Environment Actions Observations State Reward The Q-function captures the expected total future reward an agent in state,s, can receive by executing a certain action,a RL Learning Algorithms Value Learning Policy Learning DQN (Deep Q-Networks) Concepts Training Summary Downside of Q-learning Complexity Can model scenarios where the action space is discrete and small Cannot handle continuous action spaces Flexibility Policy is deterministically computed from the Q function by maximizing the reward Cannot learn stochastic policies Policy Gradient Methods Key ideas Directly optimize the policy pi(s) Continuouse action spaces Traning Initialize the agent Run a policy until termination Record all states, actions, rewards Decrease probability of actions that resulted in low reward Increase probability of actions that resulted in high reward Applications Alpha Go Lecture 6: Limitation and New Frontiers\nModule Summaries: Briefly summarize each module of the course, highlighting key concepts and algorithms covered. Lecture Notes: You can include your personal notes from lectures, focusing on important points and areas of difficulty. Assignments and Projects: Discuss the assignments and projects assigned in the course, sharing your approach and solutions. 3. Resources Share any additional resources you found helpful while taking the course, such as:\nOnline Tutorials: Links to relevant online tutorials or articles that provide further explanation on specific topics.\nResearch Papers: References to important research papers in the field of deep learning.\nSoftware Libraries: Information about deep learning libraries used in the course, such as TensorFlow or PyTorch.\n4. Personal Insights and Reflections Share your personal thoughts and reflections on the course, including:\nChallenges Faced: Discuss any challenges you encountered while learning the material and how you overcame them. Key Takeaways: Highlight the most important things you learned from the course. Future Applications: Explore potential applications of deep learning that you find interesting. 5. Conclusion Conclude your post by summarizing your experience with the course and expressing your thoughts on the field of deep learning as a whole.\n","permalink":"https://csc-bo.github.io/posts/mit6s191/","summary":"1. Introduction to Deep Learning After watch one of Feifei Li and Geffery Hinton\u0026rsquo;s video on Youtube, I start to gain interest in DeepLearning, then I found this MIT course online that is a very comprehensive introduction for the Deep Learning which perfect for beginners like myself. This blog post marks the beginning of my journey into the world of machine learning, serving as a collection for my course notes and insights from related materials.","title":"Notes for MIT 6.S191"}]